Let's consider the random surfer model we just defined. There's a problem in the current definition in particular suppose that a page x or web page x has no outgoing links. In a random surfer model with probability one minus alpha, we go to a random web page from the entire graph. And with probability alpha, we follow a random link from this current page x. Now what happens if this page x is a sink node and has no outgoing links? What do we do in this case with probability alpha? Currently the model is not well-defined because of this case. And there are several alternatives that we can consider which will make it well-defined. The simplest option is just to self-loop. What exactly do we mean? We mean that if there's no outgoing links, if this page acts as a sink node, then with probability alpha, we just stay at the page x. We had a link from x to itself. The downside of this approach is that we're adding an incoming link into x. So this is going to artificially boost the page rank of page x. Another option is to simply remove these sink nodes. Now once we remove some of these sink nodes, then there will be new sink nodes that might be created. So we have to recursively apply this approach. We have to keep removing sink nodes recursively until there are no sink nodes remaining in the graph. The problem with this approach is that we shrunk the graph, so there are some nodes which will not get a page rank. One more natural approach I want to consider is that in this case of a sink node x, with probability alpha, we'll go to a random web page chosen uniformly at random from the entire graph. In other words, we're going to set alpha equal zero for just these sink nodes. So, for a sink node with probability one, we'll choose a random web page from the entire graph and go to that random web page. This is a quite natural approach and this is apparently what page rank actually does according to Wikipedia.