Recall our setting. We have a database of size M and we have a hash table of size C times M for some C strictly greater than one. What we just showed is that the false positive probability is approximately .6185 raised to the power of C. This .6185 corresponded to one half raised to the power LN two. Let's now look at some specific examples to see how this performs. Let's suppose we did the naive scheme, where K equals one. So, we didn't do the optimal choice of K. We just set one hash function. And let's look at the case where we do 10 times larger or 100 times larger. Now, this expression for the false positive probability was assuming the optimal choice of K. In order to analyze this case where K equals one, we have to go back to our expression of F of K. If you look back at that expression and you plug in K equals one and C equal's 10, or C equal's a 100, you get the following. In the first case, the false positive probability is .095. And in the second case it's point.00995. Now, suppose we do the optimal choice of K. So, then our false positive probability is going to be this expression. Let's look at C equals 10. What do we get? We get.0082. A reasonable gain. But not that much better than C equal's a 100 with this simple K equals one case. Let's try to C equals a 100. Sorry. Hash table is a 100 times bigger than the database we're trying to store. But this is just a binary string, right? So, it's very reasonable to consider a hash table which is 100 times bigger. Now, the false positive probability is 1.3 times ten to the minus 21. The key thing is that this is exponential in C. So, taking C equals to a 100, it's tiny. This is really a minuscule probability. And if this is not small enough for you, you can go C equals to 200, or 300 and you're going to get a really, really tiny probability of a false positive. So, if you're willing to have a very small probability of a false positive, then you have this very simple data structure which just corresponds to having a binary string. It's very simple to maintain and is very fast query times and the false positive probability is very small. The downside of this data structure is that occasionally, you might have some false positives and also it doesn't easily allow for deletions from the database. Though, there are some heuristics for allowing deletions, these are modifications which are called Counting Bloom Filters. Well, that completes our description of Bloom Filters. I look forward to seeing your projects where you're going to implement Bloom Filters and you're going to explore whether these approximations that we did in our analysis were reasonable or not.