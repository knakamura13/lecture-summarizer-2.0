LP1 Linear Programming - 411 - Linear Programming
Linear programming is a very general technique for solving optimization problems. It handles any problem that can be formulated as an optimization over a set of variables with a goal known as the objective function and the constraints can all be expressed as linear functions of the variables. We'll see that max flow can be formulated as a linear program. Also, we'll see that many problems from operations research can be expressed as LPs. We'll describe the general idea of the simplex algorithm which is widely used for solving LPs. We'll take a closer look at one of the beautiful aspects of linear programming, LP duality. We'll explain what LP duality is and some of the useful consequences for us. Finally, we'll look at an application of LP to approximation algorithms. We'll use linear programming to design an approximation algorithm to the classic set problem.

LP1 Linear Programming - 412 - Lecture Outline
In this section, we'll explore linear programming. This is a powerful technique. We'll see how to solve a variety of problems using linear programming. We'll start by seeing how we solve max flow using linear programming, then we'll see a few simple examples of linear programs for production problems. After looking at these examples, we'll give the general formulation of linear programming, and look at the standard form for LPs. Finally, we'll look at LP duality, which is a beautiful and important aspect of this topic. Let's dive in and look at how we express max flow as a linear program.

LP1 Linear Programming - 413 - Max-Flow as LP
The input to the max flow problem as a directed graph G along with positive capacities for every edge. Now our linear program is going to have m variables, one for every edge. Now we're going to have an objective function. This is the function that we're trying to optimize. This should be a linear function of the variables. Now for max flow, we're trying to maximize the size of the flow. We measure the size of flow by the flow out of the source vertex or the flow into the sink vertex. We'll state it in terms of the flow out of the source vertex, and thus we're maximizing the total flow out of the source vortex. So we're summing over edges out of the source vertex S, and we're looking at the flow along that edge. Now we have some constraints on our variables. For every edge, the flow along that edge has to be non-negative and it needs to satisfy the capacity constraint. So the flow must be at most the capacity. We also have a constraint for every internal vertex, so for every vertex, except for S and T we need that the flow into vertex V must equal the flow out of vertex V. So the flow is conserved. So if we look at the total flow into V, that needs to match the total flow out of V. Now why is this a linear program? Because the objective function is a max or a min of a linear function of the variables, and all the constraints are linear functions of the variables. What we'll see is that this formulation of the max flow problem is quite powerful. We can easily solve several variants of the max flow problem with simple modifications to this LP.

LP1 Linear Programming - 414 - Simple 2D Example
Let's take a look at a simple example of a linear program. This is motivated by a basic production problem. Consider a company that makes two products, let's call them A and B. We want to figure out how many of each product to produce in order to maximize our profit. Now let's make some modeling assumptions. Each unit of A that we sell makes a profit of one dollar and for each unit of B, we'll assume that it makes a profit of six dollars. Now let's make some assumptions about the demand for each product. We'll assume that the demand for A is at most 300 units per day and at most 200 units per day for B. We'll also assumes some supply constraints. We'll assume our employees can work at most 700 hours per day and each unit of A that we produce takes one hour, whereas each unit of B takes three hours. Now let's express this is as a linear.

LP1 Linear Programming - 415 - 2D LP Formulation
First off, what are the variables in our linear program? What we're trying to determine how many units of A and B to produce each day? Hence, let x1 be the number of units of A that we should produce per day and x2 is the number of units of B to produce per day. Our goal is to maximize the profit. So we have a maximization problem. Now, how do we express the profit in terms of x1 and x2? Well, we make a profit of $1 per unit of A, so that's x1 and $6 per unit of B that we sell. So, that's 6x2. Now what are the constraints? First off, the demand constraint. Well, we want to make at most 300 units of A. So, this means that x1 is at most 300 and x2 is at most 200. Now, also we want x1 and x2 to be non-negative. We can't produce a negative amount of any of the products. So, let's throw in these non-negative constraints. Now let's look at the supply constraint. The total supply, the total number of man hours available is 700 and it takes one hour per unit of A and three hours per unit of B. So, we want that x1 plus 3x2 is at most 700.

LP1 Linear Programming - 416 - 2D LP Recap
Now let's go ahead and restate the LP we just formulated. Our objective function was to maximize X_1 plus 6_x_2. Recall, this was a profit we obtained. Now,this was subject to the following constraints. First off, the demand for X_1 product A was at most 300 units. For Product B the demand was at most 200 units, so X_2 is at most 200. We need it to ensure that these variables are non-negative. Finally, we have the supply constraint. Now let's look at this problem geometrically. For now let's ignore the objective function and let's look at the five constraints. Now each of these five constraints is a half plane. If it was equality, it would be a line, but we're doing one side of the line. So, this gives us a half plane. Now note, we're in two dimensions because we have two variables. Now we want to satisfy all five constraints, so we want to look at the intersection of these five half planes. The intersection of these five a half planes is going to give us a set of feasible X. A set of X which satisfy all five constraints and then we're going to look in that feasible set to find the X which maximizes this objective function. So let's first look at this feasible region.

LP1 Linear Programming - 417 - 2D Geometric View
let's take a look pictorially at these five constraints now once again we have two variables so we're in two-dimensional space now our first constraint says that x1 is a most 300 so we have this line x1 equals 300 and we're looking at the left side all points on the left side of this line the second constraint is that x2 is the most 200 so we're looking at our points below this line and we want points satisfying both constraints so we're looking at the points in the bottom left quadrant of these two vectors next we have the non-negative constraints putting in both non-negative constraints now we lie in this rectangle and then we have our final our fifth constraint which says that we lie on this side of this line hence this tan region is our feasible region all the points in this tan region in this convex polygon satisfy all five constraints

LP1 Linear Programming - 418 - Optimum
Now, how do we find the optimal point? Our goal is to maximize this function x1 plus 6x2. So, let's look at this line x1 plus 6x2 equal C. We're trying to maximize the C, which intersects this feasible region. So, let's find the maximum value of this C, where this linear function intersects the feasible region. So, let's take a look at this function, this linear function for different values of C. Now, here's the line x1 plus 6x2 equals 600. Now, all these points on the line which intersect the feasible region, so these points, all satisfy the five constraints and achieve profit of 600. Now, clearly these are not optimal because we can move up this line. Now, we can think of moving up this line into the last point when we intersect the feasible region. Now, it turns out the maximum value of C for which this line intersects the feasible region is C equals 1300. This line x1 plus 6x2 equals 1300 intersects the feasible region at this corner of this polygon, this vertex of this feasible region. This is the point 100, 200. So, we know the optimum is at the point x1 equals 100 and x2 equals 200 and that achieves a profit of 1300. Now, this was a simple two dimensional example. Let's look at a simple three dimensional example and then we'll get some idea of how it generalizes to higher dimensions.

LP1 Linear Programming - 419 - Key Issues
Before moving on, let's use this simple example to identify a few important issues. The first and perhaps the most important issue is that we're optimizing over this entire feasible region, this tan set here. Now in this example the optimum happened be at an integer point, this point 100, 200. So x_1 and x_2 are integer values, but we have no way to only consider solutions lying on integer points. We can only find the best point in this whole region. For example, for a different objective function, the optimum may lie at this vertex. At this vertex. X_1 is an integer value, it's 300, but X_2 is a fractional value. In such a scenario, what do we do if we only want to consider integer points such as in our example for the production situation? Well, if we end up with a fractional point, which is the optimal point, we could try to round it to an integer point. That's something we'll do in the lecture LP four on the Max-set Approximation Algorithm. The key point is that linear programming optimizes over this entire set and when we optimize over this entire set, then the problem is polynomial time solvable. So, linear programming LP is in the complexity class P. Now, the corresponding problem where we look for the best integer valued solution is called integer linear programming and is denoted as ILP. The ILP problem is NP-complete. So it's unlikely that we can solve it in polynomial time. We'll see this fact that ILP is NP-complete in the Max-set Approximation Algorithm lecture. Another key idea is that the optimal point for this linear program lies at a vertex of this polygon. What does a vertex mean? A vertex means a corner of this polygon. So, there are five vertices of this polygon: one, two, three, four, and five. Why is it the case that the optimum must lie at a vertex of this polygon of this feasible region? Now, it may be the case that other points are also optimal. We're just saying that vertices are at least as good as any other point. Look at this line for the max C achievable. In this case, C equals 1300. Suppose this optimal line intersects a point such as this point Z which is not a vertex of this polygon. Well, this point Z lies on on edge of the polygon. Therefore, it must be the case that one of these two endpoints of this edge must be better than this point Z, or it has to be the case that this optimal line intersects this edge. If this optimal line intersects this edge, then all the points on this edge are optimal. Therefore, the two endpoints are both optimal and therefore an optimal point lies at a vertex. The last important concept is convexity. This feasible region, this tan region is convex. What exactly does convexity mean? Well, if we take any two points in this set and we look at the line connecting them, then that entire line, that entire edge is contained in this set. This means that we don't have a shape like this. The feasible region can't go down and then back up because if we take a point over here and a point over here and we look at the line, the edge connecting them, then it goes outside the set. So this is non-convex set. Because the feasible region is defined by the intersection of half spaces, it must be a convex set. We can't get these nasty shapes. Since it's convex, if we have a vertex such as this point,100, 200, which is better than its neighbors, these two points, well, the feasible region can't go down and then back up. So, if this point is better than its neighbors, then the entire convex region, the entire feasible region is below these two lines. Therefore, this point is optimal because if a point is better than its neighbors, then that vertex is an optimal point. That's the key point of convexity, the optimal point always lies at a vertex of this feasible region. There may be other points which are optimal, for example, this entire line might be optimal, but there always is a vertex of this feasible region. In this case, these two points which will be optimal. This concept that an optimal point lies at a vertex of the polygon and the key fact that if a vertex is better than all of its neighbors, then its' therefore the global optima. This leads to the simplex algorithm which we'll detail later, but let's give a brief sketch of it. It's sort of a local greedy approach. We'll start at some vertex of the polygon. We'll check its neighbors and we'll look if any of its neighbors are better. Then we'll move to a neighbor which has a better objective function and we continue until we find a vertex of the polygon in the feasible region which is better than all of its neighbors and therefore it's optimal. That's the basic idea of the simplex algorithm. Now let's move on to another three dimensional example.

LP1 Linear Programming - 420 - 3D Example
Let's take a look at another example before giving the general formulation of linear programming. Now this will be a slight twist on the previous example. Now we have three products A, B and C. We'll look at the profit obtained from each, the demand for each, the supply just as before and we'll add one additional constraint compared to before, this will be a packaging constraint. We'll say the profit is one dollar for each unit of A that we sell, six dollars for each unit of B and ten dollars for C. The demand is at most 300 units per day for A and at most 200 units per day for B and unlimited demand for C. The supply that we can produce is at most 1000 hours per day. Each unit of A takes one hour, each unit of B takes three hours to produce and each unit of C takes two hours to produce, finally the packaging. Now we can produce at most 500 units in total per day. We'll say that A doesn't require any packaging, B uses one unit of packaging per unit and C takes three units of the packaging per unit.

LP1 Linear Programming - 421 - 3D LP Formulation
Now, let's formulate this problem as a linear program. Now, we're going to have three variables X1, X2, X3, corresponding to how much we produce of A, and B, and C, respectively, per day. Now, our goal is to maximize the profit. So, our objective function is going to mean maximize X1 plus 6 X2 plus 10X3 based on our profit assumption. Now, we have two demand constraints. X1 is at most 300 and X2 is at most 200. Now, we have one supply constraint, X1 plus 3X2 plus 2X3 is at most 1000. And finally, the packaging constraint says that X2 plus 3X3 is at most 500. And let's not forget the non-negativity constraints. X1, X2, X3 all have to be non-negative.

LP1 Linear Programming - 422 - 3D Geometric View
Let's recap our LP once again. Our objective function is to maximize x1 plus 6x2 plus 10x3. Subject to the following seven constraints. We have the two demand constraints. We have the supply constraint, the packaging constraint and finally the non-negativity constraint. Now, let's take a look at this geometrically. Now, we have three variables, so we're going to line three dimensional space. Now, if we had equality for one of these constraints, there would be a hyperplane, a three dimensional hyperplane. Now, within inequality we're taking one side of that hyperplane. So that's a half space. So we're looking at the intersection of these seven half spaces. This is a geometric view of the feasible region. The points in this ten polyhedron are the feasible axes. These are the points which satisfy all seven constraints. Now, notice this is a convex set, so it can't go down and then back up. Convexity is an important property for the algorithms to solve linear programs. Now, let's take a look at the general formulation of linear programs and then we'll come back to this example and we'll figure out the optimum for this example.

LP1 Linear Programming - 423 - Standard Form
Let's look at the standard form for linear programs. We have n variables x1 through xn. Our objective function is to maximize a linear function of these n variables. We have these coefficients, c1 through cn, and we're maximizing this linear function of the n variables. We're optimizing subject to the m constraints. The first constraint is specified by a11 through a1n. These are the coefficients for x1 through xn. We want that this linear function is at most b1 for a given b1. This is the first constraint. There are n constraints. The mth constraint is specified by am1 through amn, that specifies the left hand side and we want this to be a most mn. And finally, we also have the non-negativity constraint. We want that these n variables are non-negative. We have m plus n constraints. We can specify this more compactly using linear algebra. Let's go ahead and do that.

LP1 Linear Programming - 424 - Linear Algebra View Question
Now the variables are specified by this column vector x. The objective function is specified by these coefficients, this column vector c, and hence our objective function is to maximize c_transpose times x. Then we have our constraint matrix. This is an m by n matrix A. Finally we have the constraints for the right hand side. This is specified by this column vector, b1 through bm. There're m constraints, so this vector has size m. Going back to our Lp formulation, we have our objective function. This is subject to the following constraints. We want those x where A times x is at most b, and we have the non-negativity constraint, x is non-negative. Now the last constraint is important, the non-activity constraint, because if the feasible region is non-empty, so there are some feasible x, then we know that the zero vector is a feasible point. So we can trivially find a feasible point or determine that the feasible region is empty, and therefore this Lp is infeasible.

LP1 Linear Programming - 426 - Converting to Standard Form Question
Now how can we convert an arbitrary linear program into this standard form? For instance, suppose that we want to minimize this linear function instead of maximize this linear function, where we simply multiply this whole thing by negative one. And this is equivalent to maximizing negative C times X. So, we can replace this vector C by negative C and then we can maximize and it's equivalent to LP. Now, what if we have a constraint that says A1X1 through AnXn is at least B, instead of at most B. Once again we can multiply this by negative one. And this is equivalent to -A1 times X1 through An times Xn is at most negative B. Now, what if we have an equality constraint? So we want to set it equal to B, where we can replace this by two inequality constraints. We can say that A1X1 through AnXn is at most B and at least B. And the second of these at least B constraint. We can replace in this manner by an inequality at most negative B. Now, what if we have a strict inequality? So we say that X is strictly less than 100. How do we convert this into a non-strict inequality? Well, suppose the LP just said maximize X subject to the constraint that X is strictly less than 100. What's the solution, what's the optimal value of this function, this linear program? Well, it's not clear what the optimal is. So this ill defined linear program. So strict inequalities are not allowed in linear programming. Now, geometrically the way to view these strict inequalities. Well, first off look at these non-strict inequalities. In this case the feasible region corresponds to this closed convex polyhedron. So the optimal points will lie on the boundary of this polyhedron. But if we have strict inequalities, then the points on the boundary of the polyhedron do not lie in the feasible region. So it's an open set. And the problem is ill defined because we don't know what the optimal is. So strict inequalities are not allowed. Finally, what do we do if we have an unconstrained variable X? So X can be positive or negative. We don't have this non-negativity constraint. How do we add in this non-negativity constraint? Well, we create two new variables, the positive magnitude of X and the negative magnitude of X, X plus and X minus. We constrain X plus to be non-negative and X minus to be non-negative. So these corresponds to the magnitudes and we replace X by X plus minus X minus. And in this manner we can consider all variables to be non-negative.

LP1 Linear Programming - 428 - General Geometric View
Now in general, we have n variables. So we're going to line n dimensional space. Now n is large, so we're in high dimensional space. Now we m constraints plus the n non-negativity constraints. So we have a total n plus m constraints. Now what's our feasible region? Now, each constraint corresponds to a half space and n dimensions. Now the feasible region are those points which satisfy all n plus m constraints. So we want to lie in all n plus m half spaces. So the feasible region corresponds to the intersection of these n plus m half spaces. Now this is going to correspond to a convex polyhedron in n dimensional space. Now the vertices of this polyhedron are the corners of it. Now how do you get a vertex of this polyhedron?

LP1 Linear Programming - 429 - Vertices
Now, how do you specify the vertices of the feasible region? Well, let's take a look back at our simple two dimensional example. How do I specify this vertex of this polygon? It satisfies these two lines with equality. The first line is X1 equals 300 and the second line is X1plus 3X2 equals 700. Now, satisfying these two constraints with equality specifies this point, but then I have to check that this point satisfies the other inequalities. So in general, in n dimensional space, a vertex of this convex polyhedron is specified by specifying the end constraints that you satisfy with equality, and then we have to check that the remaining m constraints are still satisfied. Now we can get a trivial upper bound on the number of vertices of this convex polyhedron, we have to specify the constraints that are satisfied with equality. There are n plus m constraints to choose from. So the number of vertices is most n plus m, choose n, this is exponential in n. So in the worst case, there are a huge number of vertices in this convex polyhedron, in this feasible region. Now for a particular vertex, how many neighbors does it have? Can we get an upper bound on the number of neighbors for a specific vertex? Well, neighbor would correspond to swapping out one constraint with equality with a different constraint. So we have n choices for which one we swap out, and then we have m choices for which when we swap in. So the number of neighbors for a specific vertex is at most, n times m.

LP1 Linear Programming - 430 - LP Algorithms
Now let's give a quick overview of the type of algorithms for solving linear programs. Now there are two types of methods which are guaranteed to solve linear programs in polynomial time in the worst case. These are the ellipsoid algorithm which was the first example of a polynomial time algorithm for linear programming and interior point methods. Now ellipsoids algorithms are more of theoretical interest, but interior point methods are used quite extensively right now. Now an important algorithm is the simplex algorithm. As a disclaimer for the simplex algorithm is that in the worst case it takes exponential time. Despite that worst case bound, simplex algorithm is widely used because the output of the simplex algorithm first off is guaranteed to be an optimal. So when the simplex algorithm completes, it's guaranteed to give an optimal solution to the linear program and it works quite efficiently, very fast on a humongous LPs. There are very fast LP solvers using the simplex algorithm. We're going to take a look at the high level idea of the simplex algorithm.

LP1 Linear Programming - 431 - Simplex Algorithm
Take a look at the basic idea of this simplex algorithm. We want to start at some feasible point. Well, we start at the zero vector. This is the vertex of the feasible region, because it satisfies the N non-negativity constrains with equality. Now, of course we still have to check that this point satisfies the other M constraints. Now, if it does not satisfy any of the M constraints, then we know that this point is not in the feasible region and therefore the feasible region is empty. So this LP is infeasible. There are no X which satisfy all of the constraints. Now, this starting point is feasible. Then we're going to do a sort of local search. So, we're going to look at the neighboring vertices and we're going to try to find one with higher objective value. What do we mean? We mean that the value of the objective function is higher and we want one which is strictly higher. Now recall there are most N times M neighboring vertices. So, in the worst case we can check all the neighboring vertices. Now, if we find a neighboring vertex with higher objective value then we move there and then we repeat. So, for this new point, we'd look for a neighboring vertex which has higher objective value. And we continue, keep repeating. Now, what do we do if there is multiple neighboring vertices with higher objective value? Well, that's one of the heuristics of the variance of simplex algorithm. We could choose one of them at randomly. We could take the neighboring vertex with highest object of value. There's various heuristics that we can use. But what happens in the case that this current point is better than all of the neighboring vertices? Well, recall our feasible region is a convex polyhedron and we're walking on vertices of this convex polyhedron. So we're at some vertex and all the neighbors are smaller than it. Since it's convex, it can't go down and then back up. So, if all the neighbors are smaller than it, then the whole convex, the whole feasible region is smaller than it. So, the current point must be the optimum. So, we simply output the current point and this is guaranteed to be the optimal, the global optimal of the LP. To solidify the idea of the simplex algorithm, we're going to illustrate it on one of our examples. So, let's go back to our three dimensional example, our example with three variables and implement the simplex algorithm on that example.

LP1 Linear Programming - 432 - Simplex Example
Now here's our earlier example with three variables, and let's illustrate the simplex algorithm on this example. The feasible region is this three dimensional convex polyhedron. How does simplex start? Well, we start at the point (0, 0, 0), and we can check that this point is feasible, it satisfies all of the constraints, and the value of the objective function for this point is zero. So we have profit zero for this point. Now, where is this point in the feasible region? It's his point in the back. Now this point is defined by these three inequalities. We satisfy these three constraints with equality, and the remaining constraints, the remaining four constraints, we satisfy with inequalities. Let's look at a neighboring vertex. Let's look at this vertex. This vertex is defined by this constraint with equality, and these two non-negativity constraints but we dropped that one. So we're looking at the vertex defined by this first constraint with equality, and these last two non-negativity constraints with equality. This corresponds to this vertex of the feasible region, and this is the point (300, 0, 0), and it has profit 300. So when we consider it from the zero vector, then we're going to move here because it has higher profit. Next, let's consider this neighboring vertex of the current point. This new vertex satisfies these first two constraints with equality, and the last constraint. It corresponds to the point (300, 200, 0). And if you plug it into the objective function, it has profit 1500. The profit is higher, so we move to this new point. Now let's see if there's a neighbor of this new point which is better. Let's consider this neighbor. This is at the intersection of these three faces, these three faces are the first three constraints. This is the point (300, 200, 50). The value of the objective function for this point is 2000, so we move there. From the current point, let's try a new neighbor. Consider this neighboring vertex, this vertex is defined by equality by these three constraints, and it satisfies the other constraints, so it lies in the feasible region. It corresponds to the point (200, 200, 100). You can see that because X_2 = 200. And then once X_2 = 200, and this constraint says that X3 = 100. And then finally, this specifies that X1 = 200. And then if you check the value of the objective function, we get that it's 2400, so we move to this new point. Finally, we check all of the neighbors of this vertex, and we see that all of the neighbors are worse. So we know that we're at the optimal point and we successfully solved this LP.

LP2 Geometry - 433 - LP Geometry
Let's discuss the geometry of linear programs. Consider a linear program in standard form. Let's say, that there are N variables. So, X is a column vector of size N. And let's let M denote the number of constraints. As we discussed earlier, the feasible region, the set of axes which satisfy these constrains, forms a convex set. Each constraint defines a half space in N dimensions. The set a points satisfying all M constraints are in the intersection of these M half spaces defined by the constraints. So, this forms a convex polyhedron in N dimensions. We illustrated the simplex algorithm on our earlier example and recall that the simplex algorithm is walking on the vertices or the corners of this feasible region. Now, the key point is that since this feasible region is convex, the optimum, the points maximizing this objective function, are going to be achieved at vertices of the feasible region. Now, that's not always true. So, let's explore exactly when it is true.

LP2 Geometry - 434 - LP Optimum
Now, the key, once again, for algorithms which solve linear programs is that the optimum is achieved at a vertex of the feasible region except for the following cases. The two exceptions to this are when the LP is infeasible, and when the LP is unbounded. An LP is infeasible means that the feasible region is empty. There are no points which satisfy all of the constraints. Here's a simple example of an infeasible LP. We have two variables, x and y, and objective function is to maximize 5x minus 7y. And this is subject to the following constraints, x plus y is at most one, 3x plus 2y is at least six, and x and y are non-negative. Now let's take a look at the feasible region for this LP.

LP2 Geometry - 435 - Infeasible Example
Now here's our example from the previous slide. Here's the illustration of the feasible region. First off, we have the non-negative constraints defined by the axes. So in the upper right quadrant, the first constraint says that x plus y is that most one. That's illustrated by this line, and we're looking at the points on this side of this line. The next constraint says that 3x plus 2y is at least six. That's defined by this line, and we're considering this half space. Now notice that the two half spaces defined by these two constraints don't intersect. So there are no points x, y satisfying both these constraints while maintaining the non-negativity constraint. Hence, the feasible region is empty. So there are no points satisfying these constraints. So this objective function, this LP, is undefined. Now notice that infeasibility had nothing to do with the objective function. Regardless of the objective function for these constraints, the LP will be infeasible.

LP2 Geometry - 436 - Unbounded Example
Now les take a look at what it means for an LP to be unbounded. This means that the optimal value of the objective function is arbitrarily large. Let's take a look at an example of an unbounded LP. Let's look at the following LP. We have two variables x and y, we're maximizing x plus y subject to the following constraints, x minus y is at most 1, x plus 5y is at least three. And then we have the non negativity constraints. Let's take a look at the feasible region for this LP. Here's the feasible region based on the non-negativity constraints. We're in the upper right quadrant. The first constraint x minus y is at most one, defines this half space and x plus 5y is at least three, defines this upper half space. And this colored region are the feasible points. Now the picture is truncated at y equals five but actually this feasible region is unbounded set. Now let's look at this objective function x plus y. What we've illustrated here are the heat map and the level set for this object to function x plus y. So the blue regions are the lower values of this objective function and as we get up to the red regions, we get the higher values of this objective function. So notice the objective function is increasing as we move up. And we've also illustrated the level sets so we look at x plus y equals c for different values of c. So here c equals two, c equals four and as we keep going up, we get larger values of this objective function. And once again, the picture is truncated y equals five, but the feasible set is unbounded. So we get unbounded values of feasible points x, y. For unbounded values of c. Now one important point is the fact that this LP is unbounded, is a function of this objective function. The optimal value for this objective function is achieved in this unbounded region of this feasible region. Now there could be some objective functions where their optimum are achieved at these vertices. Let's take a look at such an example.

LP2 Geometry - 437 - Unbounded to Bounded
So here, again, is the feasible region for this LP and let's change this objective function. Let's maximize 2x-3y. Let's look at the level sets for this objective function in this feasible region. Here's the feasible region once again with the heat map and the level set's mark. So if we look at this line, 2x-3y=c, we achieve c=-12 on this line, and then it increases as we go down. So the optimal value of this LP is going to be achieved at this vertex of the feasible region. So this maximization is defined for this objective function, whereas for the other objective function, the maximization was undefined. So whether an LP is feasible or infeasible simply depends on the constraints, whether an LP is bounded or unbounded depends on the objective function.

LP2 Geometry - 438 - LP Optimum - Part 2
Now, how can we check whether our LP is feasible or infeasible? And if it is feasible, how can we check whether it's unbounded or not? Let's first address how we determine whether it's feasible or not, and later in the lecture, we'll address whether it's unbounded or not. To figure out whether a feasible LP is unbounded or bounded, we're going to have to look at the dual LP. So we'll defer that discussion until after we discuss duality. Let's look now at how we determine whether an LP is infeasible.

LP2 Geometry - 439 - Infeasible
Let's consider an LP in standard form, and we'd like to figure out whether there are some axis which satisfy these constraints. So, this has nothing to do with the objective function. Simply looking at the constraints, is there any point x satisfying all of the constraints? So, is the feasible region empty or not? Let's make a new variable z. Now, X was a vector of size n, so, it consisted of n variables. That's not a vector, that's just a single variable. Now, consider one of the constraints here, a1x1 plus a2x2 and so on, up to anxn is at most little b. Now, is there a solution to this under the non-activity constraint? Well, there may or may not be. But suppose, we take the same constraint and we add in z to the left hand side. We keep the non-negativity constrains for the variables x1 through xn, but z is unrestricted. So, it can take on positive or negative values. In this case, there is always a solution to this new equation. Why? Simply set z small enough, and then, this is trivially satisfied. So, let's think of z as negative infinity though, in fact it's a finite number. It suffices to take a small enough number relative to the numbers a1 through an and little b. So, this constraint is trivially satisfied for a small enough z. Now, the question is, whether there is a solution to this equation, when z is at least zero. If we can find a solution to this new equation, where z is non-negative then, we can drop z and we still have this condition satisfied. So, our original constrain is equivalent to the new constrain with the extra condition that z is at least zero. But if we drop this constraint, that z is non-negative then, there's trivially a solution to this. So, we have a starting point, and then we can run an LP, and we maximize z, and we see whether there's a solution with z as non-negative. So, we just consider one constraint, let's consider all n constraints. So, we look at the new LP which has the constraints Ax plus z is at most b, and x is non-negative, but z is unrestricted. Since z is unrestricted, it's trivial to always find a feasible point. We simply set z to be a small enough number. Now, our goal is to figure out whether there is a solution whereas z is non-negative. So, we add in the objective function maximize z. This new LP is always feasible, and we can run this LP and check whether the optimal value of z is non-negative. If the optimal value is non-negative, then that point, those Xs, if we ignore z, give us a feasible point to the original LP. And if this LP is optimized when z is negative, then we know that the original LP is infeasible. So, this gives us a way to check whether an LP is feasible or not, and it also gives us this feasible point for this LP if it is feasible. So, that'll give us a starting point for the simplex algorithm.

LP3 Duality - 440 - LP Duality
In this lecture, we'll explore the beautiful theory of LP duality. We'll start with a motivating example and then this will lead us into the general formulation of the dual LP. And we'll look at the weak duality theorem and some of its consequences. And finally, we'll look at the strong version of the duality theorem.

LP3 Duality - 441 - Example
Here is one of our earlier examples of an LP. This LP had three variables X_1, X_2, X_3 and four constraints, and the non-negativity constraints. Now this was an example where we illustrate the simplex algorithm on and we found that the optimal value was achieved at this vertex (200, 200, 100). And when you plug in this point into the objective function, the profit, the value of the objective function is 2400. Now suppose somebody simply gives us this point and claims that it's optimal. Is there some way that we can verify that this point is in fact an optimal solution? So what we're going to try to do is we're going to try to upper bound the profit, we're going to try to upper bound this objective function and show that the profit is at most 2400. How are we going to achieve an upper bound on this objective function? Well, we know that these constraints are all satisfied. So we're going to take linear combinations of these constraints and then try to achieve an upper bound on this objective function.

LP3 Duality - 442 - Upper Bound
Here again is the LP that we're considering, and we're interested in this point 200, 200, 100, which has profit. The value of the objective function at this point is 2,400, and we're trying to verify that this is the optimal, the maximum profit that we can achieve. Now consider this vector Y. It's got four coordinates, and we're considering the vector zero, one-third, one, eight-thirds. Let's ignore how we got this vector Y for now, we'll address that later. Let's simply consider this vector for now. Now this vector has four dimensions because we have four constraints in this LP. So this number, these constraints. So X1 is the most 300, is the first constraint, and so on. The fourth constraint is X2 plus 3X3 is at most 500. Now we know any feasible point satisfies all four constraints, so it also satisfies any linear combination of these constraints. So let's take the linear combination defined by Y. So we're going to look at Y1 times the first constraint, Y2 times the second constraint, plus Y3 times the third constraint, and Y4 times the fourth constraint. Let's go ahead and write this out. So this linear combination, this expression is equivalent to the following. As to Y1 times the first constraint. So we get X1Y1 on the left hand side, and the right hand side we get 300 times Y1. Now for the second constraint, X2Y2 on the left hand side, and 200Y2 on the right hand side. Now let's use the third constraint. The left hand side has X1 times Y3 plus 3X2Y3 plus 2X3Y3. And the right hand side has 1000Y3. And finally the fourth constraint gives us X2 times Y4 plus 3X3 times Y4, on the left hand side,.then we get 500Y4, on the right hand side. Now we know that any feasible point X satisfies this inequality for any non-negative Y. If Y happens to have negative coordinates, then we have to worry about flipping the sign. But assuming Y is non-negative, then this inequality is satisfied by any feasible X. Now before we plug in our specific Y, let's collect terms and simplify this expression slightly. What are the X1 terms? Well we have a Y1 and a Y3. Now what are X2 terms? Well we have a Y2, 3Y3 and Y4. And lastly, what are the X3 terms? We have a 2Y3 and 3Y4. That completes the left hand side. The right hand side stays the same. So we have 300Y1 plus 200Y2 plus 1000Y3 plus 500Y4. Now finally, let's plug in the specific Y which we chose. So the coefficient of X1 is zero plus one, so it's one. So we get X1. For X2, we get one third plus eight thirds plus three. So we get 6X2. For X3, we get two plus eight, so we get 10X3. And the right hand side when you plug it in, it computes to 2,400. Now notice that the left hand side is exactly the objective function of the original LP. And we proved that this objective function is at most 2,400. So we've shown that any feasible X, the value of the objective function is at most 2,400. Therefore, the maximum profit that we can achieve is at most 2,400 and we have a point which achieves this profit. Therefore this point is optimal.

LP3 Duality - 443 - Dual LP
How do we find this Y? Or a better question is, what were the conditions that we needed on this Y? What do we need out of this Y? What we wanted that when we plug this Y in, that the left hand side was the objective function. Actually, we just need that it's at least the objective function. If the left hand side would have been X1 plus 7X2 plus 10X3, then, that still would have served as an upper bound on the objective function. Is that would be only bigger, so any upper bound on that new function would serve as an upper bound also on the original objective function. So let's ignore this specific Y. What are the conditions, once again, that we need on this Y? What we need that this left hand side is at least the objective function. So we need that the coefficient for X1 which is Y1 plus Y3, we need to be at least the coefficient of X1 in the objective function. So we need that Y1 plus Y3 is at least one. Similarly, for X2, we need that Y2 plus 3Y3 plus Y4 is at least six. And 2Y3 plus 3y4 is at least 10. Now any Y satisfying these three constraints, when we plug it in, the left hand side will be at least the value of the objective function. So the right hand side will serve as an upper bound on the left hand side and therefore, is also an upper bound on the original objective function. So any Y satisfying these three constraints, gives us, yields an upper bound on the objective function. So we want to get the smallest possible upper bound. So we want to minimize the right hand side and this way we get the smallest possible upper bound. So our objective is to minimize 300y1 plus 200y2 plus 1000y3 plus 500y4. This is the right hand side here. And this is our dual LP. This is in fact a linear program. We're just trying to find the feasible Ys. Those Ys satisfying these three constraints and we want to find the feasible Y which minimizes this objective function. So our original LP had a maximization. Now we have a minimization and our constraints originally were less than or equal to constraints and now they flipped to at least constraints. And notice that all these numbers, these coefficients are related to the original LP. Look, in our new objective function, where do we get 300, 200, 1000 and 500 from? Well these are the right hand sides here. The right hand side there came from the right hand sides of the original constraints. So the right hand sides of the original constraints tell us the coefficients for our new LP. Where did the right hand side for these three constraints come from? Well these came from the coefficients in the objective function. So the coefficients in the original objective function define the right hand sides in this dual LP constraints. And the right hand side in the original LP constraints define the objective function in this dual LP. So there's this sort of symmetry. Finally, where do the left hand side of the constraints come from? Well this first constrain, these are the terms which contain X1. Which constrains contain X1? Well the first constrain and the third constraint contain X1 and therefore, Y1 and Y3 appear in the first constraint. Which constrains contain X2? The second one, the third one and the fourth one. And therefore, in the second constraint, we have Y2, Y3 and Y4 appear because the second, third and fourth contain X2. One last note, notice, we have four constraints in the original LP. We're going to take a linear combination of these four constraints. We want to find out what are the coefficients in that linear combination. So our variables, we're going to have four variables in our new LP, one corresponding to each of the constraints. So there are four constraints. Our new LP will have four variables and our original LP had three variables, and that's going to yield three constraints because we have to look at the coefficients for each of those variables and make sure they're at least the coefficients in the original objective function. So a number of variables in the original LP defines the number of constraints in the new LP, the dual LP. And the number of constraints in the original LP, defines the number of variables in the new dual LP.

LP3 Duality - 444 - Dual LP Example
Let's summarize and recap the previous example. So, this was the original LP. We had three variables and four constraints. Our new LP, the dual LP had four variables: y1, y2, y3, and y4. The objective function was to minimize 300y1 plus 200y2 plus 1000y3 plus 500y4. This was the right hand side of the linear combination of these constraints. First constraints, so that y1 plus y3 is at least one. This gave us a coefficient of at least one on the left hand side. y2 plus 3y3 plus y4 is at least six. We took the constraints containing x2 that the final left hand side and the right hand side was from this coefficient of this objective function in the original LP. The third constraint was 2y3 plus 3y4 is at least 10. And finally we needed that the y's were non-negative. Now, a feasible x here, achieved this profit. So any feasible x gives us a lower bound on the profit that we can achieve. It actually shows an example where you can achieve that profit. Now, in this dual LP, any feasible y gives an upper bound on the profit that we can achieve in the original LP. Now, this new LP is called the dual LP. The original LP in order to give it a name, it's referred to as the primal LP.

LP3 Duality - 445 - General Form
Now, let's look at the general formulation of this Dual LP. Now, the original LP, the primal LP, is of the following form. Where maximizing C transpose X, subject to the constraints, a times x is at most b, and x is at least zero. Now, this LP has N variables, X is a vector of size N, and it has M constraints. Capital A is a matrix of size M by N. It has M rows and N columns. Now, what is a Dual LP? Now, instead of maximizing, we're trying to minimize. We're trying to minimize the profit. We're trying to find the smallest upper bound possible on the profit. We took a linear combination of these constraints, and the right hand side was our objective function. We're trying to minimize the right hand side. The right hand side is defined by this vector b. So, our objective function is going to be minimize_b_transpose_y. Now, the right hand side of our constraints comes from the objective function, the coefficients and the objective function. So, there are constraints. The right hand side is going to be at least C. How do we get the left hand side? We look at these constraints and we check which ones contain x1, the first and the third. What does that correspond to in this matrix A? That corresponds to the first column on the matrix A. The first column tells us which constrains contain x1. The second column tells us which constrains to contain x2. Second, third, and fourth. And therefore, we have a y2, y3, and y4 term. So, instead of looking at the matrix A, we're going to look at A transpose, because we're interested in the columns of A. And we're going to multiply this by Y. So, our constraints are going to be a transpose times Y is at least C. And once again, we have that Y is non-negative. So, given a primal LP in canonical form, this is the dual LP. And whereas the primal LP has M constraints, the dual LP has M variables. And the primal LP has N variables, the dual LP will have N constrains. One last note, in order to apply this formulation of the dual LP, we need that the primal LP is in canonical form. So, all the constrains have to be less than or equal to, and the objective function has to be a maximization problem. But, we discussed earlier how to convert any LP into canonical.

LP3 Duality - 446 - Dual of Dual
Let's do a quick sanity check. Let's take a primal LP. So take a primal LP in canonical form, maximize C transpose X, Ax is at most B, X is at least zero. Now let's take the dual of this LP and then let's take the dual of that dual LP. What should the final result be? Well, it should be equivalent to the original LP. The dual of the dual should be the primal. So let's check that as true. Let's take the dual of this LP. This gives us minimize B transpose Y. Our variables now are Y. The constraints are that A transpose Y is at least C, and Y is nonnegative. Now we want to take the dual of this LP. First, we have to convert it into a canonical form. Now in order to convert this LP into canonical or standard form, it changes min into a max. So we multiply it by negative 1. This minimization is equivalent to maximizing negative B transpose Y. We do a similar trick to convert this greater than or equal to constraint into a less than or equal to constraint. It becomes negative A transpose Y is at most negative C and we keep the last non-negativity constraint. Now let's take the dual of this LP. Let's denote the new variables as Z. The dual will be a minimization problem. The objective function will be defined by this vector, negative C. So we get negative C transpose times Z as the objective function. The constraints will be defined by the transpose of this, so it will simply be negative A times Z is at least negative B. And Z is at least 0. Now if you convert this into canonical or standard form, the objective function becomes maximize C transpose Z which is the same as the original primal LP with X replaced by Z. And this constraint becomes A times Z is at most B. Which is again, is equivalent to the original constraint with X replaced by Z. So these two LPs are equivalent. So if you take an LP and you take it's dual, and then you take it dual once again, you get back the original LP. The primal LP.

LP3 Duality - 447 - Quiz Dual LP Question
Let's get some practice writing the dual LP. Here's a simple example LP with three variables: x_1, x_2, and x_3. The objective function is to maximize five times x_1 minus 7x_2 plus 2x_3. There are two constraints. The first constraint is that x_1 plus x_2 minus 4x_3 is in most one. The second constraint is that 2x_1 minus x_2 is at least three. And, finally, there's the non-negativity constraint x_1, x_2, and x_3 are at least zero. For this primal LP, what is its dual LP? Now, let's break it up into several parts, then do it more slowly. The first question is: how many variables are in this dual LP? And the second question is: how many constraints in this dual LP? And by constraints we're not counting the non-negativity constraints, so this one has two constraints. So, let's first address these two questions and then we'll address the details of the dual LP.

LP3 Duality - 449 - Quiz Dual LP Constraints Question
Let's look at the solution to our earlier quiz before detailing the dual LP. Since the primal LP has three variables, x1, x2, x3, the dual LP has a constraint for each variable in the primal LP. So, there are three constraints in the dual LP. And since the primal LP has two constraints, the dual LP has a variable for each constraint. So there are two variables in the dual LP. Let's denote these variables as y1 and y2. Now, since the primal LP is maximizing, in the dual LP, we're trying to find the smallest possible upper bound. So we have a minimization problem. Now the objective function is going to be a linear function of y1 and y2. The question is, what are the coefficients of this objective function? Let's denote these coefficients as d1 and d2, and we're trying to find this vector d. Now there are three constraints. Each one is a linear function of y1 and y2. And let's denote the left-hand side, the coefficients of the left-hand side of the constraints by matrix e. So for the first constraint this will be e1 1 and this will be e1 2. Second constraint will be e 2 1 times y1 plus e2 2 times y2. And for the third constraint we'll have e3 1 and e3 2. Now in the dual LP, in standard form, these constraints will all be at least constraints, and let's denote the right-hand side of these constraints by the vector f. So this will be f1, f2, and f3. And the next quiz is to detail this dual LP. To do that we need to specify this vector d, d1 and d2. We need to specify this matrix e. And finally, we need to specify the right-hand sides of the constraints, which is the vector f. And, of course, there are also the non-negative constraints, y1 and y2 are at least zero. So why don't you go ahead and specify this vector, and this vector, and this matrix, and then we'll show the solution video, which is detailing this dual LP.

LP3 Duality - 450 - Quiz Dual LP Constraints Solution
To define the Dual LP, we first have to convert the primal LP into canonical or standard form. This conversion is just for convenience. It's easier to remember how to convert an LP in standard form into its Dual LP. When we have an LP like this, with these mixed inequalities, it's harder to remember how to convert into dual LP. Now to convert this into canonical or standard form, we need to flip this inequality. We flip it by multiplying both sides by negative one, and it becomes minus two times x_1, plus x_2, isn't most negative three. This is equivalent to the original constraint. To get the coefficients for the object function in a dual LP, we look at the right-hand side of the constrains in the primer LP. The right-hand sides are one, and negative three. So this vector d is one, negative three. And the objective function is minimizing Y_1, plus negative three times Y_2. This matrix E will be the transpose of the constraint matrix E over here. The first row in E will correspond to the column for x in the primal. The column for x is one, negative two. So the first row will be one, negative two. So the left-hand side of the first constraint is Y_1, minus 2Y_2. Second row is defined by x_2, which is one, one. So the left-hand side is Y_1, plus Y_2. The third row is defined by x_3, which is negative four, zero. So the left-hand side of the third constraint is negative 4_Y_1. For the right-hand side of the constraints in the dual LP, we look at the objective function in the primal LP. The coefficients are five, negative seven, two. So this vector f is five, negative seven, two. And the right- hand sides are five, negative seven, tow. So the first constraint is Y_1, minus 2Y_2 is at least five. The second constraint is Y_1, plus Y_2 is at least negative seven. The last constraint is negative 4Y_1 is at least two. And this defines a Dual LP.

LP3 Duality - 451 - Weak Duality
Now, we can state that weak duality theorem. This is an immediate consequence of how we formulated the dual LP. Take a feasible point for the primal LP. The value of the objective function in this case is C transpose X. And recall, we're trying to find the X which maximizes this objective function. Now, let's look at the dual LP. Take a feasible point Y for the dual LP. What's the value of the objective function for this dual LP? Well, that's B transpose Y. Now what we need to know is that any feasible Y gives an upper bound on the objective function of the primal LP. So this B transpose Y is an upper bound on the objective function. So we know that C transpose X, this is a value of the objective function that we can obtain at the point X. This is upper bounded by the value of the objective function at Y which is B transpose Y. Y gives an upper bound on the objective function of the primal LP. And this is the value of an primal LP objective function that we can obtain. So the left hand side is the primal LP objective function, and the right hand side is a dual LP objective function. Since any feasible point gives an upper bound on the primal LP objective function, we have this week duality theory.

LP3 Duality - 452 - Matching Values
Now, in our original example of the dual LP, we use a specific primal LP where we had run simplex, we found the point 200, 200, 100 and had profit the value of the objective function was 2400. Now we were able to find a feasible Y where it's value of the objective function in dual LP was also 2400. So that established, that proved that the optimal value was at most 2400 and since we achieved 2400 at this point X therefore, it was an optimal point. And in general, we have the following corollary of this theorem, the weak duality theorem. If we find a feasible point X for the primal LP and a feasible Y for the dual LP where their values of the objective functions match. So C transpose X equals B transpose Y. So, we have equality here which is what we achieved in that early example. Then what we can conclude from this weak duality theorem, is that X and Y are both optimal. X is an optimal point for the primal LP. It maximizes the objective function of the primal LP and Y is an optimal point for the dual LP. It minimizes the objective function of the dual LP. So, if we can find a feasible point X in the primal LP and a feasible point Y in the dual LP, where their objective functions match, then these two points are optimal. Now, does there always exist a point X in the primal LP and the point Y in the dual LP where we have equality? Yes, there does. That's the strong duality theorem. Under certain conditions, we have to have an optimal value for the primal LP and an optimal value for the dual LP. So, we need both the primal and the dual to be feasible and bounded. So, let's first discuss these special cases. The cases where one of them is infeasible or unbounded.

LP3 Duality - 453 - Unbounded LP
Now let's take a look at another corollary, the weak duality theorem. Here's the second corollary. Suppose the primal LP is unbounded so the left hand side here will be infinite. What does that imply about the dual LP? Well, the right hand side has to be bigger than infinite. There's no way to achieve that. So the dual LP must be infeasible. So if the primal is unbounded, then the dual LP is infeasible and vice versa. If the dual is unbounded, then the primal is infeasible because the dual LP is minimizing the right hand side. So if that's negative infinity, then this is undefined. So the primal must be infeasible. Now notice this is not an equivalence. This is not an if and only if statement. It can be the case that the primal is infeasible and the dual is infeasible. So the implication has only one direction. If we know that the dual is infeasible, then the primal LP might be unbounded or it might be infeasible. And you'll do as a homework exercise to construct an example where the primal is infeasible and the dual is also infeasible. But now this implication is important. It says that if the primal LP is unbounded, then the dual is infeasible. Now let's use that.

LP3 Duality - 454 - Check Unbounded
Now let's go back to one of our questions from earlier. Can we check whether an LP is unbounded? So consider an LP in canonical form, maximize C transpose X, AX is at most B, and X is non-negative. Now, can we check whether this LP is infeasible or not? Well, we saw how to do that. We make a new LP by adding another variable Z, and we maximize Z and we change the constraint to AX plus Z is at most B, and we check whether there's a solution where Z is non-negative. If there is such a non-negative Z which satisfies this new LP, then that gives us a feasible point of the original LP. So we saw how to do this, and we saw how to obtain a feasible point of this LP, if it is feasible. Now, how do we check whether this LP is unbounded or not? Well, what do we know? We know if it's unbounded then the dual LP will be infeasible. So we could simply check the dual LP and check whether it's feasible or not. Well, if we simply check the dual LP and we see it's infeasible, then what do we know about the primal? Well, if the dual LP is infeasible, then the primal is either unbounded or infeasible. We don't know which of these two is the case because the weak duality theorem just tells us that if the primal is unbounded, then the dual is infeasible. But it's non-equivalence. So how do we check whether this original LP is unbounded or not? Well, first we check whether it's feasible or not, whether there is a feasible point for this primal LP. Assuming there is at least one feasible point so the feasible region is non-empty. So it is not infeasible. And then we check whether the dual LP is infeasible. So now we know that the primal is not infeasible. So it's true to this case. So now if the dual LP is infeasible, then we know the primal is unbounded. So we simply have two feasibility questions. We check whether the primal is feasible or not. And then we check whether the dual is infeasible or not.

LP3 Duality - 455 - Weak Duality - Part 2
Now to recap, our weak duality theorem gave us two nice consequences. The second consequence that we obtain was that if the primal LP was unbounded then the dual is infeasible. This gave us a way of checking whether an LP is unbounded or not. We simply first check whether the primal LP is infeasible or not. If it's not infeasible so the feasible region is non-empty then to check whether it's unbounded, we checked the dual LP and see whether it's infeasible or not. Now, the original consequence was that if we find a feasible point for the primal LP and a feasible point for the dual LP and if the value of the objective function for these two points match up, we have equality then these two points that we obtained X and Y are optimal. So, X achieves the optimal value for the primal LP object function and Y achieves the optimal value for the dual LP objective function. Now for this specific example we consider, we were able to obtain a Y which achieved this equality. Now, the question is, is there always a Y and an X which achieve equality here? And that's the case if the primal LP and the dual LP are both bounded and feasible.

LP3 Duality - 456 - Strong Duality
The strong duality theorem states that, the primal LP is feasible and bounded. So it is not infeasible and it is not unbounded. So the feasible region is non-empty, and there is an optimal solution. If this is the case for the primal LP, then the dual LP has a similar scenario. It's also feasible and bounded and vice versa. Now, if the primal LP is feasible and bounded, that means there exists an optimal solution. So an equivalent formulation of this statement is the following. The primal has an optimal point X star, if and only if the dual LP has an optimal point Y star. So if there is an optimal point for one, then there's an optimal point for the other. Now, what do we know about these optimal points? When we saw in our earlier example that these objective functions match up. And that turns out to always be the case. The value of this object function at X star, equals the value of this objective function at Y star. So C transpose X star equals B transpose Y star. So for every LP given the optimal solution X star, there is always a certificate from the dual LP which certifies that that is in fact an optimal solution. Now, one of the nice implications of this strong duality theorem is that if we write the LP for the max flow problem, then the value of the objective function for that LP is the size of the max flow from S to T. Now, if we write the dual LP it turns out that what we get is the capacity of the min st-cut. Now, since these are dual LPs of each other, then we know that we have equality. So the size of the maximum flow equals the capacity of the min st-cut. So this max flow min theorem which we saw earlier in the max flow section, we can also prove it using strong duality theorem.

LP4 Max SAT Approximation - 457 - Max-SAT
We've seen now, several times, assess viability problem. We input once again is the Boolean formula in conjunctive normal form with N variables and M clauses. And the output from the SAT problem is an assignment, a true false assignment, for the N variables, so that the formula evaluates true or we simply output no, if there's no such satisfying assignment. Now, as we know the SAT problem is NP-complete. Hence, we can't hope to find a polynomial time algorithm for the SAT problem. Now, this is a search problem. Let's look at the optimization version. It's the Max-SAT problem. Now, in the Max-SAT problem the input is the same. It's a boolean formula in conjunctive normal form. And once again will use N to denote the number of variables and M for the number of clauses. The differences in terms of the output. Even if there's no satisfying assignment, we're still going to output an assignment. And here, we're going to output an assignment which maximizes the number of satisfied clauses. In the optimization version of the SAT problem, we want to find an assignment which satisfies many clauses as possible. Now, this is still a hard problem. In particular the Max-SAT problem is NP-hard. It's no longer a search problem so it is no longer in the class NP, because we have no way of verifying that the number of clauses satisfied is maximum. By clearly, this Max-SAT problem is at least as hard as the SAT problem. It's straightforward to reduce SAT to Max-SAT and therefore Max-SAT is NP-hard. So once again, we can't hope to solve the Max-SAT problem in polynomial time. Instead we're going to aim to approximate the Max-SAT problem and to do that we're going to use linear programming.

LP4 Max SAT Approximation - 458 - Approximate Max-SAT
Let's look in detail at what we mean by an approximation algorithm for the Max-SAT problem. Consider an input to the Max-SAT, a formula F where it has M clauses and let's let M* denote the maximum number of satisfied clauses. So if we look over all assignments to the variables in F, the maximum number of clauses satisfied by any of those assignments is M* and this is of course dependent on F so let's denote as M* of F. Now let's simplify the notation a little bit and let's drop this of F part and let's simply denote it as M*, which is the solution to the Max-SAT problem. Now clearly M* is at most M. If M* equals M, then that means formula F is satisfiable. Now, we're going to construct an algorithm which we will denote it as to capital A. It's going to take a formula F as input and it's going output L on this input F and to be precise actually, its going to output an assignment which satisfies L clauses of F. So, L is the number of clauses satisfied by these assignments outputted by A. Now how does L compare to the optimal solution M*? Where in the first instance, we're going to guarantee that L is at least M* over 2. Even though we don't know M*, we're going to guarantee that the output of our algorithm is at least within a factor of one half of the optimal number of satisfied clauses. And if this holds for every formula F, then this is a one half approximation algorithm. And then later in this lecture we're going to prove one half to three quarters and we're going to get to three quarters approximation algorithm for the Max-SAT.

LP4 Max SAT Approximation - 459 - Outline
And we're going to start by looking at a very simple randomized algorithm, and we'll show that this simple algorithm achieves a one-half approximation algorithm for Max-SAT. After that, we're going to look at an LP-based algorithm and we're going to prove that this new LP-based algorithm achieves a 1-1_over_E approximation factor, so to improve the previous algorithm by a little bit. Finally, we're going to look at an algorithm which is a combination of these two algorithms, and this combined algorithm is going to achieve a three-quarters approximation. So let's dive into the simple scheme.

LP4 Max SAT Approximation - 460 - Simple Scheme
Now let's consider some input f to the max stack problem. And once again, let's denote the variables as X_1 through X_n. So there are n variables. And there are m clauses which we'll denote as C_1 through C_M. Now we're going to do the simplest possible scheme for making a true false assignment for these n variables. We're not even going to look at the formula f. We're simply going to assign these variables randomly to true and false. For each variable X_i, independently of the other ones, we're going to set X_i to be true with probability half, and false with probably half. So we simply flip a fair coin and if heads comes up, we set X_i to be true and if tails comes up, we said X_i to be false. And we do that for all n variables. Now how does this random assignment perform? We want to look at the expected performance of this random assignment. How do we measure the performance of the algorithm? Well, we measure the performance by looking at the number of clauses satisfied. Therefore, let W denote the number of satisfied clauses by this random assignment. Now since the assignment is random, W is also a random variable. Now since W is a random variable, we want to look at its expected value which in some sense is the average value of W. In particular, the expectation is the average value of W weighted by its probabilities. Now what are the possible values for W? Well, the minimum number of clauses we can satisfy is of course zero and the maximum number is at most m. So let's sum over the possible value. Let L denote the number of satisfied clauses, and L is going to vary between 0 and m. And then we get this value L and we have to weight it by the probability that the random variable W is equal to L. This is simply the definition of the expectation of a random variable. Now the expectation in this form is quite difficult to analyze. Why? Because whether a particular clause is satisfied or not is related to whether another clause is satisfied or not. Because they might have variables in common. Now what we're going to do is we're going to break up this random variable, which is the total number of satisfied clauses, into a clause by clause quantity. And in this manner we're going to be able to analyze each clause in isolation, independent of what happens for the other clauses. And in that way it's going to be straightforward to analyze the expected performance of the algorithm. So let's look at how exactly we do that.

LP4 Max SAT Approximation - 461 - Expectation
Once again capital W denotes a number of satisfied clauses for a random assignment. Each variable is assigned true or false with probability one half. Now, we want to look at a simpler quantity. In particular, we want to look at one clause in isolation. So, consider the J's clause. C sub J, and let's make a new random variable W sub J. Now, we're just talking about one clause, so either that clause is satisfied or not. Therefore, this new random variable is going to take either value 1 or 0. It takes value 1 if the clause CJ is satisfied and it takes value 0 if it's not satisfied. Now, what happens if we sum this quantity over all clauses? So, we look at the sum for J going from 1 to M of W sub J, where we get a 1 for every clause which is satisfied and 0 if it's not satisfied. So, this gives us a total count of the number of satisfied clauses. Therefore, it's equal to capital W. Now recall, our original goal was to analyze the expected of performance of the algorithm. So we want to look at the expectation of capital W, expected number of satisfied clauses. Now, we can plug in this identity, so we can replace capital W by this sum. We have the expectation of the sum over J of W sub J and now recall the new rarety or expectation. So, we can take the sum outside. The expectation of two random variables X and Y. Expectation of X plus Y is the same as expectation of X plus the expectation of Y. So, now we have that the expectation of W equals the sum over J of the expectation of WJ. Now, we can analyze this quantity expectation of WJ. Notice, this just depends on the clause CJ. It doesn't have anything to do with the other clauses. So, now we can focus on this clause in isolation. We don't have to look at any of the other clauses.

LP4 Max SAT Approximation - 462 - Analysis
We're looking at this random variable W sub J, which takes value 1 or 0 depending on whether the clause Cj is satisfied or not by the random assignment. We want to analyze the expectation of W sub J. So we take the two possible values weighted by the probability of achieving that value. So, we have value 1 times the probability that Wj equals 1 plus 0 times the probability Wj equals zero. That clearly, this term goes away and we're left with the probability Wj equals 1. This is the beauty of considering a random variable which takes value 0 or 1. The expectation is simply the probability that that random variable equals value 1. Now, considering a sample of clause Cj, suppose this x1, or x2 bar, or x3 bar, or x4 and so on up to xK. So the clause has size K. Now, in order to achieve value 1, the clause has to be satisfied. Let's look at the compliment. What's the probability the clause is not satisfied? Now, in order to not satisfy this clause, we have to set x1 to false, x2 to true, x3 to true and so on. That's exactly one setting of these variables, these K variables, so that this clause is not satisfied. The chance x1 is set to false is one-half. The chance that x1 is set to false and x2 is set to true and x3 is set to true and so on, so these K variables are set exactly to this one assignment. The probability of that one assignment is 2 to the minus K. That's the probability that this clause is not satisfied. What's the probability that is satisfied is a compliment of that, so it's 1 minus that. The probability that Wj equals 1 is equal to 1 minus 2 to the minus K. Now, notice something interesting, this gets better as K grows. The worst case is when K equals 1, it's a unit clause. So, suppose it's just x1, then what's the probability this clause does not satisfy is one-half and the probability it is satisfied is one-half. So, this quantity is always at least one-half. Since K is

LP4 Max SAT Approximation - 463 - Finishing Off
Now, once again, we wanted to look at the expected number of satisfied clauses for this random assignment. We let W denote the random variable for the number of satisfied clauses, and we let WJ denote whether clause CJ is satisfied or not. And then this simplified to a sum over J from 1 to M, of whether the expectation of whether clause CJ is satisfied or not. And then we just prove that this expectation, the probability that clause CJ is satisfied, is at least 1/2. This is at least 1/2 for each of these M clauses. Therefore, this whole sum is at least M over 2. So what we've shown is a randomized algorithm, which achieves a one half approximation factor, though it's simply the expected performance of this algorithm. Now, it turns out we can de-randomize this algorithm. So we can find it's terministic algorithm which is guaranteed to find a 1/2 approximation, so the number of satisfied causes. And this uses the method of conditional expectations. It's quite simple, so let me quickly give you the idea of how it works. So we're going to run through the variables one by one, so let's let I go from 1 to N. Now we're going to try the two possible settings for XI, True or false, and we're going to compute the expected performance for each. So given this setting for XI, and given a fixed setting for X1 trough XI-1. So the early ones are fixed, and now we're trying both possible settings for XI. We compute the expected performance for a random assignment for the remaining variables. XI+1 to XN. Now this expected performance is quite easy to compute based on the analysis approach we just did. And what we're going to do is we're going to take the better of these two assignments, and then we're going to fix it, and then we're going to move on to the next variable. Now one interesting feature is that we're not saying something about the optimal number of satisfied clauses. For instance, our formula F might have 12 clauses, and maybe the maximum number of clauses that can be satisfied simultaneously is maybe 10. So M* in this instance would be 10, and M would be 12. Now, what we're proving is that a random assignment satisfies at least six of the clauses. Now since sets within a factor of 2 of the total number of clauses, therefore we're within half approximation of the maximum number of satisfied clauses. And actually, what was shown is that every formula has an assignment which satisfies at least half of the clauses. Intuitively, if the average is at least M/2, there must be at least one setting which achieves the average.

LP4 Max SAT Approximation - 464 - Ek-SAT
There's one important point that we have to make about the performance of the algorithm which will be useful later. Instead of max-SAT, let's consider max-Ek-SAT. So every clause has size exactly K. For instance, let's consider K=3, and suppose every clause in our input formula f had size exactly 3. Now in this case what's the probability that a particular clause C_j is satisfied? Well, there's one setting of the three literals which appear in this clause so that this clause is not satisfied. The probability of that assignment is one-eighth. Therefore, the probability that is satisfied is exactly seven-eighths, and therefore for the special case of max-E3-SAT, we achieve a seven-eighths approximation algorithm. And what if instead of size 3, every clause had size exactly K, then the probability that a specific clause is not satisfied is two to the minus K. So the probability is satisfied is one minus SAT, and therefore we achieve a one minus two minus approximation algorithm for max-Ek-SAT. We're going to use later that this algorithm works well when all the clauses are large. And now we're going to make a LP based algorithm which works well when the clauses are small. Now, one interesting tidbit this seven-eighths approximation algorithm for max-E3-SAT is the best possible. Hovstad proved that it's NP hard to do any better than seven-eighths for this case. If we achieve an algorithm which has guaranteed performance seven-eighths plus epsilon for any epsilon. For instance, if we achieve 0.88 approximation algorithm, then that implies that P=NP. So this very simple naive algorithm is the best possible when all the causes are of size exactly 3. Thus the hard case is when the formula has varying size clauses, has some unit clauses, some clause of size two and some clauses of size three and so on. But if all the clauses are of the same size and they happen to be of size three, then we can achieve the best possible algorithm by just a random assignment.

LP4 Max SAT Approximation - 465 - Integer Programming
Our second approach, we'll use Linear programming to get an approximation algorithm for Max-SAT. To do that, we'll use a stronger form of Linear programming known as Integer Linear Programming. Before we get into that, first, let's recall the general form of Linear Programming. Is a linear programming canonical form? For the variable vector X, where maximizing C transpose X, subject to the following constraints. Ax  b, and X is non-negative. This is a canonical form for Linear program. Now, in an Integer Linear Program, so in ILP, it has the same canonical form with one additional constraint, X is constrained to be in Z to the N, where Z are the integers. So if you think of the vector X of size N, this constraint is saying that each Xi is integer value. So what's going on geometrically? In the linear program, we had a feasible region, this convex set, define by these constraints, and we're trying to find the best real numbered point X in that feasible set, so that we maximize this objective function, but we consider all real numbered points in that set X. Now we're placing this grid, this N dimensional grid, and we're only looking at the grid points contained in that feasible set. Those are the only feasible points now. We want to find the best grid point which maximizes this objective function. Linear program had a nice property that there always is a vertex of the feasible region which is an optimal point, which maximizes this objective function. We no longer have such a property for Integer Linear Programming. And whereas linear programming is, polynomial time solvable, so it lies in the class P. In contrast, integer linear programming is NP-hard, and we're going to see that right now. We're going to see how to reduce Max-SAT to Integer Linear Programming. In fact, many of the NP complete problems that we've seen so far, such as vertex cover, independent set, and so on are easy to reduce to Integer Linear Programming. So it's quite powerful technique. Well since, ILP, Integer Lienear Programming is NP-hard, we can expect to solve it in polynomial time. So what's our game plan? Well first, we're going to see how to reduce SAT or Max-SAT to Integer Linear Programming, then we're going to look at the Linear Programming relaxation. So we're going to ignore this one constraint, this integer value constraint. So we're going to look at the best real number point X. Of course, the objective function might go up. We're going to use this real number point X which is the optimal solution to the Linear Program, to find an integer point which is nearby. That's going to give us a feasible solution to the Integer Linear Program, and then we'll see how far away it is from the optimal solution. And that will give us our approximation algorithm to the Max-SAT

LP4 Max SAT Approximation - 466 - NP-Hard
We're going to prove now, that integer linear programming is NP-hard. To do that we're going to reduce the Max-SAT problem to integer linear program. Consider an input f for the Max-SAT problem and say that f has N variables and M constraints. In the integer linear program that we create, what are the variables going to be? Well, for each variable in the formula f, we can add a variable Y_i to the integer linear program that we create. And for each clause C_j, we're going to add a variable Z_j. So our initial linear program is going to have N + M variables and we're going to add the constraints that the Y_i's are between zero and one, and also each Z_j is between zero and one. And since these variables are constrained to be integer value, they'll either receive value one or zero. It can't receive any fractional value. Now intuitively what's going on? These Y_is are going to correspond to whether this variable X_i is set to True or false. So, Y_i equals one, corresponds to X_i being true. X_i being false corresponds to Y_i being zero. Z_j is going to correspond to whether this clause is satisfied or not. So it's going to take value one, if this clause is satisfied by the literals appearing in it and it's gonna take value zero, if this clause is not satisfied. That's the intuition.

LP4 Max SAT Approximation - 467 - Clauses
We're going to have a constraint for every clause. Let's take an example clause to get some idea of what we want to achieve. Consider this clause X5, or X3, or X6. We made all the literals positive in order to make it a simple example. Now, what do we want from our constraint? What we want that if these variables are set to false. Now, setting X5 to false is going to correspond to setting Y5 to 0, and Y3 to 0. And finally, Y6 to 0. So if these three variables are set to 0, which are going to correspond to setting these variables to false, then this clause is not satisfied, then we want the variable for this clause to be value 0. So when this clause, this J clause is not satisfied, we want these variables ZJ to be constrained to be value 0. What if it's another setting? Well, then this clause is satisfied. In this case, ZJ will be value 0 or 1. We can't force it to be value 1, but what we can do is we can try to maximize these ZJ's, that'll be our objective function, and then the optimal point will take value 1. So if it can achieve value 1, then we will. In summary, the constraint we need to add is that if all three of these variables are 0, then ZJ is 0. How do we achieve that? We say that ZJ is at most Y5+Y3+Y6. So if these three variables are 0, then ZJ must be 0. Now if at least one of these is 1, then ZJ can takes values 0 or 1. Recall that these variables are constrained between 0 and 1, and also these Y's are constrained to be 0 or 1.

LP4 Max SAT Approximation - 468 - Another Clause
Take a look at another clause that has some positive and negative literals. Consider this clause of size four: X1 bar, or X3, or X2, or X5 bar. Now, we want, that if X1 is set to true, X3 is false, X2 is false, and X5 is true which is going to correspond to Y1 being one, Y3 and Y2 being value zero and Y5 being value one. Then, we want that the variable Z, for this clause, let's say it's clause J, is forced to take value zero which corresponds to this clause being unsatisfied. Well, previously when we had just had positive form of these literals, then we looked at the sum of the Y's appearing. Now, we're going to look at the complement for these negative literals. So, we're going to look at one minus Y1 plus Y3 plus Y2 because these appear in positive form. Plus one minus Y5, because it appears as a negative form, and we're going to constrain ZJ to be at most this quantity. So, if this setting is true, then ZJ must be zero. For any other setting of these four Y's, then ZJ can be zero or one. And in general, for clause CJ, we have to consider which literals are positive form and which literals are in negative form. So, let's use CJ plus and CJ superscript minus. CJ plus will of course be the positive literals in the Jth clause, in this case, it's X2 and X3 and CJ minus will be the negative literals, in this case X1 and X5.

LP4 Max SAT Approximation - 469 - Reduction
Now, we can look at general reduction. Consider Max-SAT input little f. Let's define the ILP. Our goal is to maximize the number of satisfied clauses, so we have a maximization problem. For each clause we have a variable Zj, which denotes whether the clause is satisfied or not. It's going to take value 1 if the clause is satisfied, and 0 if it's unsatisfied. So since we want to maximize the number of satisfied clauses, that means we want to maximize the number of clauses where the variables Zj equals 1. This sum, will be the number of satisfied clauses. Now what are the constraints? Recall we have a variable for each variable in the formula. And we also have a variable in the ILP for each clause in the formula. So, let's go over the variables in the formula first, so for i going from 1 to n, we're going to constraint Yi to be between 0 and 1. And since this is integer value, it takes value 0 or 1. Now, for each clause, similar constraint Zj takes value 0 or 1, and then we have the constraint for the clause that we just reviewed. So for Clause Zj, we look at the positive literals. So for each variable which appears in this clause in the positive form, the bad case is when that corresponding variable Yi is set to 0. That means that literal is not satisfied. Similarly for each variable which appears in the negative literal it's unsatisfied, when Yi takes value 1, which means 1 minus Yi has value 0. And we're going to impose this as an upper bound on Zj. So if all the literals in this clause are unsatisfied then Zj is forced to take value 0, so this clause is unsatisfied. And if at least one literal is satisfied and then Zj can takes value 0 or 1. And since we're maximizing, Zj will take value 1 if it can. Finally, we have the constraint that these variables, the Yi's and the Zj's are integer value. So they lie on this N dimensional grid. Actually in this case, we're in N plus M dimensions. And that defines our reduction from Max-SAT to integer linear programming. In fact it's equivalent, so this integer linear program is equivalent to the original Max-SAT problem on this in-

LP4 Max SAT Approximation - 470 - LP Relaxation
Take this ILP, this interlinear program that we just defined, and consider an optimal point for this ILP. Of course we don't know how to find this optimal point in polynomial time, and we're not in fact finding it. We're just considering this optimal point for the purposes of the proof. So let Y-star and Z-star denote the optimal point for this ILP. Now what's the value of the objective function at the optimal point? Where the value of the objective function is simply the sum of the Z-stars. What do we know about the value of the objective function at this optimal point? Well it equals M-star, what is M-star? M-star is the maximum number of satisfied clauses in the original formula F. M-star is what we're trying to find and we can't solve this ILP in polynomial time, but we can solve any linear program in polynomial time. Can we convert this ILP into a similar LP? While this last constraint is what makes this an ILP so let's drop this last constraint. So, now the feasible points are no longer constrained to be integer value and then this changes from an ILP to a linear program this is simply a linear program now. We can solve this linear program in polynomial time, and let's denote the solution to this linear program instead of by Y-star and Z-star, let's put the-hat there. So, Y-hat-star and Z-hat-star are the optimal solution to this linear program, these points we can find. What's the relation between the objective function for the linear program compared to the ILP objective function at the optimal point? Where the value of the objective function for this LP at this optimal point, is the sum of the Z-hat-stars. How do these compare? Well any feasible point for the ILP is also a feasible point for the LP. Because the ILP constrains it to lie in this grid, the LP considers any point. So, the LP is considered more points so, since the ILP optimal point is also feasible for the LP, and the optimal solution for the LP is at least as good as the optimal solution for the ILP. So, we know that this is the objective function value, is at least this objective function value for the ILP optimal solution. So, we can find upper bound on the maximum number of satisfied clauses in F. Does that do us any good? Well not necessarily, that doesn't help us find M-star in any way. What we're going to do now, is we're going to convert this LP optimal solution into a feasible point for the ILP. It may not be an optimal solution to the ILP but it will be feasible. How do we do this? Will we have to convert this LP solution into a point on the grid. The simplest way to do that is to convert these fractional points these Y-hat-stars to integer points by rounding them to the nearest integer point, and we're going to do this in a probabilistic way. Now once we round these to integer points then we have a valid true false assignment for our set input F. And then we're going to prove that this rounded integer point is not that far, not that much worse than the original LP solution, than the fractional solution. And since this fractional solution is at least as good as the optimal integer solution, then we know that the integer solution that we found is not that far off from the optimal integer solution. So, we're going to take this LP solution around it, that'll get worse than this optimal solution, we will show that this integer solution that we find is not far off from this fractional solution.

LP4 Max SAT Approximation - 471 - Rounding
Once again, we take this LP, and we find the optimal point, and we denote the optimal point by this vector Y ^* and Z ^*. Hats correspond to LP's, so they might be fractional values. Without the hats, it corresponds to integer so our goal is to find an integer point, we'll denote it by YI and ZJ. We drop the stars because it might not be optimal any longer, but we want this integer point that we find by rounding this point. We want this integer point that we find to be close to the optimal integer point. How do we prove that this point that we find, this integer point that we find is close to the optimal integer point because we don't know at this point? Well, we show that this rounding procedure doesn't change the objective function too much. So this integer point that we find is close to this optimal fractional point, this optimal LP solution, and this is at least as good as this. Therefore, if this is close to this one, then it's also close to this point. How do we round from this fractional point to this integer point? We'll call our LP and our integer linear program had the constraint that these variables are constrained between 0 and 1. Therefore, this optimal solution also satisfies these constraints so YI*^ is between 0 and 1. Therefore, we can think of it as like a probability so we're going to round this, so we're going to set YI to be 1 or 0 with probability proportional to this. With probability YI^* is between 0 and 1, and it's a real number. With that probability, we set YI to be one. So if this is 3/4, then with probability 3/4, we set YI to be 1. And with probability 1/4, we set it to be zero. This is known as randomized rounding. Now this actually completes our algorithm. We have assignment now. We have a true false assignment for the variables in the original formula. So we've taken this fractional point, and we round it to an integer point. Notice we don't have around these Z's, we just have to round these Y's. Now, if YI = 1, then we that the variable XI to be true. If YI = 0, we set the variable to be false. So we have a true-false assignment for the X's, and it's a randomized algorithm for setting these X's. So, as we did before, we wanted to look at the expected performance of this randomized algorithm.

LP4 Max SAT Approximation - 472 - Expectation
Once again, we want to look at the expected performance of this randomized algorithm that we just defined. Therefore, we let capital W denote the number of satisfied clauses. This is in the random assignment. The assignment produced by solving the LP and doing randomized rounding. Since the assignment is random, capital W is a random variable so we're going to look at its expectation. Now, to simplify the analysis of the expectation, we're going to do a clause by clause analysis. As before, we consider each clause, so we consider the J clause, and we denote the random variable WJ takes value 1 if this clause CJ is satisfied, and 0 if it's not satisfied. Now if we sum WJ, where J going from one to M, then for each satisfied clause we get a count of 1. So the total count is going to be the total number of satisfied clauses which equals capital W. We can analyze the expectation of capital W, the expected number of satisfied clauses in a manner similar to we did earlier in the lecture. We can use this identity to re-express expectation of W to be the expectation of this sum. And then we can apply linear narrative expectation and take the sum from inside the expectation to outside. We get that the expectation of W equals the sum over J from 1 to M of the expectation of WJ. Now, the expectation of WJ is quite simple because it's a 0-1 random variable. So the expectation of WJ in this case is just the probability that this clause CJ is satisfied. Now we're going to prove that the probability that this clause is satisfied, is at least 1 minus 1 over E times the LP value for this clause. The LP value for this clause is ZJ hat star. In some sense, this is like the probability that this LP satisfies this clause and this rounding procedure afterwards. This assignment that we end up with will satisfy this clause with probability at least 1 minus 1 over E times this original probability. We're going to put this lemma momentarily, but now let's plug that back into this computation. We can take this 1 minus W over E outside of the sum so we have 1 minus 1 over E times the sum from J going from 1 to M of Z-J had star. Now, what does this quantity? This is the value of the objective function for the linear program. What do we know about the linear program versus the integer linear program? Well, the linear program is at least as good as the integer. So this value of the objective function is going to be at least the value of the optimal for the integer linear program. This is going to be at least 1 minus 1 over E times this value for the integer linear program. For the integer linear program it's equal to the max sat so it's equal to the maximum number of clauses satisfied which we denoted by M star. And in conclusion, we can show that the expected performance of this algorithm, the number of satisfied clauses in expectation, is going to be at least the optimal number times 1 minus 1 over E. So we're going to be within 1 minus 1 over E of the optimal number. Therefore, we have a 1 minus 1 over E approximation algorithm which improves upon our one-half approximation algorithm. So we simply have to prove this lemma and then we have our algorithm.

LP4 Max SAT Approximation - 473 - Lemma
Take a look at this lemma that we're trying to prove. Look at a clause Cj and let's say Cj is of size K and let's suppose the clause is X1 or X2 and so on, up to XK. So assuming all the literals are positive form. Let's just do the analysis for this simpler case and the same approach will apply to the general clause. And in fact since we're just looking at this clause in isolation and because of the symmetry, we can always convert this clause to appear in this form. Now, what was the LP constraint for this clause? Let's look at it in terms of the optimal solution to the LP. So variable X1 has this variable in the LP Y1_hat_star. We want that if this is 0, meaning X1 is false and if that's the case for all K of these variables, then we want that this variable Zj_hat_star is 0 in this case. So if all of these literals are unsatisfying, meaning all these variables are set to false then all of these Y's are set to 0, then Zj must be 0 corresponding to this clause being unsatisfied. This is the LP constraint. Now, we're trying to analyze the probability that this clause is satisfied after the randomized rounding procedure. This is of course the same as 1 minus the probability that this clause is unsatisfying. What's the probability that this clause is unsatisfied? Well, what's the probability that all these variables are set to false? So we're going to look at the product over these K variables. The I one is set to false with probably one minus Yi_hat_star. Now, for your call dilemma, we're trying to show that the probability that this clause is satisfied is at least 1 minus 1 over E times this quantity Zj_hat_star. What we know is that the probably it satisfied equals 1 minus this product, of these Yi's. Now we know how to relate the sum of these Yi's to this Zj_star. How do we relate the product to the sum? So we want to relate this product to this sum and then we can relate it to Zj_hat_star. To relate this product to this sum, we're going to use the geometric mean, arithmetic mean, inequality.

LP4 Max SAT Approximation - 474 - AM-GM
Let's take a look at the arithmetic mean, geometric mean and inequality. Let's take a look at its general form. We have K non-negative numbers, W1 through WK. Think of these as weights. We want to look at the mean of these weights. So we can look at the arithmetic mean and the geometric mean. The arithmetic mean takes the sum and divides by the number of terms. The geometric mean takes the product and takes the K through. How do these two things relate? While the arithmetic mean is always at least the geometric mean. And this holds for any non-negative weights. How do we relate this to our problem? Well, we're going to set WI to be one minus YI star hat. And then the arithmetic mean is one over K times the sum of one minus Yi hat star And this is at least the geometric mean, which is the product over I of one minus YI hat star and then we take the K through of the right hand side. Now, alternatively, instead of taking the K through to the right hand side, we can take the Kth power of both sides and we have the following inequality. This is the one we're going to use.

LP4 Max SAT Approximation - 475 - Analysis
Going back to our analysis from earlier, we're looking at the probability that this Jth clause is satisfied. This equals one minus the probability it's unsatisfied, which is the product over I, from one to K of one minus YI hat star. Now, we can apply the arithmetic mean, geometric mean and inequality. Now, we can apply the arithmetic mean, geometric mean and inequality. The arithmetic mean is one over K times the sum of I going from one to K of one minus YI hat star. And then we're raising it to the Kth power, since we dropped that one over Kth root of the geometric mean. Now, the inequality said that the geometric mean is at most the arithmetic mean. But we're doing negative of this so we get that it's at least. Now, we can simplify this a bit. We can do the sum over each of these terms separately. And the first term is a sum over I from one to K of one. That's simply K. So, that divides by this one over K. So, we get one for the first term. So, we get a one for this first one, and the second term becomes one over K times the sum over I from one to K, of YI hat star. And then we get this whole quantity raised to the Kth power still. Now we're in business, because we have this sum over these YI hat stars. Now, if you recall before, we had this constraint from the linear program. It said that if each of these variables was set to false, so each of these Ys was set to zero, then the corresponding ZJ must be zero. Now, this is actually a linear program, so we don't have this integral constrains. So, we simply know that the sum of these Ys is at least this real number Z. But, we still have this inequality, and our goal is to relate this probability that this clause is satisfied to this quantity ZJ hat star. So, we can apply this inequality, plug it in, and we can drop these Ys and get the Z. Now, this is at least. Then we're doing a negative, and then another negative. So, we get that this is at least one minus the quantity, one minus the ZJ hat star over K, all raised to the Kth power. So, we replace the sum over Ys by ZJ hat star. Now, we have to do a bit of calculus to simplify this. What we want to do is we want to move this ZJ outside of this Kth power.

LP4 Max SAT Approximation - 476 - Calculus
Let alpha to note ZJ hat star. What we've shown so far is that the probability that this clause CJ satisfied is at least one minus one minus over alpha over K to the K. Now, once again, we want to get this alpha outside of this power. We want to make it a linear function of alpha. Let's look at this function of alpha. Let's define F of alpha to be this right hand side. And our claim is that F of alpha, so this right hand side here, is at least some constant times alpha. It's a constant in the sense it's independent of alpha. It does depend on K, the size of this clause. Now, what does that constant there? Well, it's going to be similar to this without the alpha. It's going to be one minus quantity one minus one over K raised to the Kth power. Once we prove that, then we can apply that here and we get that this is at least this quantity, which is this constant times alpha. We got the alpha outside of the power so it's now linear in alpha. It's linear in this Z. Now, how do we prove this claim? First off, you take the second derivative and you prove that it's negative. Therefore, this function is concave. Since F alpha is a concave function, it's going to look something like this. We're trying to relate F of alpha, this curve, to this linear function. Let's denote this linear function is beta-alpha, so this quantity here is beta. Let's look at this line beta-alpha. For large values of alpha it might not be bigger. But notice, what values of alpha are we interested in? We're interested in alphas between zero and one, so we only need this claim for values of alpha between zero and one. So, since it's a concave function it's got one peak. If we look at the end points that we're interested in, one and zero, and if we prove that this curve is above this line at these two points, zero and one, then it's also above at every intermediate point. So once we take the second derivative and prove it's concave then it suffices to check the point zero alpha, equals zero and alpha equals one. And if you plug in alpha equals zero, we get zero on both sides, and alpha equals one, it's straightforward to check that that's also true.

LP4 Max SAT Approximation - 477 - Finishing Up
Okay, we're almost done. Actually, we really are done. We just have to summarize. We're looking at the probability that this clause, Cj, is satisfied. This is equal to one minus the product over I, of one minus Yi^*. Then we apply the AM-GM inequality to convert this product into a sum. Then we apply the LP constraint to relate that sum over the Yi's to Zj. And then we got this inequality. We got that this is at least one minus, one minus the Zj^* over K, raise to the k-power. Now what we just proved is that this whole thing is at least, this constant which is a function of K, but independent of Z, one, minus the quantity one minus one over K to the K, times ZJ^*. Okay, now let's simplify this a little bit. If you recall, the Taylor series for E to the -X, it's one minus X, X squared over two, X-cubed over three factorial and a plus-minus, alternating series. Now, what if we want to relate E to the minus X to 1 minus X? Well, the next term is positive. So E to the minus X is going to be bigger. So one minus one over K is at most E to the minus one over K raised to the Kth power. We have 1 over E, and then take minus and then we have it, it's at least. So we have this quantity becomes one minus one over E, times Zj^*, and that's what we wanted to prove. That was the lemma we claimed. And therefore, we have a one minus one over E approximation algorithm. That completes the analysis of this LP-based algorithm.

LP4 Max SAT Approximation - 478 - Summary
Now, let's summarize some important points about this LP-based algorithm. It's a very general approach. We can take a hard problem and oftentimes we can reduce it to integer linear programming. Integer linear programming is very general, very powerful technique. So it's very common that we can reduce these NP-hard problems to integer linear programming. Then we can relax it to a linear programming. We just drop this integrality constraint, and then we can solve it. There are polynomial time solvers, though we can often use simplex algorithm. It's often fast. Then we can take this LP solution and we can round it into some randomized way as we just did for its SAT. And this gives us a feasible point for the integer linear program and hopefully, it's a reasonable heuristic for this solution, the optimal value of this ILP.

LP4 Max SAT Approximation - 479 - Comparison
Now, we've seen two algorithms from Max-SAT. Now, what if we look at Max-SAT on exact K-SAT formulas? So these are formulas were all clauses have size exactly K. And let's look at the performance of these two algorithms we've seen so far as a function of K. So we have the simple algorithm, that's a simple randomized assignment. So each variable is set to true with probably a half, and false with probably a half. And we have this LP based scheme. So we write Max-SAT is an integer in linear program, we relax it to a linear programming, and then we round it. How do these two schemes compare for different values of K? Let's take a look. Let's look at the performance of these two algorithms for K=1. So it's all uni clauses, so it's quite trivial, K=2 and K=3, and in general K. If you recall the simple algorithm, the probability of clauses not satisfied is 2 to the -K. So the probability this satisfied is 1-2-K. So in general for k, it achieves of 1-2-K approximation factor, plugging in K=1, and get one-half. K=2, we get three-quarters, K=3, we get 7 A's. Now the LP, the general form, was quite complicated. We proved for general k that it achieves one minus the quantity, one minus one over k to the kth. Plugging in K=1, we get 1, which is quite good. Simple scheme was quite bad for K=1, but the LP based scheme is quite good. Now K=2, they match, both three-quarters. For K=3, we got one minus two-thirds cubed. If you plug that into a calculator, that's roughly 0.704. So for K, at least three, the simple scheme beats the LP scheme. But for small clauses, the LP scheme is at least as good or even better. The key observation is that if you look at each row, the max in each row, the best of these two schemes for every K is at least three-quarters. So, can we combine these two schemes to achieve a three-quarters approximation? Yes we can.

LP4 Max SAT Approximation - 480 - Best of 2
Here's our combined algorithm. Take our input formula F. We first run the simple randomized algorithm, which assigns each variable true or false independently with probably a half, and we get an assignment which satisfies, let's say M1 clauses of F. So M1 is the number of clauses satisfied by this simple algorithm, this simple randomized algorithm. We also run the LP-based scheme, and we get an assignment, we look at how many clauses are satisfied by that assignment. Let's say it satisfies M2 clauses and whichever of these two is better, we take that assignment. So we look at both of these assignments and we take the better of these two assignments, that's our algorithm. Now if we look at the expected performance of this algorithm, what we're going to achieve is the max of these two. So the performance, the expected performance of this best of two algorithm, is the expectation of the max of M1 and M2. That's what our algorithm is going to achieve. And what we can prove is that this max is at least three-fourths the optima value. Why is that? Well, that follows from the fact that each of these algorithms, the best of these two algorithms for each row on the previous slide, is at least three quarters. For every specific K, we get at least three quarters, and then we can analyze this in a clause by clause manner, so that we get at least three quarters of the optima value. You can look at the online notes to see the calculus behind this. But the punch line is that this algorithm, this combined algorithm, gives a three quarters approximation algorithm for max M, even when the formula has clauses of some small and some big. So even with formulas with varying length clauses, we achieve a three quarters approximation.

