>> So how do I define a Markov chain in general? In general I'm going to have capital N states, and we are going to label these states by 1 through N. So in this example, capital N equals four. But in general, think of in our applications capital N is going to be huge. For instance, when we do PageRank, capital N is going to be the number of webpages on the internet. The total number of webpages on the internet. Now, this weighted directed graph is defined by its adjacency matrix. This is the adjacency matrix for this graph where we think of this is vertex, this state. Listening to Kishor is state one, this is state two, playing Starcraft is state three and sleeping is state four. So this is the adjacency matrix for this graph, the weighted adjacency matrix. Now I denote this by P, and we refer to this as a transition matrix Y, because the entry P I J corresponds to the probability of transitioning from state I to state J. So if I'm at state I at time T, the probability I'm in state J at time T plus 1 is exactly the entry P I J. I look at row I and column J. For instance, if I'm playing Starcraft at time T, the probability that I'm checking email at time T plus one is exactly 0.3. Now, the property that you noticed before about P is that each row sums up to one. The terminology for this is that P is a stochastic matrix. If the columns also sum to one, then it's called doubly stochastic. But for a Markov Chain, all I know is that the rows sum up to one. It doesn't necessarily mean that the columns have to sum up to one.