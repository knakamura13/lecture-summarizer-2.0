Let's take a look at the correctness of this algorithm for our queries. Suppose x was inserted into our database s, and we do a query on x. What do we output? Well, when we inserted x into the database, we set all of these k bits to one. So when we do a query, we're guaranteed that all of these bits are set to one, and so we're going to output Yes, because none of the bits ever change from ones to zeros. Bits only change from zeros to one. It's a one directional process.So if x was inserted into the database, when we do a query on x, we always output Yes. It is in the database. Now, suppose x was not inserted into the database and we do a query on x. Sometimes, we might say yes, we believe it's in the database. In which case, we get a false positive. We falsely say that, yes, it's in the database. How can this occur? This can occur if all of the k bits were set to one by other insertions. So for each of the k bits of x, so take the ith bit. So this is hi of x. There is some element, z, which was inserted into the database s and one of the k bits for z exactly matches the ith bit of x. Which of the k bits for z? Let's say the Jth bit for z. So the Jth bit for z matches the ith for x. In other words, h_i of x as the ith for x, matches the jth bit of z. So h_i of x equals h_j of z. This means that when z was inserted into the database we did the insert of z. Then we set this bit which matches the ith bit of x to one. And if this is true for every bit of x, so all the k bits of x are set to one by some other insertion then we're going to get a false positive on x. So this scheme has this extra robustness or redundancy. In order to get a false positive, we need all of these k bits to be set to one by some other insertions, whereas the previous scheme only had one bit which we're checking. Now we have k bits which need to get set to one in order to get a false positive. So it seems like things improve that the false positive rate goes down as k increases. But in fact there's an optimal choice of k. If k gets too large, the false positive rate starts to shoot up again. Why is that? Well if k is huge, then for every insertion you're setting k bits to one. So you're setting many bits to one if k is huge. So that means that for each of these and insertions, each of these elements in s, they have many bits, many choices of j which are set to one. So it's more likely if k is big, that one of these k bits is going to match up with one of the bits of x. So if k is too large, every insertion is setting too many bits to one. If k is small, then when we're checking it, when we're doing the query on x, we're checking too few bits. So there's some optimal choice of k, not too large and not too small. What we want to do now is more precisely analyze these false positives. What's the probability of a false positive? We want to look at it as a function of k and then we can figure out the optimal choice of k in order to minimise the false positive rate. And then we can compare and see what that false positive rate looks like to see whether this is a good data structure to use