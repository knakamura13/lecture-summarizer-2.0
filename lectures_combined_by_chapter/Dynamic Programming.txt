DP1 FIB LIS LCS - 1 - Dynamic Programming Overview
Our first topic is dynamic programming. This is one of my favorite topics to teach because it is extremely useful. Students often have some trouble with it initially, but with enough practice it will sink in and actually, they will seem quite easy once you get the hang of it. I'll do my best to show you how I solve dynamic programming problems. And, from there the key to really mastering dynamic programming is to do lots and lots of practice problems. The homework is just a start. Do additional problems from the textbook, and from other algorithms textbooks, and from other courses you can find on the web. We'll start with the toy example, computing Fibonacci numbers, to illustrate the basic idea of dynamic programming. Then, we'll dive into a variety of example problems to get a feel for the different styles of DP algorithms;- Longest increasing subs sequence (LIS), longest common subsequence (LCS) the classic Knapsack Problem, chain matrix multiplication and finally, we'll look at a few shortest path algorithms using DP.

DP1 FIB LIS LCS - 10 - LIS Subproblem Attempt 2
We now have a new subproblem formulation, let L of i denote the length of the longest increasing subsequence on the first i elements of the input array a_1 through a_i and which includes a_i itself. This is the extra restriction that we added into our subproblem formulation, okay? It requires that a_i is included in the subsequences that you consider. Now, it will be straightforward to express a recurrence, which formulates L of i in terms of L_1 through L_i minus one. Let's go back and look at our earlier example and now we'll see, the straightforward recurrence that arises. Here is our earlier example. And let's look at L for this new subproblem formulation. In this case, L of one is one because we're looking at the length of the longest increasing subsequence on an input of size one just five itself and it has to include five. Then for i equals two with length two from the subsequent five, seven. Now, we start to see a difference when i equals three. What is the length of the longest increasing subsequence in five, seven, four which includes four? That's just four itself. So that's length of one, okay? Recall in our earlier definition of L, it was length, it was three this entry. Once again, for i equals four, L of four is one because of minus three. So i equals five, now, the length is three from five, seven, nine. For i equals six, the length is two from minus three one, for i is equals seven, the length is four from five, seven, nine, ten. Finally, let's look at the case i equals 10 which is the case which caused the problems in the earlier definition of L. Now, in this case, we want to see which subproblems allow us to append A on to the end of it. I notice a can be appended on to subsequences ending at five, seven, four minus three but nine, it cannot get appended on to, that won't be increasing. It cannot get appended on to 10 either. But the others allow it to be appended on to the end. So, we're going to take the longest of those and we're going to add plus one for adding eight on to the end of it. So whatever is the longest increasing subsequence ending at five, we can append eight on to the end of it. We don't have to know the subsequence itself. We just have to know, it's of length four and it ends at five and then we can append A on to the end. We obtain the sequence of length five, therefore, L of 10 is five. Now, this highlights the recurrence for the solution of L of i, in terms of smaller subproblems L_one through L minus one, i minus one

DP1 FIB LIS LCS - 11 - LIS Recurrence Attempt 2
Outlets formally state the recurrence for L of i in terms of smaller subproblems. L of i requires that ai is included in the subsequence. Therefore we get 1 for including element ai in it. And then we take the longest subsequence that we can append on to the beginning. So we're going to take the max over all earlier indices of the length of the subsequence ending at that character aj. Now that earlier subsequence allows us to append ai onto the end of it only if aj is strictly smaller than ai. So you only want to consider j's where aj is strictly smaller than ai. And then we can take that earlier subsequence ending at aj and append ai onto the end of it and we get L of j for that earlier subsequence plus 1 for ai. And of course, we need that j is earlier in the input array than ai. Just in case the mathematical notation is confusing for anybody let me re-express it with slightly different notation and re-explain what it's saying in words. So L of i, the length of the longest increasing subsequence on the first i elements, which includes ai. So we get one for that element ai and then we're taking, we have ai here. Then we're going to take the longest increasing subsequence that we can put at the beginning. It's going to end at some element aj. Okay. So we're going to try all possibilities for that j. Now, we need that index j is earlier in the input array. So we're going to try j's between i minus 1 and 1. And we only want to try j's where aj is strictly smaller than ai. So we're doing a max over j. J is the variable that we're varying. We're trying j's where it's between 1 and i minus 1 in such that aj is strictly smaller than ai and we're taking the value L of j. Finally, we have our sub-problem definition and we have our recursive relation that the sub-problems satisfy. Now to be straightforward to detail our dynamic programming algorithm. So let's give this pseudocode for a dynamic programming algorithm and then analyze its running time and then we'll have completed this problem.

DP1 FIB LIS LCS - 12 - LIS DP Algorithm
Now, let's date the pseudocode for the dynamic programming algorithm for the longest increasing subsequence problem. And I kept the recurrence from the previous slide about the L of i in terms of smaller subproblems as a sort of crib notes for ourselves. Here's a pseudocode for our dynamic programming algorithm for the LIS problem. The input is a1 through an. Our solutions are in one dimensional array L and we're going to fill the table L from bottom up. So we're going to start at index i equals 1 and go up to i equals n. So we're going to have a single for loop for i going from 1 to n. Now our subproblem formulation requires that ai is included in the solution for L of i plus one term. Therefore, we start by setting L of i equal to 1. And then we're going to do the max over j. So we're going to have another for loop which varies j from 1 to i minus 1. Now we need to check that aj is strictly smaller than ai. If aj is strictly smaller than ai, then we need to check if the solution we can obtain by appending ai onto the end of the solution ending at aj is longer than our current best solution. So if aj is strictly smaller than ai, and our current best solution L of i is strictly smaller than the new solution, we can obtain by appending ai onto the end of L of j. In this case we want to update our current best solution, which is now the solution that we obtained by appending ai onto the end of the solution that ends at aj. Now we've given an algorithm which defines our table L. But what is the solution that we're trying to output from this algorithm? For the case of Fibonacci numbers, it was the last entry in the table. In this case, the last entry of the table is the longest increasing subsequence ending at an. That's not necessarily the solution that we're trying to obtain. We're trying to find the longest increasing subsequence regardless of where it ends at. So what we need to do is try to look through L and find out maximum entry in the entire one dimensional array. So let's go ahead and do that. We'll make a variable max, which will maintain the index with the largest entry of the table. We'll start max at 1 and then we'll go through the entries at the table to see if we find a larger entry than the current max. This for loop simply finds the largest entry of the table and stores the index for that entry in the variable max. Finally, what do we return? We return the entry at index max. This completes the formulation of our dynamic programming algorithm. Now let's take a look at the running time of our algorithm.

DP1 FIB LIS LCS - 14 - LIS DP Running Time Solution
We have one for loop, which varies over order n elements, and we have another nested for loop, which varies over, at most, n elements. Within these two nested for loops, we have a if-then statement, which takes order one time. Therefore, the total time for these nested for loops is, order n-squared time. Finally, we have another for loop which finds a max entry in the table. This is one for loop, it's not nested, this is order n time. Therefore, the total run time of our algorithm is order n-squared time.

DP1 FIB LIS LCS - 15 - LIS Recap
This completes the formulation of our dynamic programming algorithm, and the analysis of its running time. But now, let's go back and take a look at some important aspects of our algorithm design. What was the first step in our recipe for designing a dynamic programming algorithm? It was to define the sub-problem in words. In this case, define alibi in words. Our initial attempt was to use a prefix of the input, to find the length of the longest increasing sub-sequence on the first I elements of the array. Then, our second step in our recipe is to find a recurrence relation that the solutions' sub-problems satisfy. In that case, for that definition, we were unable to do so. So what did we do? We went back and we reformulated our sub-problem definition. We added an extra condition into it, and then we were able to write a recurrence for the sub-problems. Now, the intuition for why we wanted to strengthen the sub-problem definition. It goes back to, if you recall perhaps from a discrete math class way long ago, when you were trying to prove some statement by induction, how did you go about it? Well, you would first state an inductive hypothesis. Usually, the inductive hypothesis is of the same form as the statement that you're trying to prove. Then you try to prove that hypothesis using induction, but sometimes you run into difficulties. And what do you do? You go back and you alter your inductive hypothesis. Typically, what you do is you try to strengthen your inductive hypothesis by adding extra conditions in. That's exactly what we did here, we added the extra condition, that the sub-problem has to include a i itself. Okay? And then, you strengthen the inductive hypothesis and you prove that stronger statement. And that's what we're doing here. We're giving you an algorithm for this stronger problem. We're finding, in length, the longest increasing sub-sequence with the specific character at the end. Using that solution to that stronger problem, we can then solve the weaker problem, where we don't care, what is the ending character. So, a lot of the intuition for what we're doing in dynamic programming, comes from some ideas from induction proofs.

DP1 FIB LIS LCS - 16 - Longest Common Subsequence
Our next dynamic programming example is Longest Common Subsequence. Which we'll use the shorthand LCS. The input to the problem is two strings, which will denote as X and Y. And for simplicity, for now, we'll keep them both of the same length. Our goal is to find the length of the longest string which appears as a subsequence in both X and Y, and notice that we're trying to find the subsequence, not a sub-string, and which simply trying to output the length of the string. And if we can find the length, we can use that to find an actual string which appears as a subsequence and we'll see how to do that at the end.

DP1 FIB LIS LCS - 17 - LCS Example Question
Let's take a look at an example to make sure you're okay with the terminology. Here's an example with two strings of length seven. Let's do a quick check to make sure you understand the problem definition. In this example, what's the longest common subsequence? Go ahead and mark the longest common subsequence in this example and also indicate the length of it.

DP1 FIB LIS LCS - 18 - LCS Example Solution
The solution is the substring BCBA, BCB A, which is the length four. Why do we care about it? Well, I mean first off it's a nice example, which illustrates a slightly different flavor of the dynamic programming solution. That's the main motivation here, but also, actually, this simple problem is used in Unix diff. If you want to compare two files, you have this unique script diff, which compares two files and looks at the differences and it does utilizes the longest common subsequence dynamic programming algorithm.

DP1 FIB LIS LCS - 19 - LCS Subproblem Attempt 1
So let's dive into the first two steps in our recipe for designing a dynamic programming algorithm. Our first step, is to define the subproblem in words. Recall our subproblem definition from some of our earlier examples. For Fibonacci numbers, we set F(i) to be the i-th Fibonacci number. In the longest increasing sub sequence problem, we set L(i) to be the length of the longest increasing sub sequence on the first i numbers in the input. What are we going to do here? Our first attempt is always to try this same problem on a prefix of an input. So it's key. It's just identical to the original problem and it's just on a prefix. So all we've changed is from doing it on length N to length I, first I characters. So let's formalize that. So we're going to have a variable I which is the prefix length and the prefix length varies between 0 corresponding to the empty string and length N which is our original input. And we define a new function L(i), which is the length of the longest common sub sequence in the first I characters of X and the first I characters of Y. Our second step, is to define a recurrence. We want to express L(i) in terms of L(1) through L(i-1). So let's take a look at how to define the recurrence in this example, that we saw earlier.

DP1 FIB LIS LCS - 2 - Fibonacci Numbers
Given an integer n, we're going to look at an algorithm for generating the n Fibonacci number. This will be a very simple algorithm, but it will illustrate the idea of dynamic programming and then, we'll look at dynamic programming in general; the techniques for designing a dynamic programming algorithm and we'll look at some more sophisticated examples. Recall the Fibonacci numbers are the following sequence, 0, 1, 1, 2, 3, 5, 8,13, 21, 34 and so on. There's a simple recursive formula that defines the Fibonacci numbers. The first two numbers in the sequence are zero and one, and then the n Fibonacci number is the sum of the previous two Fibonacci numbers. We're going to take as input and non-negative integer n and our goal is to output the nth Fibonacci number. We want an efficient algorithm and therefore we're aiming for a running time which is polynomial in n. Now, the Fibonacci numbers are defined by this simple recursive formula. Therefore, we might think a recursive algorithm is a natural algorithm for this problem. Let's look at that natural algorithm, that natural recursive algorithm and when then we'll

DP1 FIB LIS LCS - 20 - LCS Recurrence Attempt 1
Let's detail the subproblem definition we proposed. We have a parameter i which corresponds to the prefix link. i is going to vary between zero and n. i equals zero corresponds to the empty string. i equals n corresponds to the original input. Now, we're going to define a function l of i. This will be the length of the longest common subsequence in a prefix of X of length i and a prefix of Y of length i. So, X_1 through X_i and Y_1 through Y_i. Note, this is analogous to the original problem except on a prefix of the input. One other notable difference, this does not store the subsequence itself. It simply stores the length. We want our table to store our number or true false. Now, let's take a look back at our earlier example. Now, this was our earlier example. We're trying to find a recursive relation for l of i. We're trying to express l of i in terms of smaller subproblems, L_1 through L_i minus one. Let's try to gain some insight from this example. In order to get a smaller subproblem, we're going to look at the last character. So we're going to look at X_i and Y_i. We're going to look at how X_i and Y_i are used in the solution to l of i and then, we can use the solution to the subproblem of size i minus one. We take the optimal solution for the subproblem of size i minus one and then we append on the solution for X_i and Y_i. Now, there are two cases to consider, either of these last characters are different, in this case, or they're the same. We're going to consider these two cases separately. The first case is when the last characters X_i and Y_i are the same. The second case is when the last characters X_i and Y_i are different. We're going to do this case first. X_i equals Y_i. The last characters are the same. This turns out to be the easy case. Now, let's modify our example so that the last characters are the same. So, I append on character C onto the end of both strings. Now, in this case, where the last character is the same, what do we know about the longest common subsequence? Well, we know it must include and must end in this common character. Why? Well, it give me a common subsequence and suppose it does not include this last character. Well, then, I can append on this common character and I get a longer subsequence. So, therefore, the longest common subsequence must include this last character. So, in this case, where X_i equals Y_i, what do we know about l of i? We know that the longest common subsequence includes this last character. So, we get one in the length for that common character and then we can simply drop this last character and then we can take this input sequence of length i minus one and we can take the longest common subsequence in this input sequence of length i minus one and append on this common character C. What is the length of the longest common subsequence in this input? It's simply l of i minus one. So, we have a recursive relation. We can express l of i in terms of l of i minus one. This handles a case when X_i equals Y_i. Now, let's take a look at the case when X_i is not equal to Y_i.

DP1 FIB LIS LCS - 21 - LCS Recurrence Problem
Let's take a look now, at the case when the last characters are different. This the situation in our original example, Xi is not equal to Yi, A is not equal to B. Now in this case, when Xi is not equal to Yi, the last character of the longest common subsequence can either be A or B, or neither. Certainly, cannot be both. Now suppose it's A. For example, suppose this is the last character in The Long common subsequence. And what we know about Yi B, for B has nothing left to match with an X. Therefore, we know the LCS does not include B Yi. Similarly, if the last character is B says match with this B. Then the LCS cannot include A, because it has nothing left to match within Y. The key point is that the longest common subsequence for this prefix of length i, either does not include Xi or it does not include Yi or both. So either Xi is dropped or Yi is dropped or both of them are dropped. Now let's consider the three cases. If both of them are dropped, then we can simply take the longest common subsequence in this prefix of L(i-1). So it's similar to the equal case except we don't get this plus one here. Now what happens if just Xi is dropped?. Well then we have a prefix of L(i-1) in X and a prefix of L(i) in Y. So we have no way of looking this up in our table. The solution to this sub problem is not in our table, because the prefixes are of different length in X and Y. And notice, even if we knew how Yi is matched up with an X, for instance, if we knew this B was match with this B, then we have a prefix of Length 3 and X and we have a prefix of length 6 and y. So there's still a different length. And similarly, if Yi is not included, So we dropped this last character from Y then we have prefix of length 7 in X and a prefix of length 6 in Y. So the prefixes are of different length in X and Y. And once again, the solution to this sub problem is not in our table because these prefixes are on a different length. So for this case where where Yi is dropped, we need to look up the longest common subsequence in the prefix of L(i) in X with the prefix of L(i-1) in Y. Now this isn't in our table presently and similarly when we try to solve this problem, well then we might chop off the last character from Y and then we get even shorter prefixes in length Y. So for this sub problem definition, we are unable to define a recurrence. We are unable to express L(i) in terms of smaller sub problems, but we got some insight about what is a valid. What is a good sub problem definition. The difficulty here was that the prefixes are of the same length for x and y, but we need to allow them to be of different lengths. So how do we achieve that?. We're going to change from a single parameter i to a pair of parameters i and j. I will correspond to the length of the prefix X and j will correspond to the length of the prefix Y. And, our table will now be a two dimensional table. So L(i) J will be the length of the longest common subsequence in X1 through Xi with Y1 through Yj. And then, we're going to try all possibilities for i and j.

DP1 FIB LIS LCS - 22 - LCS Subproblem Attempt 2
So let's go back and revise our subproblem definition with the insight that we just gain. The key insight was that we're going to need a prefix of x and a prefix of y of possibly different lengths. Therefore we're going to have two indices, i and j, which are going to correspond to the length of the prefix and x and the length of the prefix and y. And as a consequence, we're going to end up with a two-dimensional table. In all our previous examples, we had a one dimensional array or table. And now, we're going to have a two-dimensional table or array, because we have two indices, i and j. So let's go ahead and formalize our subproblem definition. For indices i and j where once again i varies between 0 and n, and similarly j varies between 0 and n. We're going to define the function. We're going to define the function L of i_j, which is the length of the longest common subsequence in the first i characters of x and the first j characters of y. The key is that x is of length i and y is a prefix of length j. Now let's go ahead and see if we can define recurrence for this new subproblem definition. Let's start off with the base cases. Base cases are going to be when i is 0 or j is 0. Let's start with considering the case when j is 0. So what is L of i0 and what is L of 0j for the case when i is 0. Well, for Li0, so here j is 0. That means that y is the empty string. So what is the longest common subsequence? Well, in both where y is the empty string, so there is no subsequence in it. So the length of it is of length 0. Similarly, if we're taking an empty string for x then the longest common subsequence is the empty string itself, which is of length 0. So those are two base cases. Let's move on to the more interesting cases.

DP1 FIB LIS LCS - 23 - LCS Recurrence Unequal Case
So let's go ahead and try to find the recurrence for our new sub-problem definition. The length that the longest common sub-sequence, in the first I characters of X and the first J characters of Y. And let's go back and look at our earlier example. Here's our earlier example, and notice that here, they're both of the same length and just to illustrate our new sub-problem definition, let's add in one more character to Y, so there are now of different length. And if you recall our approach from before, what we're going to do is, we're going to look at the last character of both sequences, and we're going to condition on whether they're the same or different. If they're the same and then we can possibly match them together, if they're different, there's no way we can match them together. So let's do the case with a different first. Our key insight, is that in this case with the last characters are unequal, then the longest common sub-sequence either ends in A the last character of X, or in the last character of Y which in this case is D, or it ends in neither. Now if it ends in A, that means it does not end in, it does not end in D, it does not include D in the final solution, okay? There might be other occurrences of D, but we're differentiating the multiple occurrences of this character. This character is not included in the final solution, or this character is not included in the final solution, or both of these final characters are not included in the optimal solution. So there are three cases to consider, either the last character of X is not used in the optimal solution, the last character Y is not used in the optimal solution, or the last character of both is not used in the optimal solution. If the last character of both is not used, we can get there by dropping the last character of X and then dropping the last character or Y or vice versa or dropping less character Y and then X. So there's really only two cases to consider, either we drop the last character of X, or the last character of Y, and we're going to take the best of those two. So let's recap, so if we dropped the last character of X and X I is not used in the optimal solution then, L of I J is the same as L of I minus one, which has just dropped X I from it and J. The other scenario, is that we dropped the last character of Y and in this case L of I J, is L of I, J minus one, we just dropped Y J from Y. How do we know which of these two is the better solution? We just take the larger of the two. So that's going to be our recurrence. Our recurrences is L of I J is going to be the max of these two possible solutions. Let's recap, in this case where X I and Y J are different, so the last characters of the two prefixes are different, then we have a recurrence. L of I J is going to be the best of the two possible scenarios. Either we dropped the last character of X, and therefore we have this solution for I minus one characters of X, and J characters from Y, or we drop the last character from Y and we get the solution, L of I J minus one, and which of the two where we're going to take? We're gonna take the better of the two. Which means we're going to take the one which has larger length which is the max. So this defines a recurrence for the case when X I is not equal to Y J. Now let's do the case where X I equals Y..

DP1 FIB LIS LCS - 24 - LCS Recurrence Equal Case
So for a new sub problem definition, we define the recurrence for the case when the last characters of the two prefixes are different. Now, let's go look at the case where the last characters of the two prefixes are the same. So i have modified the example so that this is the case. Here, they both end in A. We're looking now at a case where X_i equals Y_j, So the last characters of the two prefixes are the same. And now, in this case, there are three possibilities to consider. As in the unequal case, we can either not include X_i in the optimal solution, in which case we drop X_i or we drop Y_j. Notice actually those two are the same in this scenario because X_i and Y_j are the same. So we're dropping X_i we're also dropping Y_j. I'm just going to foreshadow a little bit of the solution or recurrence. The other possibility which didn't exist in the unequal case is that the optimal solution ends in the common character X_i equals Y_j. As in the unequal case, what we're going to do is we're going to look at these three scenarios and we're going to take the best of the three. Now, in the first case, where we drop X_i noticed that L of i, j is equal to L of i minus one j, where we just dropped X_i from the solution, just like in the unequal case. Similarly, when we drop Y_j, L of i, j equals L_i, j minus one. Now, the interesting case which didn't exist before in the unequal case is that the optimal solution ends in this common character. And in this case, L of i, j is one plus, the one is for the common character that were pending on the end and then, we're going to take the optimal solution which drops X_i and Y_j from there and we take the optimal solution to these smaller prefixes. So this is L of i minus one and j minus one. We take the length of that optimal solution for that smaller problem, add one for this common character which we append at the end and that gives us the length of the longest common sub sequence in this new sub problem.

DP1 FIB LIS LCS - 25 - LCS Recurrence Equal Recap
So let's recap our recurrence for the case where X_i equals Y_j, the last characters are the same and the two prefixes. Here, we have three possibilities and we're going to take the best of the three, therefore, we take the max. We have L of i minus one j, which corresponds to dropping X_i, L of i, j minus one which corresponds to dropping Y_j or we include X_i, Y_j the common character at the end and we get a plus one for that, and then we take the optimal solution to this smaller sub problem L of i minus one, j minus one, we take the max of these three. Some of you may have noticed that in fact, the recurrence, in this case, can be simplified even further. We only need to consider the last case, where we append on X_i, Y_j the common character onto the end. Let me give you some brief intuition about why that's the case. Notice, if this solution doesn't include either of these characters, then we can just add them on and we get a longer. So, therefore, it wasn't optimal length. So it's got to include either X_i or Y_j. Now, it may be the case that X_i is matched with some earlier recurrence of that character. In this case, it's Y_1, but any solution which we obtain by matching X_i with an earlier recurrence of that character, we could have obtained Y instead matching X_i with this latter solution. Any sub sequence which occurs in that smaller prefix of Y, also is a sub sequence in this larger prefix of Y, okay? It's a super sign of it. Therefore, we only need to consider this case and we get this simpler recurrence for this case where they're equal, okay? And this completes our definition of the recurrence.

DP1 FIB LIS LCS - 26 - LCS Recurrence Summary
Let's recap the recurrence that the L Y J satisfies. Now we're looking at the case with the two strings are non-empty. So I has at least one and J is at least one. Now we had two cases here. We had the case X I is not equal to the Y J, the last characters are different, and we have the case where the last characters are the same. In the case where they are the same, we had the simple recurrence. We depend on X I Y J, and therefore get a plus one for that, and then we take the optimal solution to the smaller sub-problem with one less character in each string. Now in the case where they are not equal, we took the best of two scenarios. We either drop the last character from X, or we drop the last character from Y. And we take the larger of these two solutions, and we also had the base cases, where one of the two strings was empty, in which case the length is 0. Finally, now we can state our dynamic programming algorithm. The pseudocode for the algorithm. And now, notice we have a two dimensional table now. L is a two dimensional array. How are we going to fill it up? We're going to fill it up row by row. And now when we're looking at this entry L I J, what are the entries that we're going to need for it? We're going to need either this diagonal. It's right here, L I-1J-1, which will be there because we filled in the previous row, or we'll need the entry just above, or we'll need the entry just to the left. So we're just gonna look at these three entries, which will all be there, because we are filling it up row by row. So earlier in this row will be completed and the previous row will be completed. And now we can go ahead and state our dynamic programming algorithm.

DP1 FIB LIS LCS - 27 - LCS DP Algorithm Question
Now, we can state the pseudo-code for our dynamic programming algorithm for the longest common subsequence problem. The input are these two strings, X and Y. Now, let's start with the base cases which are the top row and the first column, which first set L(i,0) to be zero. This corresponds to setting the first column to zeros. Then, we set the first row to all zeros. These are our two base cases. Now we can fill the interior of our table. As we said earlier, we're going to go row by row. Index i is going to correspond to the current row. Then we go along this row. This is the index j. Now we have two cases to consider, either the current last characters of X and Y are the same or they're different. So, if Xi = Yj, we have one recurrence. And if Xi is not equal to Yj, then we have a different recurrence. In the case where Xi = Yj, the recurrence is 1+, the one comes from the appending Xi and Yj to the end, and then taking the optimal solution for the prefix of length i-1 with length j-1. In the case where they're not equal, the recurrence is the best of the two scenarios. Either we drop the last character from Y, giving us L(i, J-1), or we drop the last character from X, giving us L(i-1, j) and we take the max of these two. Finally, what is the output of our algorithm? It's the entry in the bottom right of our matrix. L(n, n) is the length of the longest common subsequence for all of X with all.

DP1 FIB LIS LCS - 28 - LCS DP Algorithm Solution
Finally, let's take a look at the running time of our algorithm. The two base cases are each order one per step and then they are in order n sized for loop. Therefore, they take order n total time. Then we have a for loop of size order n and a nested for loop of a size order n and inside it takes order one time. So the total time is order n square with these nested for loops and the total run time of our algorithm is dominated by the order n square, so we get order n square total time. This completes the dynamic programming algorithm for the longest common subsequence problem. The interesting thing that arose in this solution was that we needed to use a two dimensional table. This came about because we needed to consider prefixes of x of different length than prefixes of y.

DP1 FIB LIS LCS - 3 - FIB1 Recursive Algorithm
Look at the natural recursive algorithm for computing the Nth Fibonacci number. Recall from the previous slide, the recursive formula for the Nth Fibonacci number is the sum of the previous two Fibonacci numbers. Let's detail the recursive algorithm for computing the Nth Fibonacci number. We'll call the algorithm Fib1, a little bit foreshadowing. This will be our first attempt then we'll have a second successful attempt for efficiently computing the Fibonacci numbers. Recall the input to our algorithm is a non-negative integer n, and the output is the Nth Fibonacci number or more precisely actually the n plus first Fibonacci number if you're keeping track of the indices. First, we'll deal with the base cases. The first two Fibonacci numbers are zero and one. In general, we'll utilize the recursive definition of the Fibonacci numbers. We will recursively compute the n minus first, and n minus second Fibonacci number. And we'll return the sum of those two. This completes the definition of our algorithm. Now, let's take a look at the running time of our algorithm. To analyze the running time of our algorithm, let's create a function T of n which denotes the number of steps of our algorithm and an input of size n. We have two basic cases first. Those each take order one time. And then, we have a two recursive calls, one of size n minus one, and one of size n minus two. Those take time, T of n minus one plus T of n minus two. Putting it all together, we have the following formula. Our running time on the input of size n is order one for the two base cases and then, T of n minus one in order to compute the n minus first Fibonacci number, and T of n minus two to compute the n minus second Fibonacci number. Now, this formula may look a little bit familiar. Notice, it reminds us of the Fibonacci numbers. Fibonacci numbers, the n minus Nth Fibonacci number is the sum of the previous two. This is the sum of the previous two plus an extra term. So, it's at least as large as the Nth Fibonacci number. Therefore, our running time on input of size n is at least the Nth Fibonacci number. Now, unfortunately Fibonacci numbers grow exponentially and then more precisely. They grow exponentially in this constant phi. What is phi? Well, phi is this constant defined as one plus square-root five over two which is roughly 1.618. But phi is a magical constant. It's known as the golden ratio. The golden ratio has many applications. For example, if you're designing a building such as the Parthenon or if you want just a cool fact to show your kids, then look up the golden rectangle. But for us, the punchline is that the Fibonacci numbers grow exponentially in n. And the running time to compute the Nth Fibonacci number using a recursive algorithm is at least the Nth Fibonacci number. So, our running time is exponential in n. So this is a terrible algorithm for computing the Nth Fibonacci number. Let's take a look at exactly why our running time is so terrible and then, we'll get an idea for a more efficient algorithm.

DP1 FIB LIS LCS - 33 - DP1 Practice Problems
At this point, I suggest you try some practice dynamic programming problems. There are a lot of great practice problems in the Dasgupta book. Let me suggest a few problems for you. Now, these are problems from the textbook Algorithms by Dasgupta, Papadimitriou, Vazirani. This problem 6.1, which is about finding a contiguous sequence of maximum sum. Now, note a contiguous subsequence is the same as a substring. Now, whenever I sign you a homework problem, I'm always going to tell you some little blurb about what the problem is about, so you can identify it. Because, some of the online versions of this textbook have different numbering. So, you should always make sure it's the correct problem, if you have an online version. The next problem is 6.2 where you're planning a trip and you want to figure out the hotel stops to minimize the penalty. Problem 6.3 is about yuck Donald's. In 6.4, you're given a string and you wanted to see if you can break up that string into a set of words. Now, 6.11 is longest common subsequence. We already did that in this lecture, but for practice, you might try the variant where you did the longest common substring. It's useful to look at how dynamic programming solutions vary. What's the difference when you have substring versus subsequences? Once again, at this point, you should be able to do these practice problems: 6.1, 6.2, 6.3, 6.4, and 6.11, or this variant of 6.11 where you do substrings instead of subsequences. Let me give you a quick summary of the approach we used for designing a dynamic programming algorithm. And you can implement this approach when you try these practice problems. The first step is to define the subproblem in words. How do you do that? Well, you'd take the same problem, the original problem, on a prefix of the input. So, the subproblem should be of the same form as the original problem, but instead of being an input of length N try it on a prefix of the input. So, an input of length I. The second step is to define a recurrence relation. If you have a one dimensional table, then you wanna express T of I or the Ith entry of this one dimensional table in terms of smaller subproblems, T 1 through T I minus 1. This is what we did for LCS, though in that case, it was a two dimensional table. Now, when we did LIS, Longest Increasing Subsequence, we were unable to do this. And what we needed to do was strengthen the subproblem. So, we had to go back and redefine the subproblem. So, we've strengthen the subproblem by adding the constraint. Typically, we add the constraint that the last element, element I, is required to be included in this solution to this subproblem. So, for LIS, we said, LFI was the longest increasing subsequence in the prefix of the input of size I. But the last element A I had to be included in that solution. Now, one thing to keep in mind is typically, when we add in this constrain, and the final output is no longer the last element in the table, but instead we're going to have to do a max or a min. But we're going to have to go over the whole table and look for the best, the optimal solution in the table. Now, I want to show you how I solve these practice problems. So, I'm going to show you how I approach this problem 6.1. But to get the most out of it, I suggest that you try the problem first, and then if you're having difficulty, you can watch the next slide to see how to solve it.

DP1 FIB LIS LCS - 34 - DP1 Practice Problem 6.1
The input to this problem is a_1 through a_n. Our goal is to find the contiguous sub-sequence. Noticed, that a contiguous subsequence is the same as a substring, and we want to find the substring with maximum sum. Now, the first step is to define the subproblem in words. I'm going to try the same problem on a prefix of the input. So, we have a parameter i, which is the length of the prefix. So, we have an i, and this is going to vary between zero and n. And now, I'm going to find s of i, in words, first off. It's going to be the max sum that I can obtain. The same format as the original problem. But, instead of considering the whole input, and I'm going to consider the first i numbers. So, it's a max sum from a substring of a_1 through a_i. So it's the first i characters of the input. Now, we want to try to find a recurrence relation, we want to express s of i in terms of s_1 through s_i minus one, smallest subproblems. Let's take s of i minus one, is the max sum that we can obtain from a substring of a_1 through a_i minus one. So, let's take that sum. Now, there's two possibilities that I would consider. Either, or add on, a_i on to the end of it or not. First off, can I add on a_i on to the end of this solution? Well, I have to maintain that it's a substring, that is contiguous. I don't know where this one ends, so, I don't know whether I can add on a_i unto the end of this solution. So, I need to know where this one ends. With this current definition, there's no way, I don't know how to express s of i in terms of the smaller subproblems. But I notice, that if I knew where this solution ended at, so, if it included a of i minus one, then, I know that if it includes a_i minus one then I can add on a_i and maintain that it's a substring. So, I have to go back and strengthen my subproblem definition.

DP1 FIB LIS LCS - 35 - DP1 Practice Solution
Here's a solution to problem 6.1. Let's reformulate our subproblem using our insight we just gained. We wanted to add the extra condition that the subproblem had to include a_i. So we define s of i as a max sum which we can obtain from a substring of a_1 through a_i with the extra restriction that a_i has to be included in that substring. Now, we will be able to re-express a recurrence for s of i. First off, let's handle the base case; that's i equals zero. What's s of zero? Well, we have the empty string, so that's, of course, zero. Now, let's look at s of i for i at least one. Well, we are required to use a_i, so let's add an a_i. So by including a_i, this sum is at least a_i plus where do we append onto it? Well, there are two possibilities; either we just use a_i by itself or we append it on to the longest substring from a_1 through a_i minus one and we're going to take the best of those two possibilities. So if we have a_i by itself, then we get zero for the rest, for a_1 through a_i minus one, or if we take the optimal substring for a_1 through a_i minus one, what's the sum from that? Well, it's s of i minus one and we're going to take the best of these two scenarios. Now, clearly if this is negative, then we're going to just use a_i by itself, and if this is positive, then we're going to take this solution and append on a_i to it. So this gives us the recurrence, it's easy to fill the table by going from i equals zero up to n. What's the final output of the algorithm? It's not necessarily s of n because that's the longest the maximum sum from a substring which includes a_n. But we're just looking for the maximum sum we can obtain from any substring, we don't care what the last character is. So, we need to try all possibilities for the last character or number. So we do a max over i of s of i. So we take our one-dimensional table and we look for the max entry in that table and that's the output. And what's the running time of this algorithm? Well, each entry of the table takes order one time because we just have to look at two numbers and how many entries in the table are there, and there's order n. So the total run time is order n.

DP1 FIB LIS LCS - 4 - FIB1 Exp. Running Time
Let's take a look at the recursive nature of our algorithm. At the top level of the recursion, we're computing the nth Fibonacci number. Then we have recursive subclause computing the n minus first and n minus second. And we recursively compute the n minus first Fibonacci number, in which our algorithm is going to recursively compute the n minus second and n minus third Fibonacci number. And for the n minus second, it recursively computes the n minus third and n minus fourth Fibonacci numbers and so on. Notice that the n minus fourth Fibonacci numbers can be compute in many times. Similarly, when we get down to small Fibonacci numbers, we're gonna be recomputed them many times exponential number of times. That causes the inefficiency of our algorithm because we're recomputing the solution to the small sub-problems many times. To get around that, we're gonna flip the algorithm on its head. We're gonna compute the small sub-problems first, and then work our way to the larger sub-problems. In particular, we're gonna make an array F, we're gonna call it, to denote the Fibonacci numbers, and F of I is gonna be the I Fibonacci number. We're gonna start at index zero, and we're gonna record the first Fibonacci number, and the second Fibonacci number, and then we're gonna use a recursive formula to compute the third Fibonacci number and so on. We're gonna increase our index using the recursive formula. Finally, we'll get the n Fibonacci number. This is the idea of our algorithm. We start with the small indices, compute the small Fibonacci numbers, and then, we'll work our way up to compute the larger Fibonacci numbers using our recursive formula. This is our dynamic programming algorithm. So let's define it more precisely, so we are all clear.

DP1 FIB LIS LCS - 5 - FIB2 DP Algorithm
Now, let's detail our dynamic programming algorithm for computing the nth Fibonacci number. Here's our second attempt at an algorithm for computing the nth Fibonacci number. Recall, we're going to create an order array F which is going to store the Fibonacci numbers and we're going to start at the small indices namely. At the first two indices, we store the two base cases, zero and one, which are the first two Fibonacci numbers in the sequence. Then we have a for loop which goes from two up to n which is our goal. And using the recursive formula, the ith Fibonacci number is the sum of the previous two which are already stored in our array. Finally, we return the nth Fibonacci number which is stored in the last index in our array. This completes our algorithm. Notice the key thing is that our algorithm has no recursion in it. We have a recursive formula which defines the Fibonacci numbers but our algorithm has no recursion in it. Finally, let's analyze the running time of our algorithm. Once again, the base cases take order one time each. Then we have a for loop of size order n. The one step of the for loop takes order one time. Therefore, the total time of the for loop is order n time and the total run time of our algorithm is order n time. This completes our algorithm and gives us a glimpse of a dynamic programming algorithm.

DP1 FIB LIS LCS - 6 - FIB2 DP Recap
Before moving on to a more sophisticated example, let's recap a few key issues. I want to stress one important point about Dynamic Programming algorithms. Our algorithm had no recursion inside of it. We used the recursive nature of the problem to design our Dynamic Programming Algorithm, but the algorithm itself has no recursion inside of it. Now, there is an alternative approach to Dynamic Programming. In this approach, you use a hash table or some other similar structure to keep track of which sub-problems that have already been solved, so that you don't recompute those problems. Now, we're not going to use this at all in our course. This approach is called memoization. Try to say that five times, I can't and therefore we're not going to use it in this class. But actually, the real reason why we're not using it, is because the goal of this unit is to learn dynamic programming. Therefore, to avoid any confusion we're going to say no recursion in our algorithms. Dynamic programming has several advantages over memoization or similar techniques. Some might say the algorithms themselves are more beautiful. Certainly, they're faster because they have less overhead from avoiding recursion. And finally, it's much simpler to analyze the running time of Dynamic Programming Algorithms, dynamic programming is widely used. At first, students often find it challenging, but what we find is that with enough practice the dynamic programming algorithms start to seem similar to each other. At that point it will click and hopefully it'll seem easy to design a dynamic programming after that. To get to that point, what do you need to do? Practice, practice, practice. Do a lot of practice problems and then it will click and you'll find dynamic programming easy. We're going to do some in class, you're going to do something during the homework and then do a lot on your own. There's a lot of practice dynamic programming problems out there in the textbook and other online resources that you can find. You do enough practice, you ace the topic.

DP1 FIB LIS LCS - 7 - Longest Increasing Subseq.
Let's look now at a more sophisticated example of dynamic programming. The problem we're going to consider is the longest increasing subsequence problem. And for simplicity, we'll call it LAS. The input to the problem are N numbers which we'll denote as, A1, A2, up to AN. Our goal is to compute the length of the longest increasing subsequence in the N numbers of the input. One important note, we're only trying to find the length of the longest increasing subsequence. We're not trying to find the subsequence itself. But if we can find the length, then we're going to get to the heart of the problem and then it will be easy to transform that into an algorithm to output an actual subsequence of longest length. Let's take a look at a specific example to help illustrate the terminology in this problem. Here's an example where N equals 12. Before defining subsequence, let's recall the definition of the more common term substring. A substring is a string that occurs inside the larger string. So it's a set of consecutive elements. For instance, here's one substring negative 3, 9, 1, 10. Another substring is for itself. And another substring is 9, 1, 10, 4, 5, 8, 9, 3. These are all substrings. How many substrings are there possible? We can specify a substring by the start index and the ending index. Therefore, there is a most order N squared substrings. Now, our problem is not defined for substrings, it's defined for subsequences. A subsequence is a string you can obtain by deleting elements of the larger string. So it's a subset of elements in order but we can skip elements. It doesn't have to be consecutive elements. Let's look at some examples subsequences. For instance, four minus three, one, nine is a subsequence. Another subsequence is one itself. Another subsequence is 5, 7, 3. Now, we're trying to find a subsequence which is increasing. That means that each number is strictly larger than the previous. In this case, it's not increasing because of 3 is smaller than 7 but an increasing subsequence for instance, is 4, 9,10. Another subsequence is 4, 4, 8, 9. But that is not an increasing subsequence. That's a non-decreasing subsequence, and we don't allow that in our example. Our goal is to find the longest increasing subsequence in the input array. For this example, what is the longest increasing subsequence? The longest increasing subsequence in this example is minus 3, 1, 4, 5, 8, 9 which is of length 6. So, the output of our algorithm on this instance is six. Now, let's try to design a dynamic programming algorithm for this longest increasing subsequence.

DP1 FIB LIS LCS - 8 - LIS Subproblem Attempt 1
Now let's look at the recipe for designing a dynamic programming algorithm. The first step, is to define the sub-problem in words. To see what I mean by this, let's look back at the simple example of Fibonacci numbers. In that example, our sub-problem definition was F of I is the I Fibonacci number. The second step in our recipe, is to state a recursive relation. We want to express the solution to the I sub-problem in terms of smaller sub-problems. So for the instance of Fibonacci numbers, we want to express F of I in terms of F one, through F I minus one. Because if you recall our dynamic programming algorithm for computing the Fibonacci numbers, we computed F 1 up to F I minus 1, so those will be stored in our table, and then we can use those to compute F of I. Now for the case of the Fibonacci numbers, it was straightforward to express F of I in terms of smaller sub-problems. Namely F of I is the sum of the previous two Fibonacci numbers. Now, let's figure out how to follow this recipe for the longest increasing sub-sequence problem. Now, let's follow the first step in our recipe. We want to define the sub problem in words. Our first attempt is always going to be, to use the identical problem on a prefix of the input. In this case, that means we're going to look at the longest increasing sub-sequence on the first I elements of the input array. Therefore, we're going to make a new function L, which is the length of the longest increasing sub-sequence on the first I elements of the input. Now, we want to figure out how to express L of I in terms of L one through L I minus one smaller subproblems. To do this, let's take a look back at our earlier example, and see if we can gain some intuition.

DP1 FIB LIS LCS - 9 - LIS Recurrence Attempt 1
Recall, our subproblem definition is L of I, is the length of the longest increasing subsequence on the first I elements of the input, all right? And our goal is to express L of I in terms of the solution to smaller subproblems. So, now, let's recall our earlier example. Here is our earlier example which had n equals 12. Now, let's look at L on this example. To start with, for L of one, we're looking at the longest increasing subsequence on the one element array. That's length one or five itself. Then, for L of two, we're looking at the two element array five, seven the longest increasing subsequence is five, seven itself, which is length two. And for five, seven, four; five, seven is still the longest and we add on three, but still length two is the longest, for 9, we can appended on to the longest solution. We get the length three with one, stays 3. When we add on 10, we can append 10 on to the longest solution. We get like 4. Now let's pay attention to the case I equals 9. What is L of 9? Well, we cannot append on A on to the end of this current longest solution but in fact, there's a different solution. -3,1,4,5, for which we can append A on to the end and we get a solution of length 5. So, the problem is, how can we compute L of 9 using L 1 through L A? How do we know whether we can append A on to the end of the current solution or not? We're not maintaining the current solution but even if we maintain the current solution, how do we know that we can append A on to the end of it? For this solution we couldn't maintain it. We couldn't append it on to the end. But for this solution we could append A on to the end of it. So, suppose we kept track of the current solution or actually, what do we need to know? We need to know the ending character of the current solution and what's the key fact? We want to know the longest increasing subsequence with the minimum ending character. So, we want to know the smallest ending character, because the smallest ending character gives us the most possibilities for appending on a new character on to the end. So in this case, in order to compute L of 9 using L 1 through L A, we need to keep track of the longest increasing subsequence solution with the minimum ending character. And in this case, it's five and then we realize we can append eight on to the end of it and we can increase those solution length from four. Let's go back to I equals 8 and we'll start to see the complication in this solution. Previously, our solution was 5, 7, 9, 10 but actually with our new formulation what we want to maintain is, -3, 1, 4, 5 because this is also of length 4 and it ends in a smaller character. For now let's go back to I equals 7. For I equals 7, the longest increasing subsequence is 5,7,9,10. But, no. When I equals 8, we need to have this sequence minus 3 1,4. Now this is only length 3. So it's a suboptimal solution but we need to maintain it in order to realize that in the next step that we can append 5 on to the end of it and then we obtain a length 4 solution for I equals 8. So, how do we maintain this suboptimal solution? The key is, that for every possible ending character, we want to maintain the length of the longest increasing solution with that ending character. We need to know the length of the longest increasing subsequence for every possible ending character. If we know that for every possible ending character, then when we get a new character, in this case five, we can try all possible ending characters and we can try to add on five. We can see whether it is allowed to add on five on to the end of it. Now, how many possible ending characters are there? What are the possible ending characters? Well, the ending characters must be an earlier element in the input array. So in this case it is 5,7,4, -3 up to 4. Those are the possible ending characters. So they're not limitless. There's a finite small number. Actually, I minus 1 possible ending characters. So, this actually gives us an idea for how to modify our subproblem formulation. We want to know the length of the longest increasing subsequence for every possible ending character. We just noticed that the possible ending characters are the earlier elements in the array. Therefore, we want to maintain the length of the longest increasing subsequence for each element of the array. This gives us an idea for the new subproblem formulation. We're going to modify the length of the longest increasing subsequence on A 1 through A I and includes A I, okay? So this will give us the longest increasing subsequence, which ends at the I element of the array, in this case five. And then, we have that for L 1 through L I minus 1 and then we can use that to decide, what is the longest sequence ending at 8 because we can see which characters allow us to append 8 on to the end of it. Let's go back and formulate this more precisely and then we'll see the recurrence.

DP2 Knapsack Chain Multiply - 36 - Knapsack Problem
The next problem we're going to discuss is the knapsack problem. In this problem, the input is n objects. For each object were given its weight and its value. And we'll assume that the weights and the values are all integers. We'll denote the weights by w1 through wn and the values by v1 through vn. Now we're given one additional input parameter, which is the total capacity available, which will denote as capital B. Our goal is to find a subset of objects. We need the subset of objects to fit in the backpack. That means that their total weight is in most capital B. And we're trying to find the subset with maximum value, maximum total value. So let's try to restate this in more precise mathematical terms. What do we mean by the total weight is in most capital B. We want to look at those objects which are in our subset or chosen subset. So these are the i and s. We want to look at the weight of these. And we want to sum over the weight and we want that total weight to be in most capital B. The total value for a subset of objects is the sum over the objects and the subset of their individual values. And we're trying to maximize that sum. We're trying to find the subset of objects with maximum value, but maintaining that it fits in the backpack. So their total weight is in most, B. Let's summarize the problem one more time just to make sure everybody understands. So we're giving as input, the weights and values. These are these two n numbers, w1 through wn and the values v1 through vn. And we're also given the total capacity, capital B. Our goal is to find the subset of objects. So a subset of 1 through n, where that subset fits in the backpack. So the chosen subset has total weight at most capital B and the subset we chose has maximum total value. So we're trying to find the subset with maximum value, total value, and fits in the backpack. You can imagine some applications of this are, where we're scheduling jobs and we have limited resources or limited computation time and we want to choose the jobs with most value for us. But really, this is a nice toy example which is going to illustrate some different style dynamic programming solution. And then we're going to see many applications in the homework of some variants which use this style dynamic programming solution.

DP2 Knapsack Chain Multiply - 37 - Knapsack Problem Variants
Now, there are two natural variants of this problem, and both have different dynamic programming solutions. So it'd be useful to look at both. In the first version, there's one copy of each object. So we're trying to find a subset without repetition. In the second version, there's unlimited supply of each object. So we can use an object as many times as we'd like. In this version, the subset S has repetition possibly. So the subset S is actually a multiset. To summarize, in the two versions of the problem, there is either unlimited supply of each object, so we can use it as many times as we'd like, or there's at most one copy of each object that we can use. We're going to start up by looking at version 1. So we have at most one copy of each object that we can use, and then we'll go back, and we'll look at the second version of the problem where we have unlimited supply of each object. So let's dive in and look at the first version and try to design an algorithm [inaudible].

DP2 Knapsack Chain Multiply - 38 - Greedy Algorithm Question
Now if you are presented with this problem in real life, the first approach you might try is a Greedy approach. let's take a look at a specific example, and then this will highlight the pitfalls with the Greedy approach. Now here's an example with four objects. The values are 15, 10, eight and one. The weights are 15, 12, 10 and five. The total capacity will be 22. Now we're looking at the version where we have one copy of each object that we can use. Let's take a look and make sure that you understand the problem. What is the optimal solution for this problem? What does the subset of objects which attain the maximum value while fitting in the backpack?

DP2 Knapsack Chain Multiply - 39 - Greedy Algorithm Solution
For this example, the maximum value that we can obtain is 18, and that is obtained by using objects two and three. The total weight of these objects is 22,12 + 10 and the value we obtained from them, the total value is 10 + 8 is 18. Now, let's compare this to the greedy algorithm. What would a greedy approach do? A greedy approach would take the most valuable object and try to fill up the backpack as much as possible with that most valuable object. What is the most valuable object? That's not the one with the total maximum total value. It's instead the one with the maximum value per unit of weight. If the weights are in pounds or kilograms and the value is in dollars, then we're looking at the object with maximum to the other value per pound or per kilogram. In summary, the greedy approach would sort the objects by their value per unit of weight, which is this, quantity ri, which is states value divided by its weight. In this example the objects are already sorted by that ratio. We have that r1 > r2, > r3, > r4. So now what would a greedy approach do? The greedy approach would try to add object one, if it can, in this case it can, then we go to object two, and it would try to add object two if it can put. In this example, once you add in object one, you have 15 units of weight. You only have seven units of weight remaining, so you can no longer add in object two. Then we go to object three. The next most valuable object. We would try to add it in, does it fit? No it doesn't fit. Then we try to add object four, if it can. In this example it can because 15 + 5 is 20. It fits in the backpack so the greedy approach could obtain the solution using objects one and object four. Notice that the total value of this solution, object one and object four is 15 + 1, so it has total value 16, whereas our optimal solution has total value 18. This example illustrates why the greedy approach fails. It would try to add an object one, and once it does that, it's filling up the backpack too much and it can no longer fit in object two or three, and it ends up being more useful to skip object one and, instead, add in objects two and three. If you want to make a sub optimal choice at the beginning to allow you to squeeze in more objects later on. Now, let's go back and try to make our dynamic programming solution for this problem.

DP2 Knapsack Chain Multiply - 40 - Knapsack No repetition
Recall our basic recipe for designing a dynamic programming algorithm. The first step is always to define the sub-problem in words. Our first attempt is always to try the same problem on a prefix of the input. Therefore, we let K of I be the max value achievable using a subset of the first I objects. All we've changed is we've changed the set of objects available to us from the first N objects 1 through N to a subset of objects 1 through I. Our second step in our recipe for designing a dynamic programming algorithm is to find a recursive relation which expresses K of I the solution to the I sub-problem in terms of smaller sub-problems. In this case K 1 through K I minus 1.

DP2 Knapsack Chain Multiply - 41 - Knapsack Recurrence 1
To summarize, K of i is the max value we can obtain using a subset of the first i objects. And we're trying to find a recurrence which expresses K of i, the solution to the i's subproblem in terms of smaller subproblems. So let's go back and look at our earlier example and see if we get some idea for a recursive relation. In our earlier example, the objects had values 15, the object 1, 10, the object 2, 8 for object 3 and, 1 for object 4. And their weights were 15, 12, and 5. Now, let's look at this one dimensional table K that we're trying to find a recursive relation for. Now let's look at our one dimensional table K, in this example and see if we can figure out a recursive relation for the solutions of K of i in terms of smaller subproblems. So let's fill it in for this example. Let's start with K-1. In this case we're looking at a subset of object 1. So either we use object 1 or we use the empty set. Clearly using object 1 is better because it fits in a backpack and has total value 15. So K of 1 is 15. In this case. Now let's look at K of two, i equals two. In this case we're looking at a subset of objects 1 and 2. So either we use both objects in this case they have total weight 27. So they don't fit in the backpack so we can't use both objects. We can use either object 1 or object 2 or neither. And in this case in this example it's better to use object 1. Now let's go to i equals 3. What's the optimal solution for i equals 3. Well in this case we want to use objects 2 and 3. They have total weight 22. And this is our optimal solution to the entire problem as we saw before and that has total value which is a team. Now note this solution is obtained by using subsets 2 and 3 whereas our earlier subproblems, their solution was obtained by using object 1 only. Now the question is can we obtain this K of 3 which in this case is 18 using K of 1 and K of 2. But K of 3 is obtained by taking a sub optimal solution to i equals 2. We don't want to use the optimal solution because that doesn't allow us to add in object 3 into the backpack. There's not enough spare capacity available so we need to take a sub optimal solution to i equals 2. The key is that that sub optimal solution to i equals 2 has enough spare capacity to allow us to add in object 3. What we really want to do is we want to take the optimal solution to i equals 2 where the total capacity available is in most the total capacity in the original subproblem minus the weight from using object 3. If we're going to add object 3 to our solution then that takes weight W3. And then our capacity available goes down by W3. And we want to take the optimal solution to that smaller subproblem which is i equals 2 in this case and we want to look at the optimal solution with this capacity with the smaller capacity. In this case that capacity is 12. And if we take the optimal solution for i equals 2 with total capacity 12 than object 1 no longer fits and only object 2 fits in there. So the optimal solution will be just using object 2 and add the total value we obtain from that is 10. So this will have total value 10 and therefore we can append on object 3 onto it and we get the solution 2 and 3 and we get the total value 18. What we see from this example is that this definition of the subproblem does not suffice. We're not able to express K of 3 in terms of K-1 and K-2 because the solution to K of 3 does not y- build upon the solution to K- i equals 1 and 1 equals 2. Instead it uses a sub optimal solution 2, i equals 2. What is that suboptimal solution? That suboptimal solution has limited capacity available. It has limited capacity available in order to allow us to later add in object 3 and obtain a better solution for i equals 3. This points us in the right direction because what we need to do is limit the capacity available for these subproblems. So in some sense we want to take a prefix of the objects, 1 through i, and we want to take a prefix of the capacity available. This is going to lead us to our second attempt for the design of a dynamic programming algorithm for this problem. We're going to define the subproblems so that it considers a prefix of the objects and it varies the capacity available.

DP2 Knapsack Chain Multiply - 42 - Knapsack Subproblem 2
Now, let's revise our subproblem definition trying to utilize some of the insight we just gained. Now, our initial attempt at a subproblem definition was k of i is the max value we can obtain using a subset of the first i objects. Now, the problem was when we tried to express K of i in terms of the earlier subproblems K1 through KI minus 1, it wasn't suffice to have the solution to Ki minus 1. But in fact what we needed was the solutions to the i minus first subproblem; we need the solution to the i minus first subproblem with the additional restriction that the total weight is no longer, at most, capital b, but its, at most, capital b minus the weight of the ith object. Because we're going to try to include the ith object and therefore, a weight available for the i minus first subproblem goes down. So this might be a sub optimal solution when the weight is capital b, but we need the optimal solution for this restricted weight. Therefore, what we're going to do is we're going to have two parameters; i and b. i is going to specify the prefix of the objects that we're going to consider. And little b is going to specify the total weight available. So then we're going to have a two dimensional table. Ok? Let's go ahead and formalize this. We're going to have two parameters,I and b as we just said. And i is going to be restricted between 0 and n, just as before. And little P is going to be restricted between 0 and capital b. And we're going to define the entry KIB. This is the entry in our two dimensional table to be the max value which we can obtain using a subset of objects 1 through i that's a prefix of the objects just as before. And the additional restriction is that the total weight is at most little b. Our goal in this problem is to compute the entry K, n, capital_B. This is the bottom right corner of this table. This corresponds to the max value which we can obtain using a subset of all n objects, n with total weight at most capital b. This is the original problem that we're trying to solve.

DP2 Knapsack Chain Multiply - 43 - Knapsack Recurrence 2
Now, let's summarize the recurrence that we have. Now, a recurrence is going to have two scenarios. Either we include object i, or we don't include object i. First off, we have to know whether object i even fits in the backpack or not. If it doesn't fit, then we know we cannot exclude object i. So, we have to condition on whether the weight of the ith object, Wi, is smaller than b or not. If it is smaller than b, then the ith object can fit in the back pack. So, we're gonna take the best of the two scenarios, either including object i or not including object i. If we include object i, we gain value Vi, for object i, plus, we gain the value from the optimal solution to the subproblem which uses a subset of objects 1 through i-1 and has total capacity available b-Wi. The -Wi is because we included, we forced Wi and object i to be included in the backpack. The other scenario is that we don't include object i in the solution and then, the optimal solution to this subset of objects 1 though i is also going to be a subset of objects 1 through i-1, since object i is not being included. And the total capacity available, it stays the same. And we're going to take the best of these two scenarios, which means we're going to take the max of these two entries. In the other case, the weight of object i is strictly larger than b, and therefore it can't get included in the backpack. So then, our entry k(i, b) is just going to be the second scenario. This defines a recurrence, but to be complete, let's define the base cases and then we can go ahead and write the pseudo-code for our dynamic programming algorithm. For the first row of our table, then i=0. That means we're taking a subset of objects which are the empty set. We're taking a subset of the empty set. Therefore, there's no objects that can get included. So, the max value we can obtain is zero. Similarly, for the first column, we have total weight available, zero. Therefore, no objects can be included and therefore the max value we can obtain is, again, 0. Now, we can go ahead and write the pseudo-code for our algorithm, but let's take a look first at how we're going to do it. We have this table. It's a two dimensional table and we're going to fill this table row by row. And the point is, that when we fill the entry k(i, b), notice our recurrence, it always uses an entry from the previous row, either the entry right above is k(i-1, b) or an earlier entry in that previous row. So, if we filled the table row by row and the entries we need for the smaller subproblems for our recurrence will be there, already completed in the table.

DP2 Knapsack Chain Multiply - 44 - Knapsack DP Pseudocode
Now, let's write the pseudocode for our algorithm. We are solving the knapsack version where objects can get used in most once. So its the no repeat version of knapsack that we're solving. Now, the input to the algorithm are the weights for the N objects W1 through Wn, the values for the N objects, V1 through Vn, and the total capacity available capital B. Now let's start with the base cases which are going to be the first row in the first column of the table. For the first row of the table, as we mentioned before, the entries are all going to be zero because we have a subset of the empty set which we are using. The first column of the table, we have total capacity zero available. So once again, we have the max value is zero. Now let's fill the interior of the table and we'll do it row by row. So, we have a for loop where i varies from 1 to N. This will be the current row. And then we'll vary the parameter little b from 1 to capital B. This will bring us along the current row. To fill the entry K(i, b), we have to first check whether fits in the current capacity available which is little b. So, we need to check whether the weight of the ith object which is Wi is smaller than little b or not. If the weight is smaller, then we have two scenarios. We can either include object i, or we don't include object i, and we're going to take the best of those two scenarios. If we include object i, we gain value Vi4, and we gain the value from the optimal solution to a subset of the first i-1 objects, and with total capacity available B-Wi. Or if we don't include object i, we gain value K(i-1,b) which is the optimal solution, which is a subset of the first i-1 objects, with the same capacity available. And we are going to take the best of those two. So we're going to take the maxi of those two entries. And the other scenario where object i does not fit in the current capacity available, then we know that the entry K(i, b) is just the same as the previous row, K(i-1, b), since the optimal solution, since it doesn't include object i, will be a subset of the first i-1 objects. And finally, what is our algorithm going to return? It's going to return the bottom right entry of the table. This is the max value which we can obtain using a subset of objects 1 through N, and the total capacity capital B. This is our original problem that we're trying to solve, and that's the solution that we're trying to obtain. Now, we can go and look at the running time of our algorithm. We first have our base cases, the first for loop is over capital B entries, that's over the first row. That takes time, order capital B. Second for loop is over the first column, that's of size order N. Now we have our nested for loops which are going over the interior of the table. First one is of size order N, the second nested for loop is of size order Capital B. And then, within this nested for loops is an if-then-else statement which is going to be order one time. This is order one time. This one is order capital B. Our loop is of size order N. And so the total run time of these nested for loops is order N and times capital B. So, the total run time is order N times capital B. That completes the algorithm for the case where objects can be used at most once, and then we'll go back and we'll look at the solutions to the problem when we allow the object to be used multiple times.

DP2 Knapsack Chain Multiply - 45 - Knapsack in Poly-time Question
The running time with the algorithm we just described as order n time capital b. The question I'd like to ask is that is an efficient algorithm or not? What do we mean by efficient? By efficient, we typically mean polynomial time. What do we mean by polynomial? We mean polynomial in the input size. So, is this running time of this algorithm that we just described is a polynomial in the input size? So I'll let you answer, yes or no? Is it polynomial in the input size, the running time of our argument we just described? And if it's no, I want you to detail exactly why is it not polynomial in the input size, and detail what exactly would it mean to be polynomial in the input size? What would be the requirement on the running time for it to be polynomial in the input size?

DP2 Knapsack Chain Multiply - 46 - Knapsack in Poly-time Solution
Now, the answer is No. This running time is not polynomial in the input size. Why not? All right. This is a polynomial. I mean this is the polynomial in n capital B, but it's not a polynomial in the input size. Why not? The problem is this factor, capital B, if we wanna represent this number, capital B, it's just a number, right? How much space does it take to represent this number? The space required is the number of bits. What's the number of bits in capital B? It's log of capital B. So, the input to this problem is this number, capital B, and to represent his number, capital B is o to log B space. And so, the input size is o to log B. Now, of course, we also have n different numbers for the weights and the values. And those are gonna each take o to one bits for each of those numbers, and there's the o to n of those numbers for the two end weights and values. So, the input size is n and log B size. So, our goal is a running time which is polynomial in n and log B, whereas this is exponential in the input size. So, our running time is exponential in input size. Now, this is not surprising. Why not? What we're going to see is that knapsack is NP-complete. What does that mean? It might be that there is a polynomial time algorithm for this problem, but the fact is NP-complete means that, if we design a polynomial time algorithm for this problem, this Np-complete problem, then every problem in NP will have a polynomial time algorithm. So, it's unlikely that we're going to design a polynomial time algorithm for knapsack because that would imply polynomial time algorithm for a wealth of other problems. And many of them people have tried for many years, so it's unlikely that we're gonna design it right now with the simple dynamic programming algorithm. When we see this proof for this NP-completeness of the knapsack problem, it will be quite illuminating and you'll see why this algorithm is not efficient. Because what we'll do is we'll take a graph problem with n vertices, this will be a hard graph problem, and then we'll convert that into a knapsack problem. So, we reduce it to knapsack and we'll make a knapsack instance where this parameter capital B will be exponential in the graph size. So, this running time, where it will depends on capital B is polynomial, and capital B will give exponential running time for that original graph problem. And that will help it illustrate why this running time is exponential in the input size, whereas this is polynomial in the input size.

DP2 Knapsack Chain Multiply - 47 - Knapsack Repetition
Now, let's look at the other version of the knapsack problem. We gave a dynamic programming algorithm for the version of the problem where we have one copy of each object, so we can use each object at most, one time. Now, we'll look at the version of the problem where we have unlimited supply of every object. Here, we can use an object as many times as we'd like as opposed to the other version of the problem where we can use an object at most once. Now, let's go ahead with our recipe for designing a dynamic programming algorithm. The first step is to define the subproblem. And, let's go ahead and try the same subproblem as what we used for the other version of the problem and see if that works. Again, try to gain some insight. So, our subproblem for the previous version of knapsack was K(i,b) is the max value we can obtain using a subset of objects 1 through i with total weight, at most, little b. Now in this version, we're allowed to use objects multiple times. So instead of a subset where an object appears at most once, we're going to consider a multiset where an object can appear multiple times in the set. That's the only difference from the previous definition of the subproblem.

DP2 Knapsack Chain Multiply - 48 - Knapsack2 Recurrence
Now, let's go ahead and see if we can write a recurrence for this subproblem definition. So let's try to express K(i,b) in terms of smaller subproblems. We're going to try to use the insight that we had from the previous version of knapsack. So we're going to have two scenarios. Either we include object i or we don't include object i and we're going to take the best of those two so we're going to take the max. Now, as in the other version of knapsack, we're going to have two scenarios. In the other version of knapsack, we either included object i or we didn't include object i. In this version of the problem, we're going to have two scenarios. Either we include no more copies of object i or we're going to add in another copy of object i. Now, in the first scenario where we have no more copies of object i then the remainder of the set is going to be a subset or a multiset actually of objects 1 through i minus 1 with the total capacity available staying the same. Therefore the solution is k of i minus 1 B. Now in the other scenario where we're adding in another copy of object. And for that copy of object we get value v i. And in addition we get the optimal solution to this subproblem where the capacity went down by w i. So that capacity went down by w i. So the capacity available is now b minus w i. But notice here the first index is i, whereas in the other version of knapsack it was i minus 1 because in this version, we're allowed to use object i again even another copy, additional copies, whereas in the other version of knapsack once we use the object i which is what's happening in this case then we could no longer use object i. So this went down to i minus 1 to keep track that we didn't allow ourselves to use it multiple times. Now let's take a look. Is this recurrence in fact a valid recurrence? Are we expressing this current subproblem in terms of smaller subproblems? Previously, when we wrote recurrence for the current entry we always expressed it in terms of entries in previous rows. So this would be this one is in row i and the previous ones were in a row i minus 1. But in this case, we're actually using the same row. Let's look at the table. We're going to fill this table row by row and when we get to this entry, k i b, this current entry, okay, we've filled up the previous rows and we filled up this current row up to that entry. Now, what entries do does this recurrence require? Well, it requires the entry which is one row above. And in addition, it requires the entry which is in the same row but it's earlier in that row. So that will already be completed in the table. So these two entries that are acquired by this recurrence are already completed by the time we get to this current entry, k i b. So it's a valid recurrence that expresses k i b the current subproblem in terms of smaller subproblems. So we can go ahead and actually use the same pseudocode as before with the slightly different recurrence. Of course, we also have to condition on we have to make sure that the i object fits in the remainder remaining capacity. So we can only do this case when this holds. So if w i is the most b, then we take the best of these two scenarios. If w i is bigger than b then we can only use this case just as before and the other um pseudocode for the other version of knapsack. And what's our running time going to be? Well, we got a table of size n times b. That's the size of our table and to fill each entry it's going to take us order one time. So a running time is going to be order and times capital B just as before. So I won't go I won't detail it because it's almost the same pseudo code is for the other version of knapsack.

DP2 Knapsack Chain Multiply - 49 - Knapsack2 Recap
Let's take a look at this algorithm for a moment. Often, when we get a solution which uses a two or three dimensional table, it's useful to look at it and see if we can simplify it to get a smaller table. And we might get a faster or less space or just a simpler solution. Okay? And that's what we're going to try to do here. So, look at our solution here. Now, why do we have this parameter i? The point of the parameter i in the original version of the knapsack problem, was to keep track of which objects we've considered or not. So, after we consider object i, then we can look at the first i minus 1 objects and look at a subset of those. But in this version of knapsack, we're allowed to use the object multiple times. So actually, it's not at all clear that we need to consider this parameter i. And in fact, we can get rid of it. So, let's write a new version of knapsack solution which, for this version, we're only going to have a single parameter. So, we'll just have a parameter for the weight available and we'll drop this parameter for the subset of objects that we consider.

DP2 Knapsack Chain Multiply - 50 - Knapsack2 Simpler Subproblem
So, let's try to do our dynamic programming solution to this version of knapsack where we have a single parameter. The single parameter is going to be, little b, corresponding to the total weight available. And this little b is going to vary between the maximum capacity available, capital B, and zero. Now, we can define our subproblem. We define K(b) as the max value obtainable using total weight, at most, little b. And we allow ourselves to use a subset of all n objects or actually a multiset of all n objects, whereas, in the previous subproblem, we have an extra parameter i, and we only allowed ourselves to use a subset of the first i objects. I was trying to write a recurrence for this new subproblem definition. For the previous subproblem definition, in order to write a recurrence, we decided whether to include object i or not. Now, in this case, we don't have an object i, the last object. So, we want to try all possibilities for the last object to add. So, the recurrence for k of b is going to be, we're going to try all possibilities for the last object to add and we're going to take the best of those. How do we get the best? We take the max, and we'll use i to denote the last object that we're trying to add. So, last object that we're going at is going to be object i, and we'll consider all i between one and n. If we add in object i, we gain value, Vi. And in addition, we gain the optimal solution to the subproblem where the total weight goes down by Wi. This is expressed in K(b-Wi). And we're trying all possibilities for i between one and n. But we need that the ith object fits in the backpack. We can have this weight, this could possibly be a negative number. So, we need that Wi's and most little b. So, we're trying all possibilities for the last object to add where that last object can be anything between object one and object n, trying all these n objects. And, if that object fits in the current capacity, so Wi is smaller than Little b. We look at adding that objects. So, we gain value Vi4 and then our capacity available goes down by Wi. So, with the remaining capacity, we take the best solution. Now, since it's a one-dimensional table, be a straightforward to write the pseudo code. The table is one dimensional. There's not much choice in how we fill up the table. We're just going to fill it starting from K of 0 up to K of B. An this last entry is the solution to our problem. Let's go ahead and write the pseudo code for this algorithm just to detail it.

DP2 Knapsack Chain Multiply - 51 - Knapsack2 Pseudocode
So, here's a Pseudocode for our repeat solution for this version of knapsack. This is a knapsack version where we allow objects to be used multiple times. So, the repeat version of the knapsack. As before, the input to the problem are the weights of the n objects W1 through Wn, the values of the n objects V1 through Vn, and the total capacity available capital B. Now, it's a one-dimensional table so has no base case to worry about. And now, we're just going to go through that one-dimensional array from bottom up. Little b is going to be the index for our current position in the array. We start off by setting it equal to zero, in case there are no objects which fit in the current capacity available. Now, we go through each object and we consider that object as a last object to add it in the backpack. And we see if that gives us a better solution than anything we've obtained before. First, we need to check whether this object, object i, fits in the current capacity of (l,b). So, it's if Wi is, at most, little b. And now, if it is, we check whether this obtains a solution which is better than anything we've seen previously for this index. So, the previous best solution is K(b) and the new solution we obtain is Vi, for adding object i, plus the best solution for capacity (b-Wi) which is K(B-Wi). So, this is the solution we obtain now by using object i. And this is the previous best solution. So, if the new solution is better than the previous best, then we're going to update the current best. Now finally, we just returned the last entry of the table and that's our solution to our problem. That's the max value we can obtain using total weight, at most, capital B, which is the solution to the original problem.

DP2 Knapsack Chain Multiply - 52 - Knapsack2 Running Time
Finally, let's take a look at the writing time. We have this one for loop, which is a size order of capital B. Then, we have a nested for loop inside, it's of size order N. Each step, in this nested for loop, takes order one time. Therefore, the total run time is, order N times capital B. It's the same run-time as the original solution we had to this version. The space is smaller, and also it's simpler solution. It's a very simple algorithm, just a one dimensional table.

DP2 Knapsack Chain Multiply - 53 - Knapsack2 Traceback
So, to output the actual multi-set, which obtains the optimal solution what we need to keep track of is, what is the last object we add in? What is the object I, which we add in, which obtains this optimal solution. In order to maintain that, we make a separate array, S. We initialize S(b)=0 corresponding to the empty set solution. And then when we update our current solution. So, when we get into this if then statement then we set S(b)=i corresponding to that the optimal solution for this subproblem is obtained by adding object I and then re-cursing on this smaller subproblem. Now, we can use this set S to backtrack. And, so, we can use it to produce a multi-set, which obtains the maximum value. The details of that backtracking are similar to what we did for longest common sub-sequence.

DP2 Knapsack Chain Multiply - 54 - Chain Matrix Multiply
Our next dynamic programming problem is chain matrix multiply. This one will be a little different style from some of our early example. Actually, the solution will be a bit more complicated than the earlier examples that we looked at. So, let's look at a specific example so we can motivate this problem and then we'll go back and define the general problem. Our example will have four matrices A, B, C, D. Think of these matrices as having integer values for the entries. Our goal is to compute the product of these matrices A times B times C times D. And we'd like to do this in the most efficient manner possible. What exactly do we mean by most efficient? Let's look at a specific example. Let's say, A has of size 50 by 20, so it has 50 rows and 20 columns, B is of size 20 by one, C is of size 1 by 10, D is of size 10 by 100. Notice one thing, the number of columns of A has to match the number of rows of B. Also columns B has to match rows of C. Columns of C has to match rows of D. Why is that? When we multiply A x B, what we do is we take a row of A, this is A, and we take a column of B and we take the inner product. So we multiply entering and then we add it to the product of these, racks of these and so on. So the number of entries in this row has to equal the number entries in this column. And then we move onto the next row with the next column, next row with the next column. So this row has to have the same number of entries as this column. That's why columns here has to equal the number of rows here.

DP2 Knapsack Chain Multiply - 55 - Order of Operation
Recall, our goal is to compute A x B x C x D. Now, matrix multiplication is associative. So, there are many ways to compute them. The standard ways to compute A x B. Take that product, x C, take that product, x D. But there are other things we can do. For instance, we can do A x B first, then we can do C x D second, and then we can multiply these two together. Or we can start with B x C, multiply that with A, and finally, multiply that with D. Or you start with C x D, multiply that with B, finally multiply that with A. Which of these is best? That's what we want to determine. Which is the best or what is the cost of the optimal parenthesization. In order to figure out which is the best or most efficient method for computing the product of these matrices, we need to assign a cost for each of these operations. So, let's take a look again at matrix multiplication and then we can figure out a reasonable notion of cost.

DP2 Knapsack Chain Multiply - 56 - Cost for Matrix Multiply
Let's take two matrices, w, and y, where w is of size a times b. So, it's got A rows and B columns. And Y is of size b time c. So, it's got the rows and c columns. And this will create the product of these matrices. So, we're looking at Z, which is W times Y. Now, note that Z is going to be of size a times c. It's going to have A rows, it's going to have the same number of rows as W and it's going to have C columns the same number of columns as Y. Now, let's look at the multiplication a little more carefully. Now, here's an example where we're multiplying W times Y, W has a lot of rows and a few columns, B columns. Similarly, Y has a few rows, it has B rows, and it's got a lot of columns, C. The product matrix, Z, is going to have A rows and C columns. Let's look at a specific entry Z, i, j. Row i. Column j. How do we get this entry? What we do is, we take row i from W and we take column j from Y and do the inner product of those entries. So, we move along the row and the column, K going from one to b and we take the Kth entry of this row times the Kth entry of this column. We multiply those together and then we take the sum of these b terms. So, to compute this one entry Z, i, j, it took us b multiplications and b-1 editions. Now, Z has a, c entries. So, in total, there will be a, c, b multiplications and there'll be roughly the same number of additions. So, therefore, we're going to say the cost of multiplying these matrices is a, b, c. Since these two terms are about the same and multiplications take longer than additions. So, this is a dominant factor. So, the cost of computing Z, the product matrix, W times Y is a, b, c where W has size a times b and Y has size b times c.

DP2 Knapsack Chain Multiply - 57 - General Problem
In the general problem, we have N matrices A1, A2, up to An. Our goal is to determine the minimum cost for computing the product of these N matrices. Now, the key parameter is the sizes of these matrices. So, we'll denote the size of the Aith matrix, Ai, as Mi-1 rows and Mi columns. So A1 will be of size M0 by M1. It has N0 rows and M1 columns. A2 will have M1 rows and M2 columns. So, the number of columns of A1 matches the number of rows of A2. Finally, the last Matrix has MN-1 rows and MN columns. All we need for this problem is the sizes of these matrices. We don't need to know the entries of the matrices given our cost. Our cost just depends on the sizes of matrices. Therefore, the input to the problem are these sizes. These N+1 parameters defining the sizes of these N matrices. And our goal is to find the minimum cost for computing the product of these N matrices. And we're just putting the minimum cost, if we can do that, then we can go back and figure out the parentheseszation, which realizes that minimum cost.

DP2 Knapsack Chain Multiply - 58 - Graphical View
To get some intuition for this problem, I want to look at an alternative representation of the problem and instead of looking at it as parenthesization we're going to represent it as a binary tree. Let's go back and look at our earlier example to see what we mean here. In our earlier example, we were looking at the product of four matrices, A times B times C times D. Now, the standard way of computing this was A times B and take that times C and then take that times D. Now we're going to represent this as a binary tree. The leaves of the trees are going to be the four matrices and the internal nodes are going to represent the intermediate computations. So, the root is going to represent the final computation, A times B times C times D. Now, for this parenthesization our first computation is A times B. So, we have the leaves for A and B and our first computation corresponds to the parent of those leaves, which corresponds to A Times B. Then, we take that matrix and multiply it by C. So, we have to leaf for C and then we have the parent of A times B and C, which corresponds to A times B times C. This internal node corresponds to A times B times C. How the subtree is structured tells us how the parenthesization is done for this subproblem. Finally, we take A times B times C and we multiply by D. So, this tree captures this parenthesization. Here's another parenthesization we can do A times B and we can do C times D and then we multiply those two together. This is represented by the following tree. We first compute A times B. Then we compute C times D. So, this represents A times B. This represents C times D. And then, we multiply those two together. This gives us our final matrix, A times B times C times D. So, the roots of these two trees represent the same product, A times B times C times D. How this subtree is structured tells us the parenthesization.

DP2 Knapsack Chain Multiply - 59 - Chain Multiply Prefixes
Now, let's go ahead and try to define our dynamic programming algorithm for this problem. The first step in our recipe is to define the subproblem in words and we'll always try prefixes as our first attempt. Therefore, we let c of i be the minimum cost for computing the product of the first i matrices in the input. Now, let's go back and see if we can define a recurrence for this subproblem definition. Let's look at our graphical view, our root, which we're trying to compute is A1 times A2 times up to An, the product of these n matrices. In our graphical view, we're going to have a left child and right child. The left child is going to correspond to some prefix. It's going to be the product of A1 times A2 up to Ai for some i. The right child is going to correspond to the product of Ai plus one up to An. Now, let's look at a recurrence which is going to tell us the minimum cost for computing A1 times A2 up through An. What we're going to do is we're going to try all possibilities for the split i and then we're going to recursively look up what is the minimal cost for computing this subtree, which has root A1 times A2 up to Ai. And we're going to look up, hopefully, recursively, what is the minimum cost for computing this subtree, which has a root A plus one through An. Now, we're aiming for prefixes, but this subproblem is a suffix. So, you might think, well, instead of just doing prefixes, why don't we just do prefixes and suffixes? Well, let's go one more level in this tree, in this binary tree and we'll see that it gets worse. Let's look at the children of this node. There's going to be some split here at some j, index j and the left's child is gonna correspond to the product of Ai plus one up to Aj and the right child is going to correspond to the product of Aj plus one up to An. Now, we'd like to try all possibilities for the split index j and then we'd like to look up in our table the minimum cost for this subtree and the minimum costs for this subtree. Well, this subtree is a suffix. That's good. But what is this tree? This is not a prefix or suffix. This is a substring. That's the key thing, is that all the intermediate computations are going to correspond to substrings. It's going to be some index i and some index j. And we're going to look at the product from i to j. And this is going to suffice to consider substrings. So, we're going to have to go back and revise our subproblem definitions, so that we don't consider prefixes, we're going to look at substring-

DP2 Knapsack Chain Multiply - 60 - Chain Multiply Substrings
There's going to be two parameters, I and J. I is going to be the start of the substring, j is going to be the end of the substring. So, i is between j and one, and j is between i and n. And then we're going to define our subproblem as C(i,j) is going to be the minimum cost for computing the product of the matrices Ai through Aj. Now, let's try to write recurrence for C(i,j). Let's start with the base case. What is the easiest case to compute for C(i,j)? That's the case when i equals j. Then we're just computing Ai. So, the entry C(i,i ) what's the cost for? It's zero, because there's no work to be done. Let's look at our matrix actually, what are these base cases correspond to? Your matrix C and these are the diagonal entries. And notice, we're computing the entries where j is at least i. So, we're just trying to do this upper diagonal over here. We're not trying to compute this lower diagonal of the matrix. Now, let's try to do in general what the recurrence for C(i,j) is.

DP2 Knapsack Chain Multiply - 61 - Chain Multiply Recurrence
We're trying to find recurrence for the entry C(i,j). This corresponds to computing the product of the matrices defined by the substring from i to j. Let's go back and look at our graphical representation. The root of the tree that we're trying to compute, corresponds to the product of the matrices Ai through Aj. Now, what are we trying to do? We're trying to find the split. Let's say at L and then the left subtree corresponds to the product of the matrices Ai through Al. The right subtree corresponds to the product of the matrices Al +1 through Aj. How is our recurrence going to work? We're going to try all possibilities for this index L for the split, and then we're going to look up in our table to minimum cost for computing this subtree, which corresponds to a smaller substring, and we're going to look up in our table the minimal costs for computing this subtree, and then we're going to combine those together. How much does it cost to combine them together? Well, this matrix is of size.Mi -1 by Ml, and this matrix is of size Ml by Mj. Sort of multiply these together it cost Mi -1 times Ml times Mj. So computing the root cause, this amount computing the left subtree costs, this entry corresponding to this substring which is C(i,l) and similarly, the right subtree corresponds to the entry Cl +1 j. So the total cost for this split at index L is the sum of these three. And what are we going to do? We're going to try all possibilities for L and we're going to take the one which has minimum sum.

DP2 Knapsack Chain Multiply - 62 - Chain Multiply Summary
Now, let's go ahead and write a recurrence for C(i,j). In our graphical view, this corresponds to the root of the tree which corresponds to Ai through Aj. We're going to try all possibilities for the split at index L and we're going to take the best of those. The best means minimum costs, so we're going to do a minimization over the choices of L and L is allowed to vary between i and j minus one. And we have this left subtree and this right subtree. The left subtree corresponds to Ai through Al. The right subtree corresponds to Ai plus one through Aj. The minimum cost for computing this left subtree is the entry C(i,l). The minimum costs for computing this right subtree is Cl plus 1j. Finally, we have to combine these together. Recall this product matrix is of size Mi minus one by Ml and this product matrix is of size Ml by Mj. To multiply these together, the cost is Mi minus one times Ml times Mj. Adding that term into our recurrence. We take the mean over the choices of L where L can vary from i to j minus one. And for that specific L, the cost is the cost for the left subtree C(i,l) plus the costs for the optimal right subtrees CL plus one j plus the cost of merging that left subtree with that right subtree which is Mi minus one times Ml times Mj. We take the sum of those three terms and we take the L which minimizes that sum some That's our recurrence for C(i,j).

DP2 Knapsack Chain Multiply - 63 - Filling the Table
Before we detail the Pseudocode for this dynamic programming algorithm, let's go back and look at our recurrence a little more carefully, and see how we're going to fill the table up. This recurrence is a little different from earlier examples, so how we're going to fill the table up is actually going to be a little bit more complicated than before. We're looking at this two-dimensional table C, and we're trying to compute the upper diagonal of this table. So those entries where j is at least i. Now, what was our base case? Our base case was diagonal, these are the entries C(i, i). This is the first thing we're going to fill in. What is the next thing that we're going to fill in? The next entries we're going to fill in are the entries C(i,i+1). Look at the recurrence for these entries. L is going to vary between i and that's it, that's the only choice for L. And then what are subproblems looks like? Our subproblems are C(i,i) and (i+1, i+1). So to compute this entry, we use these diagonal entries which are there in our base case. What is this? What are these entries correspond to in our table? These are the off diagonals, these are the second type of entries that we're going to fill in. So we're going to first do the diagonal and then we're going to do these off diagonal. And in order to compute the off diagonal, we use the diagonal entries. What is the next ones we're going to do? C(i,i+2). Look at the recurrence in order to compute these, we're going to use either diagonal entries or the off diagonal entries. So there are going to be there on the table. Finally, what is the last one we're going to compute? It's this one right here that corresponds to C(1,n) what is that? That's our final answer. That's the one we're trying to compute. This is the minimum cost to compute the product of matrices from A_1 up to A_n. So what our algorithm is going to do? It's going to start at this diagonal, and then it's going to move up, okay? How do we index that in our algorithm? Well, look. Look at this difference between the j and i. Let's call it the width and let's call that S. So S is j minus i. For the diagonal entries which are a base case, the width is S = 0. The off diagonals which we do next are have S = 1, they have width one, then we have width two, and so on until we get to width n-1. So we're going to vary the width from zero up to n-1. Now, we can go ahead and detail our Pseudocode for our dynamic programming algorithm.

DP2 Knapsack Chain Multiply - 64 - Chain Multiply DP Pseudocode
Now, let's go ahead and detail the pseudocode for our dynamic programming algorithm to compute the minimum cost for multiplying these and matrices. Recall the input to the problem are the sizes. These n+1 numbers representing the sizes of the N matrices. M zero, M one, up to Mn. Let's start with the base case which corresponds to diagonal entering. The cost for these diagonal entries is zero since there's no computation to be done. Now, we're going to use our width parameter S. We already did the case where the width is zero. So we're going to start with one and go up two with n-1 which is our final solution. Then we have a parameter i which corresponds to the row. Notice that the rows are getting truncated at the end. Let's look at our matrix just to see what we mean by this. Our diagonal as these entries. And then, when we do the off diagonal, I'm going to start at this entry one, two and is going to end at this entry n-1. So it doesn't go down to the bottom row, okay? So that's why it stops at n-s. Once it tell you the index i and it tell you the width, then that defines the index j which is the end of the substring. Therefore, we let j, b, i + s. Now we're going to compute the entry c, i, j. We're going to take a min and we're going to vary over L and keep track of the current min so far. So we're going to initialize the min, the value, the current minimum to infinity. If using infinity makes you uncomfortable you can think of setting this to some huge number. Now, we're going to vary over the choices for the split at L. Recall L can vary between i and j -1. Now, for that split at L let's look at the cost, let's define a variable cur which is the current cost for the current index L. Because Mi -1 ml mj to combine the left and right subtrees. C(i,l) for the left subtree and C(l+1,j) for the right subtree. And we want to compare this to the current best. So the current best is larger than this value cur, then we're going to reset the current best to this current value. Then, the for loop. This is in this for loop, this is in this for loop. We have a bunch of nested for loops. Finally, what we return, our final answer is the top right of our matrix. We return this entry (C(1,n)) which corresponds to the cost, the minimum cost for computing the product of the matrices A1 through An. That completes our dynamic programming algorithm. Now let's take a look at the running time. We have this base case computation which takes order and time. Now, we have this first for loop which is of size order n. Then, we have another nested for loop which is the size at most n. And now we have another for loop which is of size at most n, again. And then within these for loops, it takes order one time for this computation. So we have three nested for loops of size order n each. So the total run time is order n q total time. So that completes our chain multiply dynamic programming algorithm. And the key thing here was that we, instead of using prefixes we had to move on to substrings for the subproblem definition. And then, how we filled in the table was a little bit more complicated. Usually, it's straightforward to fill in the table, we'd go row by row. But for this, when we were using substrings we have to go from the diagonal and then work our way up to the top right.

DP2 Knapsack Chain Multiply - 65 - DP2 Practice Problems
At this point you're prepared to approach any of the problems in chapter six of the textbook. But let me point out a few particular ones which are especially useful. Once again, these are problems from the desk Gupta Papademetriou Abaza Ronnie algorithms textbook and I'm using the numbering from the print version. Problem 6.17, problem 17 in chapter 6, is about change making. Given a set of coins, a set of denominations and a particular value, can you make change for that value using that set of denominations? In fact, there are three variants of the change making problem in the textbook. I suggest doing all three. Another good problem is problem 20 which is about building an optimal binary search tree. There's problem 7 which is about finding the longest palindrome subsequence, and you might also try the variant where instead of doing a palindrome subsequence you look for a palindrome substring. So they have to be contiguous. Now, to summarize some of what we learned in this lecture, when you're devising your subproblem, try prefixes first. If that doesn't work, you might be led in the direction of substrings. Now, one important note to keep in mind is that if you do use substrings and you get it to work and you get a valid solution, then I often go back and look at whether substrings were actually necessary or could I have simplified it and used prefixes. This might lead to a faster algorithm, but it's good to get a valid solution first. Get a polynomial time algorithm using substrings if necessary and then go back and check and think about whether actually substrings were required or not. It's better to have a correct algorithm which is a bit slower than an incorrect algorithm. Once again, the key for getting fluent in dynamic programming is to do lots of practice problems. There are a lot of practice problems in the textbook, but there are a lot available in the web too from other courses and from other books. Do as many as you can and at some point you'll get the hang of it and they'll feel easy. The solutions will start to seem similar to each other but the only way to get to that point is to do lots of practice problems. So good luck. I hope you start to enjoy it once you get.

DP3 Shortest Paths - 66 - Shortest Paths via DP
In this lecture, we'll look at several versions of shortest path problems and we'll use dynamic programming to design fast algorithms for these problems. The setting is that we have a directed graph G and we'll put the arrow on top to denote that it's a directed graph. And in addition, we have weights on the edges which is denoted by W(e). Here's an example of a directed graph on six vertices. Let's add in some edge weights. So this edge from s to b has weight five, three and some of the edges will have negative weights. So let's assign this edge from A to E weight negative two and we can have these anti-parallel edges. So we have an edge from A to D and from D to A and they might have the same or different weights. Now, these anti-parallel edges such as from A to D and D to A are quite useful. They allow us to encode an undirected graph as a directed graph. So if we have an undirected graph, we can replace the edge between A and D by this pair of anti-parallel edges. And in this way, this directed graph problem is more general than the undirected graph problem because we can encode any undirected graph as a directed graph by replacing each edge in this undirected graph by this pair of anti-parallel edges. In our first problem, we have a designated start for text which will denote as S. So let's fix our start vertex S and we're going to look at the length of the shortest path from S to every other vertex in this graph. Therefore, we're going to define the following function. So for every other vertex in the graph will denoted by Z. We're going to define this function dist(z). This is defined as the length of the shortest path from s to z. Now, dist(z) is defined for every vertex in the graph. So it's an array of length n. And our goal is to compute this array. To compute the distance of Z for every vertex Z. So we want to output this array of length n. Let's take a look at this function for this example. The simplest case is dist(s). What's the length of the shortest path from s to itself? Well, this is length zero. What's the length from s to b? Well, it's five. S to a, is of length eight, five plus three. The shortest path length to e is of length six and so on. The shortest path to D is 12 The shortest path to F is 11. The classical algorithm for this problem is Dijkstra's algorithm. You probably seen it many times before. Now, I'm not going to subject you to another lecture about Dijkstra's algorithm but let me give you a quick recap or a quick reminder about what Dijkstra's algorithm accomplishes. So Dijkstra's algorithm takes a directed graph with edge weights and a designated start vertex. That's the input to Dijkstra's algorithm. The output is this distance array. It outputs the distance of Z for all vertices in the graph. Once again, this is the length of the shortest path from s to this vertex Z. Now, how does Dijkstra's algorithm work? Well, it works in a manner similar to BFS, Breath For Search. It explores the graph in this layered approach. Now, recall BFS as in DFS takes linear time, so takes order (n+m) time, where n is the number of vertices and m is the number of edges. Now, in Dijkstra's algorithm, we have weights on the edges so we have to use a min heap data structure or priority queue. Each operation in these data structures takes order log(n) time. So there's an additional overhead over BFS and Dijkstra's algorithm takes (n+m) times log (n). And this is the total runtime for Dijkstra's algorithm to find this distance array. Now, there is one big limitation in Dijkstra's algorithm, it requires that all the edge weights are positive. Now, why does it require positive edge lengths? Well, because it might find this distance to be S five. But if there is negative edge weights, then it might find a shorter path to b. But once it outputs its distance, it doesn't recompute a shorter path, is it finds a new shorter path to B then it has to re-explore the edges out of that vertex again. So Dijkstra's algorithm is not guaranteed to produce the correct output when the edge weights are allowed to be negative. We're going to look at this more general problem where the edge weights are allowed to be negative, such as in this example. And we're going to design a dynamic programming algorithm to solve the shortest path problem when the edge weights are allowed to be negative because Dijkstra's algorithm no longer is guaranteed to work.

DP3 Shortest Paths - 67 - Negative Weight Cycles
We're looking once again at a directed graph with edge weights where the edge weights are allowed to be positive or negative such as in this example. And for a designated start for text S, we want to find the shortest path from S to every other vertex. And our first question is whether this problem is well-defined. In this example, it is. We already looked at what the distance is from S to every other vertex. Let's modify this example a little bit. Let's make this negative weight a little bit more extreme. Let me change the length of this edge from A to E. From length, -2 and we'll make it -6. Now, let's take a look at this example. And what is the length of the shortest path from S to D? Where previously it was length 11, because we went S to B to A to D. That has 5+3+3, so it's length, 11. But look at this cycle which goes B, A, E, B. What's the length of this cycle? It's length -1. So every time we go around this cycle, our length goes down by one. So an alternative route from S to D is to go S to B to A to E to B, A, D. So we're gonna go here. We're gonna go round the cycle and then follow this path. What's the length of this walk? It's length 10 because we went down by one by going around this cycle. Similarly, we can repeat this cycle many times. Every time we repeat it, our length goes down by -1. So the shortest route from S to D goes around this cycle infinite number of times. Now, this is a walk, it's not a path. Because we're allowed to repeat vertices in this case. Now, the shortest path in this example is S to B to A to D. That's if we're only allowed to visit a vertex at most once. But if we're allowed to repeat vertices, I mean why not in this example? Then, we want to go around this cycle infinite number of times. This cycle is a negative weight cycle. Cycle going from A to E to B and back to A. A to E to B and back A. To some of the weights along this cycle is less than zero. So it's a negative weight cycle will define it as. Now when a graph has a negative weight cycle, then the shortest path problem is not well-defined any longer. But if it has such a negative weight cycle, then it's interesting to find such a negative weight cycle. So let's change the problem. Let's look at the more general problem. As given a directed graph with edge weights, let's find a negative weight cycle if one exists in the graph. What if there is no negative weight cycle? Well then, the shortest path problem is well defined and we'll solve the shortest path problem. So we're going to solve this more general problem. We're giving a directed graph G with edge weights and the edge weights can be allowed to be positive or negative. And we're also given this designated start for text S. And our goal is to find a negative weight cycle if one exists in the graph. Actually, to be more precise, we're going to find a negative weight cycle which is reachable from S. If there's a negative weight cycle in the graph but it's not reachable from S, then it doesn't have play any role in these distance factors. So we're going to find a negative weight cycle if one exists in the graph. And what if there's no negative weight cycle in this graph? Then for every vertex in the graph, we're going to find its distance. The shortest path length from S to this vertex. So we're going to output this array of size. And let's look at how to use dynamic programming to solve this problem.

DP3 Shortest Paths - 68 - Single Source Subproblem
So let's design a dynamic programming algorithm for the single source shortest path problem. We're given as input a graph and it's a directed graph. And the edges of the graph are waited, and they are waited arbitrarily so they can be positive, some can be negative, so we can no longer apply Dijkstra's algorithm. And we have some specified start for text S, and our goal is to find the shortest path from S to every other vertex. Now what about negative weight cycles? Let's assume for now that there are no negative weight cycles in the graph and therefore, the shortest path length from S to every other vertex is well-defined. We're going to visit every vertex at most once. We'll see how to solve this problem and then we'll see a slight tweak of the algorithm will detect whether there exists a negative weight cycle or not. But for now, let's assume there's no negative weight cycles in the graph. Problem is well-define. Since there are no negative weight cycles in the graph, as we just noticed, the shortest path from the start vertex S to any other particular vertex Z visits every vertex at most once. There's no reason to repeat a vertex because the cycles all have positive length. So let cal P denote the path, the particular path which is of shortest length from S to Z. If there's multiple ones let P be any particular one of shortest length. Since we visit every vertex at most once, what do we know about the length of P? P contains at most N minus one edges because we visit every vertex at most once. Now let's try to design a dynamic programming algorithm for this single source shortest path problem. Normally we try to use a prefix of the input in our dynamic programming algorithm. Here it's going to be a little different type of solution. Notice that the path length is at most N minus one edges. Let's try to use a prefix of the path. What do we mean by that? Let's try to condition on the number of edges in the path. What we're going to do is to introduce a variable I which is going to vary from zero to N minus one. And this is going to be the number of edges that we allow on the pass that we consider. When I equals N minus one, then we're going to allow the pass to be of length at most N minus one edges and that's going to solve the shortest path problem. It's going to be the final solution. At the beginning, the base case I equals zero, we don't allow any edges. That's more formally defined the subproblem for a dynamic programming algorithm. We're going to have two parameters, I and Z. I is going to vary between zero and N minus one and Z is a vertex of the graph. And we're going to find the function capital D of I, Z. This is going to denote the length of the shortest path from S to Z, but we only consider pass which use at most I edges. So when I is N minus one then this is the final solution that we're looking for and we're going to build up our solutions starting from I equal zero and building it up to N minus one. So now let's try to write a recurrence for D of I, Z. Our goal is to express D of I, Z in term of D of I minus-

DP3 Shortest Paths - 69 - Single Source Recurrence
Recall our subproblem definition from the previous slide. For each i between 0 and n minus 1 and each vertex z, we define D(i,z) as the length of the shortest path from s to that chosen vertex z, where we only consider paths which use the most i edges. The final solution that we're trying to find is, i equals n minus 1. Therefore, the base case is i equals 0. In this case, we're going from s and we're not allowed to use any edges. So, the only vertex we can reach is s itself and, therefore, D(0,s) is 0. The length to go from s to itself is length 0 and the length to go to any other vertex is infinite. So, for every other vertex except s the lane is infinite. That defines a base case. Now, let's try to do the case when i is at least 1. Now, let's look at the shortest path from s to z using exactly i edges. So, this path starts at s ends at z. There's some penultimate vertex, let's call it y. So, there's a path of length, i minus 1 from s to y. And then there's an edge from y to z. So, y to z is the last edge on this path and the prefix of the path is of length i minus 1 since the entire path is length i. Now, noticed our subproblem is slightly different from what we proposed before. Now, we're talking about exactly i edges, whereas, before we were talking at most i edges. Let's just look at this version for now. Can we write a recurrence now for D(i,z)? What we're going to do is we're going to try all possibilities for the last, the penultimate vertex on the path and we're going to take the best of those choices for y. What is the best? The best means shortest, which means minimum. So, we're going to minimize over the choices of y. What are the choices for y? Well, y has to have an edge to z. This is a directed graph, so we're considering those y were the directed edge from y to z in our input graph. What does the length of this path which goes through y? Well, from s to y it's allowed to use exactly i minus 1 edges. Therefore, it is D(i-1,y) because, it's the shortest path using exactly i minus 1 edges from s to y. Notice we're talking about exactly i minus 1. And again, slightly different from our original version, which said at most i. We're just going to ignore that slight difference for a moment. And in addition, we get the weight of this last edge from y to z. What is the length of this last match? It's W(y,z). That's the length of this edge from y to z. So, the length of this shortest path from s to z, which goes through y, as the penultimate vertex, has length D(i-1,y) for this first i minus 1 edges, as captured right here, plus the length of this last edge from y to z. And we're going to take that sum, which is the total length of this path from s to z going through y and we have minimized over the choices of y. And the best choice of y is going to give us this value, D(i,z). This gives us a valid recurrence when the subproblem is defined as the length of the shortest path from s to z using exactly i edges. Notice, in this case when we're looking for dist of z, this is not necessarily the shortest path using i equals n minus 1. We might not necessarily use n minus 1 edges in the shortest path from s to z. So, this is not stored in D(n-1,z). Instead, to find dist of z, we've got to look at the min overall choices of i. So, to avoid this we instead, want to store D(i,z) as the length of the shortest path using at most i minus 1 edges, instead of exactly i edges. Let's look. Is there a simple way to modify a recurrence so that it changes the subproblem definition to our original proposal? Using at most i edges instead of exactly i edges. All we have to do is we have to modify so that we take the best of this solution, which minimizes over y, and the previous solution, which was D(i-1,z). So, if this stores the length of the shortest path from s to z using at most i minus 1 edges, this gives the length of the shortest path using exactly i edges. If we take the min of these two values, then that will give us the length using at most i edges.

DP3 Shortest Paths - 70 - Single Source Summary
Let's recap and summarize our recurrence. We're looking the length of the shortest path from S to Z using at most I edges and we're considering the case where I is at least one. When I equals zero we have our base case. Now either the shortest path from S to Z uses the most I minus one edges or it uses exactly I edges. And we're going to take the best of those two scenarios. If it uses the most I minus one edges, then the solution is in D of I minus one Z. If it uses exactly I edges, then we're going to try all choices for the penultimate vertex Y and then we're going to take the length of the shortest path from S to Y using the most I minus one edges plus the length of the last edge. So we have a double min. We take the min of the I minus one case and the min over Y, choices for Y. And this defines a recurrence for D of I Z. Notice, in order to get this D for I Z, we're using entries which involve I minus one. So if we build up the table from I going from zero, which is the base case, to N minus one, we'll have a valid dynamic programming algorithm. So let's go ahead and detail-

DP3 Shortest Paths - 71 - Single Source Pseudocode
Now, let's just go ahead and detail the Pseudo-code for our dynamic programming algorithm for the shortest path, single source shortest path problem. This algorithm is called the Bellman-Ford algorithm. The input to the algorithm is a directed graph G, a star vertex S, and weights on the edges. This algorithm goes back to the 50s. It was devised by Richard Bellman and Lester Ford. Now one interesting note is, Richard Bellman who devised this algorithm, he's actually the one who developed dynamic programming approach in the 1940s, long before there was any programming on personal computers or anything like that. So let's start with the base case which is the case i=0. And we're going to initial D(o,s) so going from S to itself using in most zero edges that's going to be of length zero. Now, we're going to work up from i=1 up to i = n -1. We're going to go over the vertices of the graph Z. We're going to initialize D(i, z) to be D(i-1,z). This is saying, that if we look at the minimum length path using the most i edges, we're going to first consider using the most i-1 edges and then we'll look at the scenario using i edges. To consider the scenario where we use the i edges, we look at all choices for the penultimate vertex with this Y. And we're considering those Ys which have an edge from y to z. Now we look at the path using Y and whether that path using Y is better than the current best solution. The current best solution is stored in D(i,z) and the path through Y has length D(i,-1,y) plus the length of this last edge W(y,z). If the length of this solution through Y D(i,-1,y) + W(y,z) is better than the current best solution, then we're going to update the current solution. Finally, what do we return? We return the case where i = n -1. And we're going to return it for all values of z. I just put a dot to denote this array of size n. So D(n-1)is an array of size n or if you think of it as two-dimensional table we're returning the last row of the table. Let's take a look at the algorithm a little bit more in detail. There's one slightly non-trivial step. Here, we're looking at the edges into z. Normally, when we have adjacency list we look at the edges out of Z. So how do we get the edges into a vertex? What we do is we look at the adjacency list for the reverse graph, so we flip all the edges. To construct this graph takes linear time O(n+m) time. And then we take the adjacency list for this reverse graph and that's going to give us the edges into z because in the reverse graph these are the edges out of Z. The edges in the reverse graph which are out of z are the edges into z in the original graph. Now, let's look at the running time with this algorithm. We have a for loop over i from one to n -1. This for loop is over order n choices. Then within this for loop, we're going over the vertices a graph and then we go over the edges into that vertex. So, really the key step is right here, this for loop. How many choices are we going over here? We're going over all edges of the graph once. So this is order m choices here and within for each edge of the graph, we do a simple if then statement which is order one time. So the total time for this for loop for this nested pair of for loops, we go over the vertices of the graph and then we go over the edges into that vertex. So the nested for loops combined go over all edges of the graph exactly once. So these inner four loops take order m time and the outer for loop takes order n time. So the total time is order n times m. So this algorithm is actually slower than Dijkstra's algorithm but it allows negative weight edges. Also, in addition, it's going to allow us to find negative weight cycles. So far, we've been assuming that there's no negative weight cycles in the graph so that the shortest path from S to every other vertex is well defined. Now let's go back and see if we can figure out whether there is a negative weight cycle in the graph or not and detect it. And if there is one output it.

DP3 Shortest Paths - 72 - Finding Negative Wt. Cycle
How can we find whether a graph has a negative way cycle or not? Let's go back and look at our earlier example. Here's our earlier example on six vertices and it has a negative weight Cycle A B C A, which is of length negative one. So, what's going to happen for our algorithm, Bellman Ford algorithm that we just defined on this example? Let's look at the Bellman Ford algorithm on this example. We're going to have a two dimensional table. The columns the columns of the tables are going to be the vertices of the graph. There are six vertices of the graph, S, A, B, C, D, E. The rows of the table are going to correspond to the path lengths we consider. We're going to start with the base case, I equals 0. In the base case we have D of 0S as 0, and the other entries are infinite. Our algorithm can fill up the table from I equals 1, I equals 2, I equals 3, I equals 4, I equals 5. The current algorithm is going to finish at I equals 5, which is N minus 1 in this example. But let's do one more row of the table and see what happens. Now, let's go ahead and fill in the table by hand. In this particular example, the column for S is going to stay 0, since there's no edges into S. Now let's look at the row I equals 1. There's a path using one edge from S to A. That's of length 5. All of the vertices are inaccessible by at most one edge. So their length stays infinite. Now for I equals 2 we update the length to B to be 8. The other vertices stay infinite and A stays 5. For I equals 3 we can update the path length to C, which is it going to be of length 2 using the path from B minus 6. In addition we can update the path length to D. The path to B is A plus 4,12. And the other vertices are going to stay the same. Now for I equals 4, we can update the path length to E going through C which is of length 2 plus 5. That gives us a path of length 7 to E. Now the other interesting thing is, that there's a path to A itself. So we can go to C of length 2 and then we can go back along this edge from C to A, which is of length 2. And then we get a new path to A going along this cycle, which is of length 4. So that's a shorter path to A. The path length to A went down from 5 - 4 using this cycle as negative weight cycle. And the other Bursey stay the same. Now let's continue. Notice, now we can update the path length to B, because A's distance went down from 5 - 4. So now the path length to B using A goes down to 7. And the other vertices stay the same. Now since the path length to B went down, actually, we can update the path length to D and also to C. The path length to C now goes down to 1 and the path length to D goes down to 11, 7 - B plus 4 and the other vertices stay the same. Notice that when there is a negative weight cycle, then every row is going to be different from the previous row. It's going to be the next step on this cycle. And that next vertex on that cycle is going to have shorter path length than it previously had. So A is going to decrease, and then B is going to decrease, and then C is going to decrease, and then A is going to decrease, then B and C and A and so on. So every row is going to be different. So normally if there's no negative weight cycle, then notice our algorithm was going to stop at the case I equals N minus 1, which is I equals 5 in this example, because every path used a vertex at most once. So the maximum path length was at most five edges and minus one edges, but if there's a negative weight cycle then what we're going to have is that I equals N is going to be different. It's going to be smaller than I equals N minus 1, because every row is going to shrink further. So if there's no negative weight cycle and we can stop here. But if there is a negative weight cycle then what we're going to notice is that, I equals N is going to be different from I equals N minus one. So how do we detect a negative weight cycle? We compare these two rows and if I equals N is different from I equals N minus 1 then that shows us that there is a negative weight cycle. So how do we check that there's a negative weight cycle? We check if the entry D of NZ is smaller strictly smaller than D of N minus 1Z. For some Z, some vertex Z. So the- the row I equals N is different from the row I equals N minus 1 in some entry Z. If that is the case then there is a negative weight cycle and it involves Z. And actually we can backtrack, we can see that that cycle involved is vertex C and then we can see it involves B and A. So we can detect that cycle, C, B, A. But our check is just we take our algorithm from before, our Bellman Ford algorithm, which ran from I equals 1 to N minus 1 and instead we run it from I equals 1 to N and we check if the row I equals N is different from I equals N minus 1. If it is different, then we found a negative weight cycle. If it's not different, then we output the row I equals N minus 1 or the row I equals N, because they're both the same and that gives us the shortest path length from s to every other vertex. So that completes the dynamic programming algorithm known as Bellman Ford for finding the shortest path from a single source vertex and it allows positive and negative weight edges. And if there is negative weight edges, it can detect whether or not there is a negative weight cycle.

DP3 Shortest Paths - 73 - All Pairs Shortest Path
Let's look at one more variant of shortest path problem and this will give us a chance to look at a slightly different style of dynamic programming solution. So what we did before with Bellman-Ford was we had a single source and we looked at the shortest path from that single source to all other vertices. Now we're going to look at all pairs shortest path. Once again we're given a directed graph, G, along with edge weights. And these edge weights again can be positive or negative. Now, for a pair of vertices Y and Z, let's define dist of y, z to be the length of the shortest path from y to z. Previously, we are always looking at the shortest path from a single start vertex S to the given vertex. Now we're looking at for all pairs of vertices, we're looking at the shortest path. So we have this N squared size matrix dist. Our goal in this problem is to find the length of the shortest path between from y to z and we want to do it for all pairs of vertices Y and Z. So for all N squared vertices, we want to find the distance from Y to Z. Now last lecture we saw Bellman-Ford algorithm, which did the distance from a single start vertex s to all other vertices. Can we use that as an easy algorithm to solve this all pairs shortest path problem? Yes. So the easy approach to solve this all pairs shortest path problem is to run Bellman-Ford algorithm which has a single start vertex and we try all N possibilities for the start vertex. So we run Bellman-Ford-

DP3 Shortest Paths - 74 - Naive Approach Question
Let's pause for a quick quiz. So what was the running time of the Bellman-Ford algorithm? And now if we run Bellman-Ford algorithm N times, so we're running it for all N possible start vertices, what's the running time of this approach for the all pairs shortest path problem?

DP3 Shortest Paths - 75 - Naive Approach Solution
For Bellman-Ford, first off, takes O(nm) time for each run. Now, I'm terrible at memorizing these things, so actually, the name itself, Bellman-Ford, is not important. What's important is we had the single source shortest path algorithm using dynamic programming. And if you want to reconstruct the running time in your head, then what you can do is you can try to remember how it worked, the basic idea. So the basic idea was we conditioned on the number of edges in the path. So we went and we had this variable I which went from zero to n minus one. So we had a for loop over I, which had n choices. Okay, that's the order_n. And then in each step, we conditioned on the last edge in the path. So we had a for loop over the edges of the graph, and that gave us the m factor. So the total run time was O(nm). And now, we're running this algorithm n times, so the total run time is going to be order_n_squared_m. What we're going to do now is that direct algorithm for this problem, and this algorithm is called Floyd-Warshall algorithm, and the running time is going to be order_n_cubed. So it's better than this because in this case, m the number of edges of the graph can be up to n_squared. And if the graph is connected, it's going to be at least n minus one. So, this is possibly n to the fourth, whereas this is n_cubed. Now, just to clarify, I'm not expecting you to memorize the names of these algorithms. What I want you to do is to understand the basic approach in the two algorithms and then using that basic high level intuition, you can reconstruct the running time for these algorithms, and if you need be with enough sufficient time you can reconstruct the actual algorithms, okay? But I myself get confused all the time, which one is Bellman-Ford, which one is Floyd-Warshall, that's not the important aspect for here. I want you to understand the high level idea, and I'm illustrating these algorithms because they both have some nice dynamic programming approaches which are slightly different than the approaches that we've seen in the past. So now let's dive into this Floyd-Warshall algorithm and see how we get this n_cubed.

DP3 Shortest Paths - 76 - All Pairs Subproblem
Let's look at the basic idea for that dynamic programming algorithm we're going to construct. First, let's go back and look at the Bellman-Ford algorithm. The basic idea of this algorithm. This was the algorithm where we had a single start vertex and we're looking at the shortest path from that single start vertex, s, to all other vertices. Now, in that dynamic programming algorithm, what we did is we conditioned on the number of edges in length of the path. Obviously, we're going to try to do something different here. What else can we condition on instead of the number of edges? Actually, if you think of our basic dynamic programming approach that we always try, we try prefix of the input. Okay? What is the input here? The input here is the graph, or, one important aspect is the vertices of the graph. So, can we try a prefix of the vertices of the graph? Let's try to formalize that. First off, let's order the vertices, one through n. So, it just assigning numbers to the vertices one, two, up to n. How we do that doesn't matter. Okay? Just the point is, that now we can index the vertices by numbers one through n and we're often doing this anyways, implicitly because we have a table where we have a single one dimensional array for the vertices and we're indexing them by their numbers. Okay? The important thing now is that now we can look at a prefix of the vertices. So, we want to solve the same all pairs shortest path problem, where we only allow a prefix of the vertices to be used. So, we're going to condition on the intermediate vertices that are allowed to be used in the past that we consider. And we're going to go back and use prefixes as we've done them in many problems and we're going to use prefixes of the vertex then, okay? So, let's formalize this more precisely. So, we're going to have three parameters. First parameter is going to be the prefix of the vertex set that we consider. So, we're going to use the variable i for the prefix of the vertex set that we consider. This prefix is going to vary from zero, for the empty set, up to n. And for given i, we're going to consider the set of intermediate vertices one through i. That's going to be the set of allowable vertices to be used as intermediate vertices on the paths that we consider. Now, the other two parameters that we need are the start vertex and the end vertex. And we want to do all pairs, shortest path. So we want to find- try all possible start vertices and all possible end vertices. So let's use s for the start vertex and let's use t for the end vertex. And now, we want to vary s and t over all possible choices. So, we want to try each one through n. So s and t both vary between one and n and we want to try all n square choices for s and t. Finally, let's define our dynamic programming subproblem in words. It's going to be a three dimensional table, now. D of i, s, t, it's going to be the length of the shortest path from s to t. And now, how does i work in? Well, we're going to condition on the set of intermediate vertices that are allowed to be used. Now, the perimeter, i, tells us the prefix of the vertex set which are allowed to be used as intermediate vertices. So that we only consider paths from s to t, where a subset possibly empty set or possibly the whole thing, and we're not saying anything about the ordering of these intermediate vertices, but the only intermediate vertices that can be used on the path from s to t are one through i. And we're going to vary i from zero to n. When i equals n, that means we allow all vertices to be intermediate vertices on the path, so we're considering all paths. So then d of n, s, t, tells us the length of the shortest path from s to t. So, now, let's go ahead and try to find a recurrence for this subproblem definition that we just defined.

DP3 Shortest Paths - 77 - All Pairs Base Case Question
Let's go ahead and try to find the recurrence for the sub-problem we just defined. The sub-problem we just defined was D of IST. This is the length of the shortest path from vertex S to vertex T, where the set of allowed vertices on the path from S to T are a prefix of the vertex N. These are the first I vertices, one through I. These are the vertices that are allowed to be intermediate vertices on the path from S to T. Let's start with the base case. Why don't you go ahead and write down what is the base case and what is the solution to the base case? For intuition about the solution for the base case and what is the base case itself, think back to the Bellman-Ford algorithm for the single source problem.

DP3 Shortest Paths - 78 - All Pairs Base Case Solution
As in the Bellman-Ford algorithm, the base case is the situation when i equals zero. That's when we have the empty set as possible intermediate vertices. So we have to go directly from s to t without any intermediate vertices allowed.

DP3 Shortest Paths - 79 - All Pairs Recurrence
So we're going to have two situations, either s and t are connected by an edge and therefore, we don't need to use any intermediate vertices, or they're not connected by an edge and then there's no path from S to T. In the first case, if there's a directed edge from S to T, then D of 0, S, T is exactly the length of that edge from S to T which is W of S T. In the other scenario, there's no edge from S to T. In this case, then what is the length of the shortest path from S to T which doesn't allow any intermediate vertices. It's infinite. Let's look at the shortest path from S to T where we only allow vertices one through I as intermediate vertices. So this is the solution. This path P is the solution to this problem. And if there's multiple paths, just choose any path arbitrarily. So our goal is to figure out D of I, S, T. And we want to write recurrence for this so we want to do this for D of I, S, T in terms of smaller Is. Okay? So we're going to have two cases. We're considering this vertex as last vertex I. So what are the two cases for this path? Either this path uses vertex I or it doesn't use vertex I. Let's try the two cases separately. So what's the easy case? The easy case is when the path doesn't include vertex I and then the slightly harder cases when the path includes vertex I. Let's start with the case where vertex I is not on the path and in this case what is D of I-

DP3 Shortest Paths - 80 - Case i not on path Question
Let's pause for a moment and you go ahead and think about what is the solution to the recurrence in this case, where vertex i is not on the shortest path from s to t. And so, can we express D(i, s, t) in terms of smaller subproblem?

DP3 Shortest Paths - 81 - Case i not on path Solution
If for text i is not on the path P, then this path P only uses a subset of vertices one through i minus one, as intermediate vertices. Therefore, D of i, s, t in this case is the same as D of i minus 1, s, t. Since we're only using a subset of the first i minus one vertices intermediate vertices. The solution is obtained by this subproblem. The other case is when i is on this path. Let's dive into this a little bit more carefully.

DP3 Shortest Paths - 82 - Case i is on path
So we try to write Recurrence for D of i, s, t. We did the case where the vertex i is not on the shortest path from s to t. And now we want to do the case where vertex i is on the path from s to t. And we're talking about shortest path which only used vertices one through i as intermediate vertices. So, let's try to figure out the recurrence for this case where i is on the shortest path from s to t. Let's look at our path graphically. We're starting at s, we're ending at t. We know at some point in the middle, we go through vertex i. What are other vertices that we possibly use? We use a subset of vertices one through i as intermediate vertices. Vertex i is here, so the other vertices that we can possibly use are a subset of vertices one through i minus one. We don't know anything about that subset, what order or which particular vertices in that subset are used but, it's a subset of one through i minus one. Now, what does this path look like? It starts at s and then it goes through some subset of vertices one through i minus one. It might be the empty subset of these or might be all of them. We don't know. Just goes from s to some subset here then at some point, it visits vertex i. After it visits vertex i, what happens? It goes to some subset of vertices one through i minus one. Might be the empty subset, not sure. It might be the full subset. And then, afterwards what happens? It finishes at vertex t. So our path can be broken up into these four segments. S to this subset, then it's going, moving around in that subset. It might be an empty subset so in which case it's actually skipping, it's going directly from s to i. But it goes from s to this subset, towards around the subset, goes to i, back down and then goes around the subset and possibly empty, visit, and then back to t. So we have these four segments of our path. Let's rewrite this partition of the path into these four parts in words. So we're starting at vertex s, we visit some subset of vertices one through i minus one possibly empty subset. And we visit vertex i then we go back and visit some subset of vertices one through i minus one and then we go to vertex t. And we're trying to express d of i, s, t. Now, given our insight of breaking up this path from to s to t into these four parts, now it will be straightforward to write a recurrence for d of i, s,t.

DP3 Shortest Paths - 83 - Recurrence i is on path Question
Let's go ahead and take a break and see if you can express a recurrence for D of (i,s,t) in terms of smaller subproblems.

DP3 Shortest Paths - 84 - Recurrence i is on path Solution
Now, to write a recurrence for D of i, s, t, we want to use D of i minus one for some pair s and t, which might be different vertices. But the key thing is that we want to change this parameter i to i minus one. So how do we get that? Look at this path. This is a path from s to i which only uses a subset of the first i minus one vertices as intermediate vertices. So, that is D of i minus one s, i. That's a smaller subproblem which we've already solved in our dynamic programming algorithm. Similarly, this portion of the path is the shortest path from i to t which uses a subset of the first i minus one vertices as intermediate vertices. This is D of i minus one because the first i minus one vertices only are intermediate vertices on the path and it's starting at i and it's ending at t. So the total length of this path is the sum of these two terms. So now we have a recurrence with D of i, s, t. It's D of i minus one, s, i that gives us the shortest path from s to i using a subset of the first i minus one as intermediate vertices plus D of i minus one, i, t that gives us the path from i to t using this subset of one through i minus one as intermediate vertices on the path. So we have a recurrence for D of i, s, t in the case when i is in the path and we have the recurrence for the case when i is not in the path. What do we do? We take the best of those two sce-

DP3 Shortest Paths - 85 - Recurrence Summary
So, we've handled the case where i is on the path. We've written the recurrence for D(i,s,t) and we've handled the case where i is not on the path on the previous slide. So, let's go ahead and summarize the recurrence for D(i,s,t). We have two scenarios, i is on the path or its not on the path. We're going to take the best of those two. So, we're going to take the min, because we're trying to find the shortest path. When i is not on the path, recall it's just D(i-1, s,t). When i is on the path we have these two terms. We get the shortest path from S to i using the first i-1 as our main vertices. Then we go from i to t. This is our recurrence for D(i,s,t) in terms of smaller subproblems. Notice, its using D(i,s,t) is using D(i-1). So, if we go for i going from 0 up to n, then in order to solve D(i,s,t), it will use smaller subproblems which are already solved in our dynamic programming algorithm. So, let's go ahead and write the pseudo code for this dynamic programming algorithm.

DP3 Shortest Paths - 86 - All Pairs Pseudocode
So now we can write our pseudo code for the all pairs shortest path problem. This algorithm is called the Floyd-Warshall algorithm. The input to the problem is a directed graph G and a set of weights on the edges. And these edge weights are allowed to be positive or negative. Now we'll start with the base case. What is the base case? The base case was the case D of zero S, T. So we want to iterate through all S and all T and fill in the entries D of 0, S, T. Recall our vertices are numbered one through N, so to iterate through all possible choices of S, we just have a for loop where S goes from one to N. Similarly, to go through all possible choices for T, we're going to have a for loop that varies T from one to N. Now to fill in the entry D of zero S, T, we got to check whether there is an edge from S to T. So if S, T is an edge, then this entry D of zero S, T is exactly the weight of this edge from S to T. In the other case where S to T is not an edge, then we're going to set D of zero S, T to be infinite. Now we go ahead and do the general case, where I is at least one. So we're going to vary from one to N. We're going to try to fill the entry D of I, S, T. So once again, we're going to go very overall, choices for S and all choices for T. So we're going have for loop going over the choices for S and a for loop going over the choices for T. Finally we can fill in the entry D of I, S, T. Now let's go ahead and write the recurrence for D of I, S, T. This is the recurrence that we just defined on the previous line. There are two cases and we're going to take the min or the best of the two cases. The two cases depend on whether vertex I is on the path or is not on the path. If it's not on the path then D of I, S, T is exactly D of I minus one, S, T. In the other case, when vertex I is on the path, we can break up that path into two parts. The first part goes from S to I and the second part goes from I to T and we take the sum of those two parts. Finally, what do we return? We return the case where I equals N. So we turn this matrix D of N, for all possible choices S and all possible choices T. I put these dots that signifies we're varying over all possible choices for that index. So we want all possible choices for S and all possible choices for T. If you think of this as a three dimensional array, then we're returning the slice corresponding to I equals N which isn't two dimensional array. It has N squared entries. This details the Floyd-Warshall algorithm. Now let's go ahead and analyze the writing time of the algorithm.

DP3 Shortest Paths - 87 - All Pairs Running Time Question
So, why don't you go ahead and analyze the running time for this algorithm.

DP3 Shortest Paths - 88 - All Pairs Running Time Solution
Now, the running time of this algorithm is fairly straightforward to analyze because there's just nested for loops. And then, within the for loops, it's just order one time. Order one time. For the base cases, we have two nested for loops, each of size order n. So, the total time to fill in the base cases is order n-squared time. For the general case, we have three nested for loops, each of size order n, so the total time is order n-cubed. Order n-cubed dominates, so total run time is order n-cubed.

DP3 Shortest Paths - 89 - Finding Neg. Wt. Cycle #2
Now, what happens in our algorithm if we have negative weight cycles in the graph? Well, the algorithm as written assumes there's no negative weight cycles. If there's no negative weight cycles in the graph, then this is correct. If there are negative weight cycles, then this is not necessarily correct, and we want to detect these negative weight cycles. If there is a negative weight cycle, how can we detect a negative weight cycle? How can we find out if the graph has a negative weight cycle and output, yes, there is a negative weight cycle? And if there is no negative weight cycles then we can run the algorithm as is. To get an idea for how to detect negative weight cycles, let's look at a simple example. So this is a cycle of length -1, 5 + -7 + 1, so it's -1 total length. And let's add in two more vertices to make it a non-trivial example. Let's add some connections from d to this cycle and from this cycle back to d. And also, from this cycle to e and from e to this cycle. And let's add in some weights to these edges. So, how can we find this negative weight cycle which goes from a b c a? Cycle of length three. What's going to happen in our algorithm? There's a more open-ended question. Once you think about how can we modify the algorithm in some simple way to detect a negative weight cycle? How can we find this cycle of length three in this graph? What is the final output of our algorithm? It's D of n, s, t for all pairs s, t. So let's look at this output on this example. Take somebody on this negative weight cycle, let's say vertex a. What is D of n, a, a for this example? What is the length of the shortest path from a to itself? It's allowed to use anybody as intermediate vertices. When this example is going to be of length -1. And notice also D of n, b, b is also going to be -1 and D of n, c, c is going to be -1. That is going to signify that there's a negative weight cycle. If we have any diagonal entries, so these are passed from vertex to itself. So any diagonal entries in the matrix which are negative then that means there's a path from a vertex to itself, which is shorter, which is negative. That means there's a negative weight cycle which includes that vertex. So to check for negative weight cycle we check if any diagonal entry, any entry D of n, y, y, for any y is less than zero. And that signifies that there is a negative weight cycle which includes this vertex y. So if we think of this three-dimensional table, and we look at this slice for i=n, then we have a two-dimensional table. We look at the diagonal entries and we check if there's any negative entries on that diagonal entry. Now notice we have two algorithms now for detecting negative weight cycles. We have the Floyd-Warshall algorithm which is all pair shortest path, and that's going to find any negative weight cycle in the graph by checking the diagonal entries. We also have the Bellman-Ford algorithm, which is the single source shortest path algorithm, and that also detects negative weight cycle. So there's some difference though, an important difference, that I want to distinguish between the two algorithms.

DP3 Shortest Paths - 90 - Comparing Algorithms
Let me modify this example slightly. I've taken the example from the last slide and I would just flip this one edge from d to b, it used to be, and now goes from b to d. And, I want to compare the two algorithms we have for detecting negative weight cycles. First algorithm is Bellman-Ford. The second algorithm we have is Floyd-Warshall. Floyd-Warshall does all pair shortest path. Bellman-Ford does single source shortest path. So, we give it a start vertex and it finds the shortest path from that started vertex to all other vertices. Now, in this example, suppose we run Bellman-Ford with the start vertex as d. It's not going to do much interesting because from d, you can't even reach any end of the graph. But, is it going to find this negative weight cycle, a, b, c? It's not going to find it because that cycle is not reachable from the start vertex. So Bellman-Ford only finds negative weight cycles reachable from the start vertex. It can't find this cycle because it's not reachable from the start vertex d, in this case. If we had run Bellman-Ford from a or b or c or even e, then we would have found this negative weight cycle. So Bellman-Ford finds negative weight cycles which are reachable from the start vertex, whereas Floyd-Warshall will find a negative weight cycle anywhere in the graph because it's doing all pairs, so it's finding everything in the graph.

DP3 Shortest Paths - 91 - DP3 Practice Problems
Here's a homework problem I often like to assign after this lecture. It's from Chapter Four, problem 21. It's about currency exchange and we're looking for an arbitrage situation. We're going to start say with one currency, say dollars. We're going to change it to another currency, let's say yen. And then we exchange it to another currency, let's say pounds. And then we go back to dollars. We're looking for a situation when we do this cycle and we end up with more than one dollar. So after this series of exchanges we end up with more than we started with. This is an arbitrage situation and we're looking for such anomalies in the exchange market. Now this is in Chapter Four which is about graph algorithms. And what we want to do, is we want to reduce it to the negative weight cycle problem. So we want to reduce this arbitrage situation to a negative weight cycle. Now we just saw two algorithms for detecting negative weight cycles. Now we want to use these algorithms as black boxes. So, we don't want to modify these algorithms in any way. We want to think of them as like a library, as a subroutine we got from a library and we can't modify the code. We can simply give it some graph as input and we can get the output, which is a negative weight cycle if one exists in that graph. Now we want to use this black box subroutine to construct an algorithm for this currency exchange problem. So we want to build an algorithm for detecting an arbitrage situation. So we're going to take as input our currency exchange rates and we want to figure out how to convert these currency exchange rates into a graph. So we need to build this function. This function is reducing this currency exchange problem into a graph problem of detecting a negative weight cycle. So we had to build this function which converts these currency exchanges into a graph and then we simply want to run this graph problem for detecting negative weight cycles. We're going to take the output from that and that's going to give us the output for our arbitrage problem. We might have to do some conversion on that output, but we don't have to touched this algorithm at all. It's called a reduction. Where we're reducing the arbitrage problem, to this negative weight cycle detection problem. And we denote it this way, we're reducing arbitrage to negative weight cycles. So we want to use this as a black box to solve this problem. Reductions are going to be an underlying theme in the course. We're going to use them to design efficient algorithms and later we're going to use them to prove hardness. We're going to use them to show NP completeness. But this is a general theme, we know how to solve one problem. We want to use that to solve a new problem. So we want to reduce this new problem to some existing problem that we know how to solve.

