Now what is this Stationery Distribution PI? Is there a nice formula for this PI? In general no but in some cases yes, there is. A simple case is when P is symmetric, what does this mean? That means the entry p_i_j is equal to the entry p_j_i. So if I look at row I and column J, the probability of going from I to J, is the same as the probability of going from J to I, if that's the case then PI is the uniform distribution. PI of I is one over N for all I. Now that's the simplest case, a generalization is known as reversibility. It's kind of a weighted version of symmetry, in which case PI is not necessarily uniform but we can still figure out the PI easily. But what reversibility requires is that, if there's an edge from I to J then there has to be edge from J to I. Now symmetry says that these probabilities are the same. We don't necessarily need them to be the same but we need that if there's an edge from I to J, then there's an edge from J to I and vice versa. Now this is the case, then we might be able to figure out what PI is, we might have a nice formula for what PI is. If this is not the case, so the chain is not reversible. So for instance there might be an edge from I to J but there is no edge from J to I. Then, in general we have no idea what the stationary distribution, there is no way to figure it out with a close formula. Now we can try to look at P to some high power and figure out what the stationary distribution is but we're not going to get some nice formulas such as this, for the stationary distribution. That completes our description of the introduction to Markov chains. Now we can dive into the details of the page rank algorithm.