DC1 Fast Integer Multiplication - 170 - Divide and Conquer
Divide and Conquer Algorithms or Recursive Algorithms is one of the first algorithmic tools that many of you may have learned. For example, binary search or merge sort. To see the power of divide and conquer, we'll look at a fundamental problem multiplying two n bit numbers. Here, we'll assume the numbers are huge. For example, they may be thousands of bits long. This will be the case in an application such as RSA. Since they are so large, we can no longer utilize the hardware implementation. We'll see a clever divide and conquer algorithm which is faster than the standard multiplication approach. Another fundamental problem we'll look at is given n numbers, we'd like to find the median element. The numbers are unsorted, so they are in arbitrary order. Can we find the median without first sorting the list? We'll look at a really ingenious divide and conquer algorithm to do just that. Finally, we'll dive into the beautiful FFT algorithm, Fast Fourier Transform. It's impossible to overstate the importance of this algorithm, it's used in many fields. For example, signal processing. In fact, it was called the most important numerical algorithm of our lifetime. At least that was the case at time of the quote in 1994. To understand FFT, we're going to have to recall some basics about complex numbers. Once you understand the basic mathematics behind it, you'll appreciate the beauty and simplicity of the algorithm. It will take some time and mental effort to get up to steam for the FFT algorithm, but it'll be worth it. It's a masterpiece.

DC1 Fast Integer Multiplication - 171 - D&C Overview
The next topic we'll dive into is the divide and conquer technique. We've seen a few examples of divide and conquer already. We saw two examples in the RSA algorithm. The first was the fast modular exponentiation algorithm which used the repeated squaring idea. The second was Euclid's GCD algorithm. These are two fairly simple applications of divide and conquer approach. We're going to see some more sophisticated examples in the following lecture. What we're going to look at now is multiplying n-bit integers. One of the applications of this problem is in the RSA algorithm setting, there we have huge integers. The number of bits is like a thousand or 2000, so the basic arithmetic operations like multiplication are no longer built into hardware. The problem formulation is we're given two n-bit integers, x and y, and we want to compute their product, Z, which is x times y. And we want to look at the running time as a function of the number of bits, that's the input size. What we discussed in the RSA algorithm lecture, was that the running time using a naive algorithm for computing the product of x times y would take n square time, order n square time. What we're going to do now is an algorithm which is faster than this order n square time. We're going to do a more sophisticated scheme. After this multiplication example, we're going to look at some other sophisticated examples, namely median. We're going to see how to compute the median in linear time, and finally we're also going to look at Fast Fourier Transform, FFT. These three examples, multiplying n-bit integers, the new time median and FFT are all very beautiful algorithms and I hope you'll learn to appreciate the essence of the algorithm. Now, one important note is: I'm assuming that you've seen divide and conquer before. For example, I'm assuming you've seen things like merge sort, the order and log n time algorithm for sorting n integers. And I'm also assuming that you know how to solve recurrences. If you need a refresher on some of these topics, I suggest you look through the textbook. Now let's go ahead and dive into this topic, multiplying n-bit integers...

DC1 Fast Integer Multiplication - 172 - Multiplying Complex #s
Before we dive into our example of multiplying n-bit integers. Let's take a brief digression. We're going to look at how clever idea from Gauss. It's not at all going to be apparent how this is useful for our multiplication for example, but it will turn out to be a very useful idea which will give us an improved algorithm. Here's a setting that we're working in, multiplication is expensive, so we want to minimize the number of multiplications. Adding and subtracting is relatively cheap, so we're not going to worry about the number of additions and subtractions. So we're willing to add in additional additions and subtractions in order to reduce the number of multiplications. Now, this fact that multiplications are expensive, and additions and subtractions are much cheaper is true in most cases. For example, when we're looking at multiplying n-bit integers, multiplying them takes order n squared time whereas if we're adding or subtracting it takes order n time. Now, here's the problem of Gauss. We're given two complex numbers. How do I give you the complex number? I have to tell you the real and the imaginary part. So the first number is a + bi, a is the real part, b is the imaginary part. And the second number is c + di, c is the real part, d is the imaginary part. And our goal is to compute their product. We want to compute (a + bi) times (c + di). Well, in order to get a handle on it, let's start by just expanding this out. So I get a times c, like bi times di, i squared is negative one, so that gives me minus bd. And then, I get two terms which are multiplied by i, the first is b times c and the second is a times d. In order to compute this product it looks like we need to compute a time c, b times d, b times c, and a times d. Now each of these a, b, c, and d are just real numbers. So it looks like we need four real number multiplications. We need to compute a times c, b times d, b times c, a times d. If you give me these real number multiplications, then with one, two, three additions or subtractions, we can combine these products in order to get the product of these two complex numbers. Now our setting is that multiplications are expensive. So we're trying to minimize the number of multiplications, and we're willing to have additional additions or subtractions in order to reduce the number of multiplications. So, can we reduce the number of multiplications? Can we get it down from four? Is it possible to achieve this product with only three multiplications? It turns out we can do it. The key is that we're going to compute this sum, b times c plus a times d, we'll compute the sum without computing the individual terms. So we're not going to compute b times c, and we're not going to compute a times d, but we're going to compute their sum bc + ad.

DC1 Fast Integer Multiplication - 173 - Improved Approach
So to recap, we have these two complex numbers: a+bi and c+di, and we want to compute their product. Now our goal is to minimize the number of real number multiplications in order to compute the product of these two complex numbers. We just saw that the naive approach has four real number multiplications: ac, bd and bc and ad. Now what we want to do is we want to try to achieve just three real number of multiplications in order to compute this product. What we're going to do now is we're going to try to compute this sum, bc + ad without computing the individual terms bc and ad. That's going to leave us with a method for computing the product of these two complex numbers with just three real number multiplications needed. How are we going to get this as a term, bc + ad? Well, notice it kind of looks like a cross terms. So let's write out the correct expression so that these are the cross terms. In order to achieve this, we need to have b on one side and c on the other side. And we need to have a on one side and d on the other side. So one side has b and a and the other side has c and d. Expand this out. Notice these two terms bc + ad is exactly what we're trying to compute. What about these other two terms ac and bd? Notice, these are exactly the products that we're computing in order to compute the product of these two complex numbers. Let's look back at this expression. We're trying to compute this sum, bc + ad. So let's solve it for this term. If we solve it for bc + ad, then we get that that's equal to (a + b)(c+d) - ac - bd. Here's the key now, in order to compute this product of these two complex numbers, what are the three now real number multiplications that we need? Well one of them is ac, just like before. The second one that we need is just like before is bd. The third term that we want to obtain is bc + ad, that's right here. How can we obtain it? Well let's use this expression. Look, ac we already know, bd we already know. What's the third thing we need? We need (a + b)(c + d). This is the third real number multiplication that we need. If we compute these three terms ac, bd and (a + b)(c + d) then from those we can compute these three terms, ac, bd, and bc + ad. So the punchline is, if we compute these three real number multiplications, ac, bd, and (a + b)(c + d) then by just doing some additions and some subtractions, we can obtain the product of these two complex numbers. It turns out we're going to utilize this idea in order to multiply the n-bit integers faster than order and square time. But before we dive back into that problem, I want to make sure that you appreciate the cleverness of this algorithm. It's really quite amazing. I mean look at what you're computing here. You're taking (a + b)(c + d). Now, one of the complex numbers that you're inputting is a + bi. So what are you doing? You're taking the real part and the imaginary part, and you're adding those up and that gives you a new real number, a + b. Why would you consider looking at this number, a + b or this number c + d? Why would you consider taking the real and imaginary part of the same number and adding those up? Somehow by looking at the real plus the imaginary part for both of these complex numbers and multiplying those together that gives us a way for computing the product of these two complex numbers faster or at least using fewer multiplications. Just think about it for a specific example. Let's say you're trying to compute (5 + 3i)(7 - 6i). What are you using here? Using 8 x 1. So why would you think to use 8 and 1 from these numbers in order to compute the product of these two complex numbers? It's really quite

DC1 Fast Integer Multiplication - 174 - D&C Naive Approach
Let's get back to our original problem multiplying n-bit integers. And let's look at the straightforward divide and conquer approach for this problem. The input to the problem are two integers x and y which are both n-bits long. And for simplicity, we're going to assume that n is a power of 2. This is a common assumption in divide and conquer algorithms. It allows us to get rid of floors and ceilings in our algorithm description and in the analysis of the running time. And our goal is to compute the product, x times y. We want to look at our running times in terms of n. N is the number of bits for these two integers x and y. It's the space required to represent these two numbers. Now what's the standard divide and conquer idea? Think about merge sort. What do you do? You break the input, the n numbers that you're trying to sort, you break them into two halves. The left half and the right half. Then you recursively solve the problem on the two halves. So for merge sort, that means sort the left half and sort the right half, and then you combine the answers together. You merge them together. Now, how do we apply that same idea here for multiplying these two n-bit integers? How do we break the input into two halves? Well, we can't break it into x separately from y. So what do we do instead? We break x into two halves. The left half of x and the right half of x and similarly for y. X is n-bit number, how do we break it into two halves? Well, we looked at the first n over 2 bits and the last n over 2 bits. And we take these first n over 2 bits, and we call that a new number, x_l. That's the left side of x. And we take the last n over 2 bits. And that's another number. x_r, corresponding to the right side of x. Similarly, for y, we do the same thing. Take the first n over 2 bits and the last n over 2 bits. So we're going to break x into the first n over 2 bits. That's going to be this new number x_l, and we're going to break it into the last n over 2 bits. That's going to be this number x_r. Similarly, for y, we're going to break it into the first n over 2 bits, call that y_l. And the last n over 2 bits, call that y_r. Let's look at this specific example to see what this partition of x into two halves is going to signify. Let's look at x equal to 182. In binary, this is 1011 0110. This is 8-bits long. So we're going to break it into the first 4 bits and the last 4 bits. This is the first 4 bits, that's going to be x_l. The last 4 bits are going to be x_r. So x_l equals 1011 in binary. In decimal, that corresponds to 11, x_r is 0110 in binary which is 6. How does 182 relate to 11 and 6? Well, notice 182 is the same as 11 times 2 to the 4, 16 plus 6. And in general, x satisfies. We take this number x_l, and we multiply it by 2 to the n over 2. That corresponds to shifting it, n over 2 times, and then we add in x_r. Which means add in these n over 2 bits.

DC1 Fast Integer Multiplication - 175 - Naive Recursive idea
Let's follow through on a recursive approach. So our basic idea was to break the input into two halves. So we partition this n-bit number x into two n over two-bit numbers. So xl is the first n over 2 bits of x and xr is the last n over two bits of x. Similarly, for y, we partitioned it to yl, the first n over two-bit and yr the last n over two-bit. Notice we just saw how xl and xr are related to x. In particular, x equals xl times 2 to the n over 2. That corresponds to shifting it n over two times plus xr. Similarly, for y, y equals yl times 2 to the n over 2 plus yr. Now our goal is to compute x times y. Well, we can replace x by this quantity. X is the same as 2 to the n over 2 times xl plus xr, and we can replace y by this quantity. 2 to the n over 2 times yl plus yr. Now let's expand out this expression and see what we get. Multiplying the first terms together we get 2 to the n times xl times yl, we get two terms which are scaled by 2 to the n over 2. Namely, we get 2 to the n over 2 times xl times yr, and we also get 2 to the n over 2 times yl times xr. Finally, the last term is xr times yr. If you notice now we have a recursive algorithm for computing x times y. Now recall x and y are both n-bit numbers, so we're trying to compute the product of these two n-bit numbers. What we can recursively do, is compute the product of 2 n over two bit numbers, where we have four n over two bit numbers over here. So we can compute the product of any pair of them. For example, we can recursively compute the product of xl and yl, and xl and yr, xr and yl and xr and yr. These are four products of n over two bit numbers which we can recursively compute. This gives us a natural recursive algorithm for computing the product of x times y. Let's go ahead and detail that algorithm to make sure everybody follows what we've done so far, and then we are going to try and improve on that.

DC1 Fast Integer Multiplication - 176 - Naive Pseudocode Question
Let's go ahead and detail this straightforward divide and conquer algorithm for multiplying x times y. The input to the problem are these two integers x and y, which are both at most n-bits long. And we assume for simplicity, that n is a power of two. So n equals two to the k for some non-negative integer k. And the output of our algorithm is this number z which is equal to the product of x times y. First thing we do, is we break x and y into two, n over two bit numbers. So XL is the number corresponding to the first n over two bits of x. XR is the number corresponding to last n over two bits of x. Since n is a power two, these always divide evenly. So we don't have to worry about putting any floors or ceilings on either of these two terms. Similarly, we partition y into the first n over two bits, call that YL, and the last n over two bits of Y, that's YR. Now what we can do is we can recursively compute the product of pairs of these n over two bit numbers. We have four n over two bit numbers. We're going to take particular pairs, recursively compute their product, and combine those together to get this answer z. The first pair of n over two bit numbers that we're going to use is XL and YL. We're going to recursively compute their product, and we're going to store that answer in capital A. The second one we're going to use is XR times YR, and we're going to store that answer in capital B. The last two recursive subproblems that we're going to do are XL times YR, store that in C, XR times YL, store that in D. Now we can get the product x times y using these for quantities A, B, C, and D. In particular, Z, which is x times y, that's equal to two to the n times A, that's a times that's not an X, plus two to the n over two times the quantity C plus D, that's these cross terms, XL times YR and XR times YL plus B. We got this expression on the last slide. Finally, Z is the quantity that we want to return. Z is equal to the product x times y. That completes the pseudocode for this easy divide and conquer algorithm. Let's take a look at the running time of this algorithm now.

DC1 Fast Integer Multiplication - 178 - Naive Running Time
Let's go ahead and analyze the running time of this algorithm. How long does it take us to partition x into the two halves and y into the two halves? These steps take us O(n) time, in order to break up x into the first n/2 bits and the last n/2 bits and similarly for y. How long does it take us to recursively compute these four products A, B, C and D? These are each pairs of n/2 bit numbers. So if we use T(n) to denote the running time and the worst case for inputs of size n, then each of these takes T(n/2), and there's four of them. So the total time for these four recursive subproblems is 4 times T(n/2). Finally, given the solutions to these four recursive subproblems, how long does it take us to compute z? Well we have three additions of O(n) bit numbers. How long does that take? That takes O(n) time. We also have to multiply this O(n) bit number times 2 to the n. How do we do that? Well it's much faster than a multiplication which takes O(n) square time. This is just a shift n times. In order to multiply by 2 to the n we just have to shift this number A, n times. Similarly, to multiply this number C + D times 2 to the n/2 we have to shift it and n/2 times. So it just takes O(n) time to do this multiplication by a power of 2. So to compute z, it takes us O(n) time. So the total time is O(n) + 4 times T(n/2). So let's look at T(n) denote the running time of this algorithm EasyMultiply on input of size n and this is for the worst input. So we take the worst input of size n and T(n) is the running time on that worst case input. We just looked at the running time of our pseudocode and we saw the T(n) satisfies the following relation. We have four subproblems of size n/2 each. And we take an additional O(n) time to combine these solutions together to get the product of x times y. So the running time for an input of size n, is at most 4 times T(n/2) + O(n). Now if you remember how to solve recurrences or this is a good time to brush up on it, this recurrence might look familiar. And what you might recall is that this recurrence solves to O(n) square. So, the easy divide and conquer algorithm that we just described takes O(n) square time. So the running time of this divide and conquer algorithm is the same as of running time of the straightforward multiplication approach. Can we improve this? Can we do better? The key thing is, can we improve this four down to three? Can we get away with just doing three subproblems? This is where we're going to utilize the Gauss's idea that we talked about.

DC1 Fast Integer Multiplication - 179 - D&C Improved Approach
This is the key expression from before. X times Y is equal to 2 to the n times x_l times y_l plus 2 to the n over 2 times the quantity x_l times y_ r plus x_r times y_l. Finally, we add in x_r times y_r. Now, a straightforward divide and conquer approach, computes four subproblems recursively. It multiplies x_l and y_l, x_l and y_r, x_r and y_l and finally, x_r and y_r. We have four subproblems, and then it takes us order n time to combine those solutions together, this leads to an order n square time algorithm. What we're trying to achieve now is we're trying to reduce the number of subproblems from four to three. Thinking back on the idea from Gauss that we talked about earlier for multiplying complex numbers, what do we want to do here? We want to compute this sum, x_l, y_r plus x_r, y_l without computing these individual terms. Well, how do we do that? Well, we think of these as cross terms again. And on one side we're going to have x_l and x_r, that's one quantity. The other quantity is going to be y_l and y_r. When we expand this out, we get the following: x_l, y_l, x_r, y_r and finally, we get x_l times y_r and x_r times y_l. Now, our goal is to compute the sum, so let's solve for that. This quantity x_l, y_r plus x_r, y_l, that's equal to this left hand side, x_l plus x_r times y_l plus y_r, and then we subtract off these two terms, x_l, y_l minus x_r, y_r. Now in the algorithm, we just had this term x_l, y_l, this was the quantity A which we recursively computed. x_r, y_r was B which we recursively computed. So we have this term x_l, y_l, and we have this term x_r, y_r. What's the final term that we want to compute? It's this quantity. So we're going to take x_l plus x_r, and we're going to multiply that by y_l plus y_r. And we're going to store that answer in C. We're going to recursively compute A, B, and C, and then we can combine them together using this expression to get X times Y. In particular, X times Y is equal to 2 to the n times A as before, plus 2 to the n over 2 times this quantity. How do we get this quantity? It's simply C minus A minus B. C minus A minus B. And then finally, we add in B, this last term. And this is going to give us a better divide and conquer algorithm because now we only have three subproblems that we have to recursively compute. And then we can combine them together using this expression in order to get the product of X times Y.

DC1 Fast Integer Multiplication - 180 - Improved Pseudocode Question
Let's go ahead and detail the pseudocode for this faster multiplication algorithm. Now this is the algorithm from before. This is the order n square time algorithm, which utilized four recursive subproblems. This new algorithm is fairly similar. It's just that it differs at the end. Just this last bit is different. So let's get rid of the last bit of the algorithm. Let's change the name of this algorithm from Easy Multiply to Fast Multiply. Basic set up of the algorithm stays the same. The input is two n-bit integers x and y, and the output is the product. We partition the input x into the first n over two bits and the last n over Two bits that's XL and XR. And we also do similarly for y. We partition it into YL and YR. Now we're going to recursively compute the product of three pairs of n over two bit numbers. The first pair, as before, is XL times YL. The second pair, as before, is XR times YR. We are going to store those in A and B, as before. Now the new term is we're going to compute XL plus XR and we're going to multiply that by YL plus YR, and we're going to store that in C. This is where we're utilizing Gauss's idea. Now we utilize the expression from the last slide. Z, the product of x times y is equal to two to the n times A which is XL times YL plus two to the n over two times the quantity C minus A minus B. This is where we are utilizing Gauss's idea. Finally, we add in B. Then we return Z, which is equal to the product of x and y. This completes our algorithm, and the key fact is that now we have three subproblems that we're recursively computing.

DC1 Fast Integer Multiplication - 182 - Improved Running Time
Now if we look at the running time of our new algorithm, we have three sub-problems that were recursively computing. Each is a product of pair of n/2 or two bit numbers. And then to combine these solutions from these three sub-problems it takes us order and time to get the product of X*Y. What does this solve to? Well, let's go ahead and dive into it to give you a bit of a refresher on solving recurrences. The first step is upper bounding this O(n), by Cn for sum constant C. Now we can substitute in this expression back and forth, T(n/2). So we get Cn + 3T. Now we substitute n for T(n/2), which is Cn/2 + 3 (T(n/2)². Collecting terms, we have, Cn, plus another Cn*3/2. Then we have a 3² x T(n/2)². Plugging that back in, we get Cn/2² from that term, + 3*T(n/2)³. Collecting terms we have Cn,*1, +3/2, +3/2². The next term is going to be 3/2³, and so on. We're going to get this geometric series. How many terms in this geometric series, what's the last exponent? We're going to keep going until this term is a constant. There are going to be log2n terms. Now we have this geometric series. We got to look at the series see whether the terms are equal, they're not. Is it decreasing geometric series, in which case the first term dominates? It's not. It's an increasing geometric series, because three houses bigger than one so the last term dominates. This whole thing is on the order of the last term. We get a O(n) for this term,*3/2 to log2n for this last term. Now this 2^logn is the same as n, so those cancel. We're left with 3^log2n. Let's convert that into a polynomial. You should remind yourself how to convert this into a polynomial. Let me give you a quick reminder. 3 is the same as 2^log(3). That's the definition of log. So I have this expression, 3^log2n. And I can replace 3 by this expression. And then I raise that whole quantity ^log2n. Now these two exponents multiply together, so I can swap them. This quantity is the same as, 2^log2^log2^3. Now these two things are the same. Now, this is much simpler. Because what is 2^log2n? This is simply n. Now, I have n raised to a power, which is a constant, log2^3. this recurring solves to, O(n)^ log2^3. What does this number log2^3? Well, if you plug it into a calculator, you see that log2^3 is roughly 1.59. So we went from an O(n)² algorithm, to an O(n)^1.5 9 algorithm. And in fact, we can improve this exponent. We can get arbitrarily close to 1, but there's an expense for that. This constant hidden in the Big O notation is going to grow as this exponent decreases. And instead of breaking the input up into two halves, we're going to break it up into more parts, and then we're going to have to work harder in order to combine the solutions together.

DC1 Fast Integer Multiplication - 183 - Improved Summary
That completes the description and the analysis of our algorithm. But before we move on, I want to look at a particular example to illustrate the cleverness of the approach. So, let's take this example. X equals 182 and Y equals 154. If I write 182 in binary, it's 10110110. And 154 in binary is 10011010. Our approach breaks up X into the two halves, XL and XR. And similarly, we have YL and YR. XL is 1011, which is 11 in decimal, and XR corresponds to 6, YL corresponds to 9 and YR corresponds to 10. Now what our algorithm does is it first computes XL times YL, which is 11 times nine which is 99. Then we compute XR times YR which is 6 times 10, which is 60. Finally, we get the non-trivial weird idea. We take XL plus XR, which is 11 plus 6, and we multiply that by 9 plus 10. That gives us 17 times 19, which equals 323. How from this number 182, which can be broken up into 11 and 6, somehow we're working with 17? We combine these two, then we get 182 times 154. That equals 99 times two to the n. In this case, n equals eight. So, two to the eight is 256. Then we take 323 minus 99 minus 60, and we multiply that by two to the four, which is 16. Finally, we add in the last term, 60. And if you plug that into your calculator, that exactly equals 28,028, which is exactly 182 times 154. And the amazing part is this 17 and this 19. How do we get those? That's a very non-intuitive part. That completes the description of this faster multiplication of n-bit integers. There are similar ideas for multiplying matrices, that's referred to as Strassen's algorithm. You can look in the textbook to learn about Strassen's algorithm. I'm going to skip it because algebra gets a bit messy for that. Next we're going to look at linear-time median algorithm. It's a very clever divide and conquer approach.

DC2 Linear Time Median - 184 - Median Problem
Let's look now at another nice example of divide and conquer. This is the problem of finding the median. The input to the problem are n numbers, and these are an arbitrary order. So we assume the list is unsorted and input is given to us as this one dimensional array, capital A. Our goal is to find the median of A, this is the middle element. When n is even, it's not exactly clear what we mean by the median. So for concreteness, let's define the median as the N over second, ceiling, smallest element of A. So when n is odd. So N equals two times L plus one for some integer L. Then in this case, the median is the L plus first smallest and there are exactly L which are, at most, this median element, and there is at least L which are at least this median element. It will be useful for us to solve a more general problem. Instead of finding the median element, we want to find the K smallest, where K is an input given to us. More specifically, we're going to look at the following problem. We're given an unsorted list A, just as before, and we're also given an integer K, where K is between one and n. Our goal is to find the K smallest of A. So if we said K equal to n over two, then that'll be the median. Now if A happens to be sorted, then it's easy to find the K smallest. We just output the Kth element of the sorted list. So that gives us a very trivial algorithm for solving this problem. Given an arbitrary A, we simply sort A and then we output the Kth element of this sorted list. How long does this algorithm take? Well, merge sort or several other algorithms take order n log n time to sort A. So the total runtime of this algorithm will be order n log n. Now is it possible to find the Kth smallest without first sorting A? That's what we're going to do now. And, in fact, we'll find the Kth smallest in order and time instead of order n log n time. What we're going to do now is this very clever divide and conquer algorithm for finding the Kth smallest in order n time. This algorithm is due to Blum, Floyd, Pratt, Rivest and Tarjan from 1973. The story is that they figured it out over lunch. Once you see the algorithm and it's quite clever, it will be quite impressive that they just figured this out just over one lunchtime.

DC2 Linear Time Median - 185 - QuickSort
Now we're going to use divide and conquer to solve this case smallest problem and our algorithm, the basic approach is going to be quite reminiscent of the quicksort algorithm. Let me remind you about the quicksort algorithm and then we'll see the modifications for our approach. So we're looking at quicksort for sorting this unsorted list A. The first step in the quicksort algorithm is to choose a pivot, p. Then we partition the array A into three buckets based on their relation to the pivot p. One bucket is for those elements strictly smaller than p, those equal to p, and those strictly bigger than p. We do one scan through the array A and we put each element into one of these three buckets. Then we recursively run quicksort on the small elements and the big elements. We take that output and our final output is the sorted list of the small elements, followed by the equal elements, followed by the big elements. Now recall the whole challenge in quicksort is how to choose a good pivot. If we chose a terrible pivot, such as the smallest element or the largest element, then one of these two lists is going to be of size n minus1. It's just going to go down by one element and then the running time of our algorithm is going to be order n square. So what's a good pivot for quicksort? It's the median, or something close to the median. For our problem we're going to have the same challenge. How to choose a good pivot? That's going to be the main task. But quicksort ideally runs in order n log n time. We're aiming for an order n time algorithm. The key is that we don't have to recursively consider both a less than p and a bigger than p. We only have to recursively search in one of these two lists. Let's look at a specific example to see exactly what I mean for this third step.

DC2 Linear Time Median - 186 - Search Example
Let's look at the following example with an array of n numbers and let's choose a specific pivot. Let's say the pivot is number 11. So we make these three buckets. Those less than p, those equal to p and those bigger than p. And we're going to take a scan through the array and we're going to put each of these elements in one of these three buckets. Five is smaller than the pivot, two is smaller than the pivot and so on. Eleven happens to appear twice, so it appears twice in this equal list. And these are the final buckets. Now, what is the K we're searching for? Depending on the K we're searching for, the Kth smallest will be in one of these three lists. And we can figure out where it resides based on how large these lists are in comparison to K. The smallest list is those of size four. So if K is, at most, four then we know the Kth smallest is in this list. So in this case, what we can do is we can recursively search for the Kth smallest in this list and we can discard these two other lists. Now if K is five or six, then what do we know? Then we know the Kth smallest is in this list and then, therefore, it's equal to 11. So we don't need to recurse at all, we can just output 11. Finally, if K is bigger than six, then what do we know? We know that the Kth smallest is in this list, so we don't search for the Kth smallest in this list because we discarded six elements from these lists. So we're going to search for the K minus six smallest in this list, A greater than p. Now the key is that we're always recursing on at most one list, either the small list or the big list. Or, in the middle case, we don't even have to recurse at all. Whereas QuickSort has to recursively sort these two lists, less than p and bigger than p. In this case, we only have to recursively search in one of the two lists. Now let's go ahead and detail this for the general case and let's write the pseudo code to make sure it's all clear.

DC2 Linear Time Median - 187 - QuickSelect
So we're given this unsorted list A and we've given an parameter K and we're trying to find the K smallest in A. Our first step, as in the quick sort algorithm, is to choose a pivot P. How exactly do we do that? Well that's exactly our going to be our main task. We'll get back to that later. After we choose a pivot P, we partition A into three sub-arrays based on this pivot, the smaller elements, equal elements and bigger elements. Now we're going to recursively search in one of these three list based on K and the size of this list. Now, if this small list is to say of size four, and K is at most four, then we know that the K smallest is in the small list. And in general, if K is at most the size of this list, then we can recursively search for the K smallest in the small list. So we run this algorithm recursively on this small list and search for the K smallest and that's the output of our algorithm. In the middle case, K is bigger than the size of the small list but it's not big enough that it resides in the big list. So therefore, we know that the K smallest is in the middle list and therefore the K smallest is exactly P. So we can just output P, no recursion needed in this case. Now in the final case, we know that the K smallest resides in the big list. So we're going to recursively search in the big list but we're not going to search for the K smallest, we're going to have to shift it, so we know that we're discarding these many elements, the size of these two lists. So we're going to look for the K minus the size of this list, minus the size of this list. So, instead of searching for the K smallest in this large list, we're going to look for the K minus the size of the small list, minus the size of the equal list. These are the elements that we're discarding. Now, this is the basic algorithm. But the key part is, how do we choose a pivot and what constitutes a good pivot? What does it mean to be good? Let's first look at what a good pivot means. What's a pivot which would lead to an order and time algorithm?

DC2 Linear Time Median - 190 - D&C High-level idea
Now we're aiming for an O(n) running time and we're trying to use divide and conquer. So let's look at the recurrences which yield an order N solution and then this will give us some idea of the algorithm basic approach. Take a look at this recurrence. What does this recurrence solve to? T(n) equals to T(n) over two plus O(n). This recurrence solves to O(n). How can we achieve a running time like this from the basic approach that we align on the previous slide? We have one sub problem which is of size and most N over 2. In our previous approach, we either recurse on A less than P or A bigger than P. In order to achieve that the sub problems of size and most N over 2, we need that the pivot P is the median. If the pivot is the median of the unsorted list, then we know that A less than P is in most N over 2 and a bigger n P is of size at most n over two and therefore the running time of our algorithm will be exactly this which solves to O(n). Now the problem is that the whole problem that we're trying to solve originally was to try to find the median of this list. So if you give us a solution then I can run for O(n) time and I can find the solution again. That's the punch line so far. But what if I don't actually have the median exactly? What if I have an approximate, a reasonable guess of the median? So suppose P is an approximate median, it's fairly close to the median but it's not exactly the median. Let's just do a thought process. Let me look at the sorted array A. So I'm not actually sorting A but I'm just thinking about it, this array A as being sorted, and the median little sign right here in the middle. And here I have the smallest element, here I have the largest element. Let me look at N over 4 smallest and let me look at the 3n over 4 smallest. Suppose instead of giving you the median, I just give you something which is guaranteed to lie within this band. So it's at least the N over 4 smallest and it's at most 3n over 4 smallest. So it's not lying in either of these extremes, it's lying in this middle band. Suppose I can find a pivot which satisfies this. So it's going to lie in this band. What does that imply about the running time of my algorithm. How large are these sub problems going to be in the worst case. When the worst case maybe it lies right here, it's a 3n over 4 smallest and then A less than P is going to be a size 3n over 4. Similarly, if it's N over 4 smallest then a bigger than P is going to be of size 3n over 4. And in general, I know that the size of the sub problem is going to be at most 3n over 4. So my recurrence is going to satisfy this relation. Now the question is, what does this recurrence solve to? Turns out, it's still solves to O(n) and in fact I can relax it even more. I don't even need three quarters here. I can look at the N over 100th smallest here and then .99n smallest over here. I just have to chop off a constant fraction on both sides. So what is my recurrence going to satisfy in this case where I lies in this bandwidth. So my pivot is at least the N over 100th smallest and is at most 99 over 100th smallest. In this case, the size of my subproblem is the most .99n. So my recurrence satisfies this relation. What does this solve to? This also solves the O(n). The key is, I need a constant here which is strictly less than one. So I'm always chopping off a constant fraction of the nodes. Now we're going to define a good pivot as a pivot which lies in this middle band between N over 4 and 3n over 4 smallest. That's going to give us this recurrence. But it's going to be important for our algorithm actually to remember this recurrence. We have some extra slack, so we're going to aim for a pivot which satisfies this relation but they're always going to be some extra slack because I'm allowed to have any constant less than one here and we have to utilize that extra slack in order to find the pivot which is a good pivot. So in the end, our running time is going to satisfy a relation similar to this, bottom recurrence. But our definition of what is a good pivot is lying in this middle band between N over 4 and 3n over 4.

DC2 Linear Time Median - 191 - Goal Good Pivot
We're going to say a particular pivot p is good if this pivot p is at least the n of the four smallest, and is at most three n over four smallest. What does that imply? That implies that the number of elements which are strictly less than this pivot is at most three n over four and the number of elements strictly bigger than this pivot is at most three n over four. So if this is satisfied, then we say the pivot p is good. Our main task is to figure out how to find this good pivot p in order n time. If we can do that, then we're going to get a recurrence such as this. We're going to have a subproblem of size at most three n over four, because of this relation. And it's going to take us order n time to partition the array and to find the good pivot. And then, we know that this recurrence solves to order n, so we'll be done. Question is, how can we find a good pivot in order n time?

DC2 Linear Time Median - 192 - Random Pivot
What's an easy scheme to find a good pivot? Well, if I have no idea what to do, what should I do? I might as well act randomly. So in our case, what does that mean? That means let P be a random element of A, choose a random element of A and said that to be the pivot P. Now, what's the probability that P is a good pivot? Let's look at our thought experiment from before. So let's look at the sorted array A. We're not actually sorting A. We're just looking at the sorted version of A for the purposes of analyzing the probability that P is good. We have the median element. We have the n over four smallest, and we have the 3n/4 smallest and over four smallest. What are our good pivots? Everybody in here is a good pivot. How many good pivots are there? There is exactly n over two good pivots. So, what's the probability a random element is a good pivot? I can order this however I won. The fact is, exactly n over two of these elements are good pivots and there's exactly n choices. So the probability a random element is a good pivot is the number of choices which lead to a good pivot divided by the total number of choices. The number of good pivots is n over two and the total number of choices is n. Simplifying this, we get one half, exactly half the elements are good pivots. So, no matter how you order this array A, we've got a probability exactly a half of finding a good pivot. Now, given a proposed element as a pivot, how can I check whether it's a good pivot or not. Well, I can just bend order and time and I can break partition A into those elements smaller, equal, or bigger than P. And if I keep track of their sizes as I go along then I can easily check in order and time whether this proposed pivot P is good or not. What happens if it's a bad pivot? What should I do in that case? Well, I can rerun this experiment again. So, I choose a new random element of A and then I check whether it's a good pivot or not. If it's still a bad pivot, then I run the experiment again and I keep going until I find a good pivot. Once I find a good pivot, I use it. In expectation, how many times am I going to have to repeat this experiment until I find a good pivot? This is like flipping a coin. If it ends up with tails, then I'll say that's a good pivot. If it ends up with heads, that's a bad pivot. I got probability exactly a half of finding a good pivot. So I got probably exactly a half of ending in tails. I keep flipping the coin until I get tails. Once I get one tails, that's a good pivot and I start my experiment. How many times am I going to have to flip the coin until I get A tails? In expectation, twice. So it's going to take me O(n) expected time to find a good pivot. So the expected runtime of the whole algorithm is going to be O(n). So this is a reasonable algorithm. But all I have guaranteed is that the expected runtime is O(n). I want an algorithm who's guaranteed worst case runtime is O(n). So, how can we guarantee to find a good pivot in O(n) time? That's the task we're going to try to tackle now.

DC2 Linear Time Median - 193 - D&C Recursive Pivot
Now our aim is to try to find a good pivot in order n time and we're going to do this in worst case, order n time. What we just saw is an algorithm which finds it in order n expected time. Now if we can successfully find a good pivot in order n time, then, the running time of our algorithm will satisfy the following relation. T(n) is in most T of three-fourths n because it is a good pivot. So, the sub-problem will be of size at most three-fourths the original size. Plus order n to find the good pivot and order n to partition a into the three sets. Now, this recurrence solves to order n. So, the overall running time of our algorithm will be order n. Now, we have a bit of slack in this recurrence. In particular, recall that instead of having 0.75 n over here, we could have had 0.99 n. We just need some constant less than one. What we're going to do is we are going to use this extra slack as extra time to help us find a good pivot. So, now, we are going to spend, instead of, just order n time to find a good pivot, we're going to spend order n time plus T of 0.2 n. So, we are going to design an algorithm with the following running time. T(n) is in most T of three-fourths n. This is for the sub-problem. Once we find a good pivot, plus T of n over five, plus order n. This T of n over five plus order n is going to be the time it's going to take us to find a good pivot. Now the key fact is that three-quarters, plus one-fifth equals 0.95. It's strictly less than one. So, this recurrence solves to order n. So if we can find a good pivot in this amount of time, T of n over 5 plus order n, then we successfully have an order n time algorithm to find the median. Now what exactly are we going to utilize this T of n over 5 in order to find a good pivot? How are we going to utilize it? Well, we're going to choose a subset of A which is of size n over 5 and then we're going to recursively run our median algorithm on this subset S. And we're going to set P, the pivot to be the median of this subset S. The time it's going to take us to find the median of this subset S is going to be T of n over 5 since the subset S of size n over 5. The question is how do we choose this subset S? We need to choose this subset S so that is a good representative sample of this entire array A. Let's take a look first at a naive choice of the set S and then we'll see how that fails and it will give us some intuition about choosing a better choice for this subset S.

DC2 Linear Time Median - 194 - Representative Sample
What's a simple idea for choosing this subset S, which is of size n over five? Well, perhaps the simplest idea is let the subset S just be the first n over five elements of A. Then we're going to set a pivot to be the median of this subset S. How does this pivot perform? Is it a good pivot? Is this pivot P a good pivot? No. Why not? Well, that would be too easy. Let's see why it fails. Let's see the scenario where this pivot P is not a good pivot. Well, suppose A was sorted. Well in this case, actually, it's easy to find the median of A. So, there's no reason we're running the whole algorithm. But suppose A is sorted and then, actually, this is going to imply that this pivot P is a bad pivot. So if A is sorted, then what is S? S is going to be the first n over five elements of A. So it's the smallest elements of A. So S contains all the n over five smallest elements of A. What is the median of these n over five smallest elements of A? The median of the n over five smallest elements is the n over 10th smallest element. So this pivot is the n over 10th smallest element of this list A. Well if this is the n over 10th smallest element. Then when we partition A into the small, equal and big sets, the big set all we can guarantee is that it's at most nine-tenths n. And then if you go back to the running time of our algorithm, then we're going to have t of n is at most t of nine-tenths n instead of three-fourths n. So this pivot P is not a good pivot, because this large set is going to be too large in the worst case. So is there a better choice for this set S? Well, there's some hope because we chose this set S right now without looking at A at all. So, can we look at A and do a little bit of computation and choose a better representative set S?

DC2 Linear Time Median - 195 - Recursive Rep. Sample
Our goal is to choose a subset S which is a representative of the set A. What exactly do we mean by representative of the set A? We want that the median of this subset S is also a reasonable approximation to the median of the set A. Now the median of A has half the elements smaller than it and half the elements bigger than it. Now in order for the media of S to approximate this median, what we want is that each element of S is somehow has a similar flavor. So we want that each element of S has the following property. We want that there a few elements of A that we can guarantee are at most X and a few elements of A are at least X so that these elements of A, which are represented by this element X. What exactly do we mean by few? Well, let's say it's two. We have at least two elements of A, which are at most X and at least two elements, which are at least X and then if we combine, add in X itself, then we've got five elements here that we're considering. We want to look at sets of five elements and we want X to represent that set of five elements. So what we're going to do is we're going to break A into N over five groups of five elements each. Let's assume N is a power of five, so that this can be done cleanly. Now how are we going to get this subset S from this partition of A? Well, we're going to choose one representative from each group. Each group is going to choose or we're going to choose one representative one element of this group, which is going to represent the group in the following sense. It's going to represent the group in the sense that at least two of the elements of this group are at most the chosen element, and at least two of the elements are at least the chosen element. Let's take a look at a particular group to see how we do this. Let's look at a group G consisting of the elements X1, X2, X3, X4, X5. Now who do we want to choose from this group to represent this group in the sense that we talked about just before? Let's look at this group sorted. Let's sort the group and relabel it so that X1 is the most, X2 is a most, X3 and so on. Now who are we're going to choose to represent it in this sense? Well, we're going to choose the middle element. The median of this group. X3 has the desired property. At least three elements of G are at most X3, including X3 itself, and at least three elements including X3 again are at least X3. If we take the median of each of these groups, then that gives N over five elements and each of those chosen elements in the subset S, it represents a distinct group of five elements so that each have this desired property and this subset S is going to be a good representative sample of this entire array A. And that's it. Now we have the whole idea for finding a good pivot. We take this array A, we break it into N over five groups of five elements each. F. Or each group, what we do is we sort it. How long does it take us to sort? Well, notice this group is of five elements. So sorting it takes order one time. It doesn't matter what algorithm we use. If we take an exponential time algorithm, exponential N five is still order one. So it takes us order one time to sort this group and then we can take the median element of this group and that's going to be the representative sample for this group and we take the one median to one representative sample from each group. That gives us N over five samples and that gives us our subset S and then we take the median that subset S, recursively we find it and that gives us the pivot that we use for A. And then we're going to prove that that pivot that we find, which is the median of this subset S, is a good pivot. Let's go ahead and detail the pseudo code for this algorithm and then we'll go back and look at the claims that we're making about this pivot, being a good pivot, and look at the running time of our algorithm.

DC2 Linear Time Median - 196 - Median Pseudocode
Now, let's detail the pseudo code for a linear time median algorithm. The input to the algorithm is unsorted array A, of size n, and an integer k. Where the integer k lies between one and little n. And the output of the algorithm is the k-th smallest element of the array A. The first thing we need to do is find a good pivot. To do that we break A into n over five groups of five elements each. Now to be precise, we should say the ceiling of n over five groups, because little n might not be a multiple of five. But let's ignore floors and ceilings in this pseudo code. Let's denote these n over five groups as G_1, G_2 up to G_n over five. Now, how exactly do we break a into these n over five groups. Well, we can do it in any arbitrary way we like. The easiest way to do it, is to take the first five elements of A and put those into group one, take the next five elements of A and put those in group G_2 and so on. Now we have chosen one representative from each of these groups. Here's a for loop to go over the n over five groups. For the ith group, group G_i, we want to find the median. To do that we first sort this group. There's only five elements so we can take any algorithm we like to sort it. And then we take the median of these five elements. And let's let mi denote the median of group G_i. Now we want to look at these n over five medians which we found in step two. Let's let capital S denote this set of n over five medians. Next, we want to find the median of this set S. This will be our pivot P. How do we find this pivot P? How do we find the median of this set? Well, we recursively call this same algorithm Fast select, on this subset S. Now S has n over five elements, we want to find its median, therefore we look for K equal to n over 10. The n over 10 smallest element of S, is the median of this set S. And we store that in element P. Now we use P as our pivot. We partition the original set A into three sets. Those elements less than pivot P, those equal to the pivot P, and those bigger than the pivot P. This requires just one scan over the set A. Now we can use the quick select approach from before. Based on the sizes of these three sets, we either recursively search in small set, the big set, or we simply output P. In particular, if the size of the small set is at least as large as k, then we know that the k-th smallest lies in this small set. So, we recursively run this same fast select algorithm on the small set, looking for the k-th smallest. Now if k is big enough, in particular, if k is bigger than the size of a small set, plus the equal set, then we know the k-th smallest lies in this big set. So, we recursively run fast select on the big set with this K scale. So, we set k equal to k minus the size of a small set and the size of the equal set the part that we discarded. Finally, if neither of these two cases held, then we know that the k-th smallest lies in the equal set, and therefore we simply output P. Now these three cases are simply the same as we detailed before for the quick select algorithm. This completes the pseudo code for our algorithm. Now we can analyze this running time assuming that this pivot P that we found is a good pivot. And then we'll go back and prove that this pivot P is in fact guaranteed to be a good pivot.

DC2 Linear Time Median - 197 - Median Running Time
We're going to prove that this P that we chose is in fact a good pivot. Now let's assume this fact for now and then let's look at the running time of this algorithm. Let's look at the running time step-by-step. The first step, breaking A into these n over five groups. How long does that take? Well, that just takes one swipe through the array A. So, that takes order and time. Now, we have to sort each group. How long does it take to sort one of these groups? Well, it's only five elements. So, even if we use a super slow algorithm, let's say five factorial, we write down every permutation of these five elements and choose the one which is sorted, that's going to take us five factorial time which is still order one. So, it takes us order one time per group and there's order and groups. So, the total time for this step is order N. Now, we're going to run this same algorithm recursively on this subset S, which is of size N over five. So how long does that take? That takes us T of N over five since this subset of size N over five. Now we partition A into these three subsets, that takes one swipe through the array A, that also takes order and time. And then finally, we recurse on one of these smaller sub-problems. How large are these sub-problems? Well, since P is guaranteed to be good pivot, we know that these sub-problems are size, at most, three-fourths N. So, our running time satisfies the following recurrence. We have three-fourths N, we have N over five for finding this median of this subset S, and then plus we have order N for several of these steps. The key is that three-fourths plus one-fifth is 0.95, which is strictly less than one. So, this recurrence solves to order N and we have an order N time algorithm. Now it remains to prove this claim, that P is a good pivot. Once we prove that P is a good pivot, we're all done with our algorithm.

DC2 Linear Time Median - 198 - Correctness
We want to prove this claim that this P that we chose in our algorithm is a good pivot. How do we find this P? Well, we took these groups G1 through Gn/5, each with five elements each. And we took the median of each of these groups and then we took the median of those medians. So we took this n/5 medians of these groups, and then we recursively ran our algorithm to find the median of that subset of size n/5 and that was our P. Now what I want to do is I want to sort these groups and I want to sort them by their medians. So this has a smallest median, this has the largest median. I'm just relabelling the labels on the groups. Now this is just in the proof. The algorithm is staying the same just for the purposes of the proof as do the thought experiment where we relabel these groups. So we've relabeled these groups so that the first group has a smallest median and the last group has the largest median. Let's look at this set A pictorially, and let's look at it by groups. Here's the first group that consists of five elements. Here's the second group G2. It consists of five elements and so on. And here's the last group Gn/5, which consists of five elements. Now, it'll be important for us to consider the middle group, the group Gn/10. Now, let's sort each of these groups by smallest to largest. This will be the smallest element of this group and this will be the largest, and this will be the median. This will be the smallest, largest, median, and so on. In every group, the middle element is the median. This is just for the picture purposes. Now what does our algorithm do? It takes the median of each of these groups. Who is the median? It's the middle element right now. Our subset S is going to consist of these medians, these n/5 medians from each of the groups and our pivot is the median of this subset S of medians. It's the median of median is our pivot P. Who is the pivot P in this picture? It's the middle element, which is exactly the median of this middle group. This is P. Now, I want to prove that P is a good pivot so I want to first look at which elements are guaranteed to be at most P. Who do I know is the most P? Well, look at this subset S, P is the median of this subset so I know that everybody earlier in this subset, these guys up to and including P itself, are guaranteed to be at most P. So that's n/10 elements that are guaranteed to be at most P. Now, take this first element. Who's guaranteed to be at most it in its group, G1? Well, I assume this thing was sorted for the picture purposes. There's two elements, which are guaranteed to be smaller than that. Similarly, in the second group there's two other elements which are guaranteed to be at most this median of this group. In every group, there's two elements, which are guaranteed to be at most this median element. So what's my conclusion? My conclusion is that all of these elements in this box are guaranteed to be at most the pivot P because their medians are at most this pivot P and these elements are at most this median. What's the area of this box? How many elements are in here? Well, there's n/10 groups and in each group I got three elements. So there's at least three n/10 elements which are guaranteed to be at most P. Now look at the partition of A into these subsets: smaller, equal, and bigger than P, and look at the subset consisting of those elements strictly bigger than the pivot P. So this excludes all of those elements which are at most P. So how large is a subset? Well, I've excluded at least three n/10. So therefore, the size of this is at most 7n/10. Recall what was my definition of a good pivot? I needed to guarantee that this set is at most 3/4n, and I've shown that it's strictly smaller than most 0.7n. Now, let's look at the other side. Which elements are guaranteed to be at least P? Well, now I'm going to look at the other corner. All of these elements are guaranteed to be at least P. How many elements are here? Well, the same count 3n/10 are guaranteed to be here. So, the number of elements that are least P is at least 3n/10 and therefore, the number of elements which are strictly less than P is most 7n/10, similarly. That proves that P is a good pivot that completes the proof of the claim.

DC2 Linear Time Median - 199 - HW Groups of 3 7
Now a natural question is, why do we break A into these groups of five elements each? Why did we choose five? What would have happened if we would have broken into n over three groups of size three elements each, or n over seven groups of seven elements each? This is nice homework question to make sure you understand the analysis of this algorithm. So consider groups of size three each and seven each, and look at the analysis of the algorithm. What is the recurrence in these cases where we break into groups of three or seven? Write down the recurrence and see whether that recurrence solves to order n or not, and that'll show you why we consider groups of size five.

DC3 Solving Recurrences - 200 - Solving Recurrences
This is a short refresher lecture on Solving Recurrences. We're going to focus on the type of recurrences that arise in divide and conquer algorithms. A classical recurrence is that for merge sort T of N equals two times T of N over two plus Order N. To sort N numbers, we partition the list into the first N over two numbers and the last N over two numbers. We recursively sort those two sub lists, that takes time, two times T of N over two and then it takes as Order N Time to merge these two sorted list together. As you probably know, this recurrence solves to order logN. Let's look at some other recurrences that we've seen so far in divide and conquer examples. The naive algorithm for integer multiplication had the following recurrence, T of N equals four times T of N over two plus order N. And we claim that this solves the Order N Square. We'll see you why momentarily. Our improved algorithm for integer multiplication had three sub problems instead of four sub problems. So we had the following occurrence, T of N is at most three times T of N over two plus Order N we claim that this solves to Order N to the exponent log base two of three. And recall this exponent is roughly one point six. Now another recurrence which we saw in the median finding algorithm was T of N equals T of three fourths N and plus Order N. And we claim that this recurrence solves to order N. Now let's go ahead and solve these recurrences. We'll start with this recurrence. The naive algorithm for integer multiplication then we'll look at this slightly more complicated example. And finally we'll look at the general solution for these recurrences of this form.

DC3 Solving Recurrences - 201 - Example 1
We're looking first at this recurrence, T(n) = 4T (n/2) +O (n). Let's start by getting rid of the big O notation. Order n means that there's some constant c so that t of n is almost 4 times T of n over 2 plus cn. So we replaced order n by most cn. We also have the base case, let's say the base cases of size 1 since the inputs of size 1 it must solve in order one time. So we can place that order 1 by a constant c and notice we can use the same constant c for both of these by taking the max of these two constants and calling that c. Let's solve for T of n. So I simply restated the recurrence, though for convenience I rearranged the terms so T of n is the most c times n plus 4T of n and over 2. Now let's replace T of n over 2 by this recurrence. So replace n by n over 2 so we get this term c times n again plus we get 4 times and then let's replace T of n over 2 with this expression. So we have 4 times T of and we're placing n by n over 2 so we get n over 2 squared and then for the last term we're replacing n by n over 2 so instead of c times n, we get c times n over 2. Now let's rearrange this and it collects similar terms. So we want to collect c times n with 4 times c times n over 2. So these have a common factor of c times n, this term is 1, this term is 4 over 2 times c times n and then finally, the last term is 4 squared times T of n over two squared. Now let's repeat this process and replace T of n over 4 by this expression. Now we're replacing n by n over 2 squared. So the first term becomes 4T of n over 2 cubed and the last term becomes c times n over two squared. Rearranging once again so bringing this term over here. So we have T of n is the most c times n times the quantity 1 plus 4 halves plus 4 halves squared plus 4 cubed times T of n and over 2 cubed. Notice we're starting to get a geometric series here.

DC3 Solving Recurrences - 202 - Expanding out
What we've seen so far is that for this recurrence T of N equals four T of N over two plus order N by substituting in two times, we get to T of N is the most C times N times the quantity. One plus two, plus two squared, plus four cubed times T of N over two cubed. Now notice we're leaving, this is four over two and four over two squared, instead of two and two squared so that we see the geometric series that's going to arise. And this illustrates how the geometric series that arises, is related to the original recurrence. The four comes from here and the two comes from this term. At this point, we see the pattern. So now I suppose instead of substituting in two times we did I times. Well, to be precise, let's suppose we're substituting, I minus one times. So, this geometric series becomes one plus four halves, plus four halves squared, and so on, up to four halves to the I minus one. And this last term becomes four to the I times T of N over two to the I. Now, when do we stop? What I do we stop at? We start when we get to the base case which is when this quantity is one and whether, we stop at one or a hundred, it doesn't matter. So, let I equal the log base two of N. Then, two to the I is N. So, N over N is one. So, this quantity is one. So, substituting in I equal log base two of N, we get the following expression. Now, let's try to simplify the terms here. First off, what is T of one? T of one is the most C. So, we can upper bound this by C. What is four to the log base two N? Well, this is order N squared, C times N is order N. Now, what about this geometric series? Now, the key thing is that this geometric series is increasing. Four over two is bigger than one it's two. So, the last term dominates. So, this whole expression is dominated by order the last term. So, this geometric series is on the order of four over two to the log base two N. We can forget about the minus one because that's hidden in the Big O notation because that's just an extra factor of a half. Now, let's simplify this quantity. The numerator is four to the log base 2N, which we said before is order N squared. The denominator is two to the log base two N which is of course N. So, this quantity is order N. Plugging it in, we have order N, times order N, plus order N squared. So, this order N squared, plus order N squared which is order N squared. So this recurrence solves the order N squared. The key element of these recurrences, is this geometric series that arises. So, let's take a look at geometric series in general.

DC3 Solving Recurrences - 203 - Geometric Series
Consider some positive constant alpha. In our previous example alpha was 4 over 2 and we're looking at this geometric series. We're summing over J going from zero to k of alpha to the j. Expanding this out we have 1 plus alpha plus alpha squared and so on up to alpha to the j. Now since we are solving our recurrences within big O notation, we don't need to solve this expression exactly. We simply need to solve it within constant factors, so that makes life much easier. Now the key for a geometric series is to figure out which term dominates either the first term, the last term or all terms are equal. The three cases are whether alpha is bigger than 1, equal to 1 or less than 1. Now if alpha is bigger than 1, then each term is growing, so the last term is going to dominate. So the whole expression is going to be on the order of the last term. Conversely, if alpha is less than 1 then the terms are decreasing and the first term dominates, so the whole expression is on the order of the first term, so it's order 1. Finally, the middle case is if alpha equals one then every term is exactly 1 and then the number of terms is k plus 1, so the whole thing is on the order of k. This first case where alpha is bigger than 1 is what arises in the recurrence we just analyzed. In that case alpha equals 2 and what we saw there was that the last term dominates. When we look at merge sort, then we'll have alpha equals one, all the terms will be the same and we'll get an order again from that. The final case is what arises in the example of the median finding algorithm, there we'll have alpha equals three quarters so the geometric series which arises will be on the order of 1.

DC3 Solving Recurrences - 204 - Manipulating Polynomials
The final tool that we'll need is manipulating polynomials. What we saw in the previous example is that 4 raised to the power log base 2 of n is the same as n squared. Now that example is quite straightforward but we're going to have other examples which arise such as three raised to the power log base 2n. And we want to convert this into a polynomial. So we want to convert this into n to the c for sum constant c and we want to figure out what is this constant c. So let's see the basic recipe for converting this into a polynomial. The key is that the base of this log is 2. So I want to change this base from 3 to 2. Well, 3 is the same as 2 raised to the power log base 2 of 3. That's by the definition of log. So let's substitute this in, to this expression. So we have 3 raised to the power log base 2n and then we replace 3 by this expression. So we have 2 raised to the power log base 2 of 3 raised to the power log base 2 of n. Notice these exponents multiply. So this is the same as 2 raised to the power log base 2 of 3 times log base 2 of n. And we can rewrite this as 2 raised to power log base 2 of n and raised to the power log base 2 of 3. What we've done is just swapped these exponents. Now look at this inner quantity. That's just, n, so we have n raised to the power log base 2 of 3. So we see that this exponent c is log base 2 of 3.

DC3 Solving Recurrences - 205 - Example 2
Now let's take a look at the following recurrence, T(n) = 3T(n/2) + O(n). This is the recurrence which arised in the more sophisticated integer multiplication algorithm. Just as before, rewriting this to eliminate the big O notation, we have T(n) is at most cn for this term plus 3T(n/2) for that term. Now substituting in as we did before and repeating it i - 1 times we get the following recurrence, T(n) is at most cn(1 + (3/2) + (3/2)^2 + and so on up to (3/2)^i-1) + 3^iT(n/2^i). Notice that before we had a geometric series with base (4/2) and now we have a geometric series with base (3/2). And instead of 4^i we get 3^i, this is because we replaced four with three here. Now once again we're going to stop when we get to the base cases which is when this is one. So we're going to stop when i = log2 of n. Why is it the base two? Because we have two to the eye here. So if we would have had subproblems of size n/3 then we would stop when we have log3 n. Now plugging this in we get the following expression, so we have T(n) is at most cn(1 + (3/2) so on up to (3/2)^log2 n-1) + 3^log2 n T(1). T(1) is at most c, cn is O(n). What about this geometric series? Well the base is (3/2). So it's an increasing geometric series, the last term dominates. We can forget about the minus one, that's just an extra factor of two thirds. So this is 0((3/2)^log2 n), the numerator is 3^log2 n, the denominator is 2^log2 n. 2^log2 n is n so that cancels with this n, this O(n) cancels with this one over order n. So we get O(1) (O(3^log2n) and this term is also O(3^log2 n). That's not a coincidence. This should be the last term of this geometric series, so they should be on the same order of magnitude. Finally we want to manipulate this into a polynomial and we just saw that this is on the O(n^log2 3).

DC3 Solving Recurrences - 206 - General Recurrence
Now, let's look at the more general form of recurrences. So for positive constants a and b greater than one. Let's look at the recurrence T(n) = a T(n/b) + O(n). Our previous two examples had b=2, and we did a case where a=4, and a=3. Now, the recurrence for the median finding algorithm has a = 1 and has b = 4/3. And merge sort has a = 2 and b = 2. Now, when you expand this out you get c*n from this O(n), times the following quantity, 1+ (a/b) + (a /b²). And so on up to a/b. And then, it's going to stop when we get the logb(n). In the last term is going to be a, ^logb(n)/T(1 ), which is O(1). Now, the key is what happens for this geometric series? It depends on whether a/b > 1, = 1, or < 1. If a is bigger than b, then we get an increasing geometric series and the last term dominates. If a = b, then we get a geometric series where I term is one. If a < b, then we get a decreasing geometric series and the first term dominates. Let's do the easy case first, a < b, so the first term dominates. So this geometric series is on the O(1). So we get O(1) for this geometric series, get O(n) here, so the whole thing is going to be O(n), so we get O(n), in this case. What happens when a = b? Then each of these terms is one. So we get log n terms. So the geometric series is on the order of log n, so we have O(n) times O(logn). So that's going to be O(n) logn. So in the case, a = b, we get O(n), log n that's for merge sort. Finally, what happens in the case when a > b? Now, the last term dominates. Notice the denominator here is b ^logb(n) which is n, so this term, cancels this term. So the whole thing is on the order of a ^logb(n). Converting that into a polynomial, we get that this recurrence in the case a > b is on the order of n, ^logb(a). And of course, one can look at more general forms of recurrences where here, we have instead of O(n), we have O(n) ^d, where d is a constant. This is commonly referred to as the master theorem and you can look at your textbook for the form of the master theorem and the solution to the master theorem.

DC4 FFT Part 1 - 207 - Polynomial Multiplication
What we've seen so far is how to use divide and conquer in a clever way to multiply large integers. So for N bit integers, we were able to multiply, compute their product in time better than order n square. A similar idea applies to matrices as well. What we're going to do now is multiply polynomials. And to do this, we're going to use the beautiful divide and conquer algorithm known as FFT. FFT stands for fast fourier transform. So here's the set up. We have a pair of polynomials A of X and B of X. For the polynomial A of X, will denote its coefficients as A knot, A1, A2 up to An minus one. So it's of degree at most n minus one. And for B of X, will denote its coefficients as B knot, B1 up to Bn minus one. And it also is of degree of most n minus one. Our goal is to compute the product polynomial, C of X which is A of X times B of X. And the coefficient of C of X will denote as C knot, C1 up to C2 n minus two. Since the degree of C of X is at most two n minus two. Now recall that the Kth coefficient of the polynomial C of X, so this is denoted by c sub k. This is obtained by taking the constant term for A of X, this is A knot times the coefficient for the Kth term of B of X, this is BK. So I look at A knot times BK and I add that to the other possibilities A1 X Bk minus 1 and so on, up to a Nk times B knot. We want to solve the following computational problem. We're given the vector of coefficients defining A of X and we're given the vector of coefficients defining B of X and we want to compute the vector of coefficients for the product polynomial C of X. Now this vector C is known as the convolution of A and B. So this star symbol denotes the convolution.

DC4 FFT Part 1 - 208 - Quiz PM Example Question
Let's take a quick quiz to make sure you're familiar with multiplying polynomials and let's consider the polynomial A(x), which is 1 + 2x + 3x squared. And B(x), let's define as 2 - x + 4x squared. So the vector of coefficients for A(x) is (1, 2, 3), and the vector for B(x) is (2, -1, 4). And now compute the convolution of A and B.

DC4 FFT Part 1 - 209 - Quiz PM Example Solution
The solution to this problem is 2,3,8,5,12. To get the x cubed coefficient 5, we obtain it in the following way, multiplying 3x square by negative x, we get negative 3x cubed. Multiplying 2x by 4x square, we get 8x cubed. Adding these up, we get 5x cubed.

DC4 FFT Part 1 - 210 - PM General Problem
Once again, the general computational problem that we're considering is, given this vector of coefficients for A of X, and this vector of coefficients defining B of X, we want to compute their convolution C. Now a simple algorithm for computing this convolution is going to compute each of these coefficients in turn. Now there were order n coefficient. How long did it take to compute each coefficient? Well a naive algorithm takes order K time to compute the Kth coefficient because there are order K terms that we have to sum up. This is going to lead to an order n squared time algorithm to compute all of the order n coefficients of the polynomial C of X. What we are going to do now in this lecture is an order n log n time algorithm for computing this product polynomial C of X, the convolution of a_n.

DC4 FFT Part 1 - 211 - Convolution Applications
Before we dive into the algorithm, let us take a look at a few of the many applications of convolution. One important application is filtering. So here we have a data set of endpoints. What we are going to do is we are going to replace each data point by a function of their neighboring points. This is used for such things as reducing noise or adding effects. Let's take a look at a more detailed example of filtering so it becomes clear. In mean filtering we have a parameter capital M and we replace the data point Yj by the mean of the neighboring 2M+1 points and we do this for all j. Now this smooth dataset Yi had can be viewed as the convolution of Y with a vector f. The vector f is this vector of size 2M+1. To smooth the data set in this way with computers convolution with this simple vector f. Now of course we can smoothen more sophisticated ways for example, we can replace this factor f by a Gaussian function, in this way points nearby Yj are given more weight. In particular a Gaussian filter uses the following vector. This quantities Z, is just a normalizing factor so that the sum of the entries in this vector sum up to one. Now there are many other filters one might consider. A different type of filter one might consider is Gaussian blur. This is used to add some visual effect to an image, in particular to blur an image. In particular, Gaussian blur applies a two dimensional Gaussian filter to an image. Now let's get back to our original problem of how do we compute the convolution of a pair of vectors.

DC4 FFT Part 1 - 212 - Polynomials Basics
And let's look at some basic properties of polynomials. Consider the polynomial A(x). There are two natural ways of representing the polynomial A(x). The first way, is by its coefficients, this is the vector A that we've been considering so far. A second way, is by looking at the value of this polynomial A(x) at n points. So, we take n points (x1), (x2), up to (xn) and and we evaluate this polynomial A(x) at these n points. The key fact is that a polynomial of degree n minus one is uniquely determined by its values at any n distinct points. So, that this statement makes intuitive sense, the example to keep in mind is that of a line. A line is a degree one polynomial, and a line is defined by any two points on that line. And in general, a degree n-1 polynomial is defined by any n points on that polynomial. The vector of coefficients is a more natural way to represent a polynomial. However, the values are more convenient for the purposes of multiplying polynomials. We'll see this in a second. Now, what FFT does, is it convert from coefficients to the values and values to coefficients. So, it does this transformation between these two representations of the polynomial. And the point is that coefficients is a more convenient way to represent a polynomial oftentimes but the values are more convenient for multiplying polynomials. So, we'll take the coefficients as input, we'll convert them to the values. We'll multiply the polynomials and then we'll use FFT to convert it back to the coefficients once again. One important point is that FFT converts from the coefficients to the values, not for any choice of (X1) through (Xn) but for a particular well chosen set of points, (X1) through (Xn). And part of the beauty of the FFT algorithm comes from this choice of these points (X1) through (Xn). Before we dive into FFT, let's take a look at why the values are convenient for multiplying polynomials.

DC4 FFT Part 1 - 213 - MP Values
One of the key ideas for multiplying polynomials is that multiplying polynomials is easy when we have the polynomials in the values representation. So suppose that we know the polynomial A(x) evaluated at two n points, x1 through x2n, and we know this polynomial B(x) at the same two n points. Then we can compute the product polynomial C(x) at these two n points by just computing the product of two numbers for each i. So C(xi) is the product of the number A(xi) and B(xi). This is just a number, and this is just a number. Since C(xi) is just the product of two numbers, it takes order one time for each i, and therefore it takes order n total time to compute this product polynomial. Now why do we take A and B at two n points? Well, C is a polynomial of degree at most 2n minus two. So we needed at least 2n minus one points. So two n points suffices. The summary is that, if we have the value of these polynomials, A(x) and B(x), at n, or two n points, then we compute this product polynomial at the same points in order n total time. So what we're going to do is we're going to use FFT to convert from the coefficients to the values, this is going to take order n log n time to do this conversion, and then I'll take order n total time to compute the product polynomial at these two n points, and then we do FFT to convert back from the value of C of x, at these two n points, back to the coefficients for C(x), and that again will take order n log n time. So let's dive in now to see how we do FFT, which converts from the coefficients to the values.

DC4 FFT Part 1 - 214 - FFT Opposites
Now here's the smaller computational task that we're focusing on now. We're given this vector a. These correspond to the coefficients for the polynomial A(x) and we want to compute the value of this polynomial A(x) at two endpoints, x1, through x2n. And the key point is that we get to choose these two endpoints x1 through x2n. How do we choose them? Well that's our main task. A crucial observation is suppose that the first n points are opposite of the next endpoints. In other words Xn+1 is -X1. Xn+2 two is -X2, and so on. The last one is X2n is -Xn. Let's suppose that our two endpoints satisfy this property, which we'll call the plus minus property, and let's see how this plus minus property

DC4 FFT Part 1 - 215 - FFT Splitting A(x)
Once again the plus minus property says that the first n points are opposite of the next n points. And let's see how this plus minus property is useful for recursively computing a of x at two n points. Let's look at a evaluated at the point xi and a evaluated at the point xn plus i. Since we're assuming the plus minus property, these two points are opposites of each other. Now let's break up this polynomial A(x) into the even terms and the odd terms. Even terms are of the form a, two k times x raised to the power two k. Since the powers even, this is the same for xi and negative xi. So these even terms are the same for the point xi and the point xn plus i. Now what about for the odd terms. Well since it's an odd power it's going to be opposite for xi and xn plus i. Now given this observation about the even terms and the odd terms, it makes sense to split up A(x) into the even terms and the odd terms. So let's partition the coefficients for the polynomial A(x) into those coefficients for the even terms. Let's call this vector a even and into the coefficients for the odd terms. Let's call this factor ai.

DC4 FFT Part 1 - 216 - FFT Even & Odd
So given this vector of coefficients for this polynomial A of X, to find A even as the coefficients for the even terms and A odd as the coefficients for the odd terms. Then we can look at the polynomials defined by this vector of coefficients. This vector A even defines this polynomial A even of Y. I have used a variable y to avoid confusion with A of X. Notice that A 2 is the quadratic term in A of X, but in A even, it's the linear term since it's the second coefficient. And similarly A 4 is going to be the quadratic term. Similarly this vector A odd defines this polynomial A odd of y. Notice that the degree of these two polynomials is N over two minus one. So we took a polynomial A of X of degree N minus one and we defined a pair of polynomials of degree N over two minus one. How does a polynomial A of X relate to the polynomial A even and the polynomial A odd? Well notice if I take the polynomial A even and I evaluated at the Y equals X squared, then I get the even terms of the polynomial A of X. Similarly if I evaluate the polynomial A odd at the point Y equals X squared. Then I get the odd terms except it's off by one in the exponent. So I multiply by X. So A of X is equal to A even evaluated at the point X squared plus X times A odd evaluated at the point X square. At this point you just start to see the semblance of the idea of the divide and conquer approach. We started with a polynomial of degree N minus one. And now we have defined a pair of polynomials of degree N over 2 minus one. So we went down from N to N over two and we got two subproblems, A even and A odd. And if we want A at a point X, then it suffices to know A even and A odd at the point X squared. So we've got two subproblems of half the size. Now the degree of this polynomial A of X went down from N minus one to N over two minus one for these two smaller polynomials. However if we need a of X at two endpoints, we still need A even and A odd at two endpoints the square of these original two endpoints. So the degree of these polynomials went down by a factor of two. But the number of points we need to evaluate them at hasn't gone down by a factor of two yet. This is where we're going to use the plus minus.

DC4 FFT Part 1 - 217 - FFT Recursion
Given this polynomial A of X, we would define this pair of polynomials, A even and A odd. They satisfy the following identity, A evaluated at the point x is equal to A even evaluated at the point x squared plus x times A odd evaluated at the point x squared. Now let's suppose that the two endpoints that we want to evaluate A of X at satisfy the plus minus property. So the first end points are the opposite of the next end point. So let's look at A evaluated at the point XI and A evaluated the point X N plus I. Because of the plus minus property, this is A evaluated at XI and A evaluated at negative X I. Plugging this into our earlier formula. We have that A evaluated at the point XI is equal to A even evaluated at the point XI squared plus XI times A odd evaluated at the point XI squared. And similarly for A evaluated to point negative XI, we have it's equal to A even again at the point XI squared minus XI times again, A odd at the point XI squared. So notice to get A at these two different points, we need A even and A odd at the same points. Our conclusion is that if we're given A even and A odd at these endpoints Y1 through YN which are the square of these two endpoints. Notice that since these two endpoints satisfy this plus minus property, the square of these two endpoints are these endpoints. Then in order N time, we get A evaluated at these two endpoints in particular to evaluate A at the point XI, it takes order one time given the value of A even at this point and A odd at this point. And similarly to evaluate A at the point XN plus I it takes order one time given the value of A even at this point and A odd at this point.

DC4 FFT Part 1 - 218 - FFT Summary
Now, let's summarize our approach. We have this polynomial, A of x of degree at most n minus one, and we want to evaluate this polynomial at two n points x_1 through X_2n. And we get to choose these two n points however we want. And we're looking at how we choose these points. One very minor point that I wanted identify now is why we consider this polynomial at 2n points instead of n points. And in fact, later we'll go back and we'll look at it at n point, instead of 2n points. But for now, we want this polynomial A of x at 2n points. Why? Because of our application to polynomial multiplication. Recall, our first step in our construction is to define this pair of polynormial Aeven, and Aodd. We do this by taking the even turns of A of x to define Aeven, and the odd terms define Aodd. Whereas the original polynomial A of X was of degree n most n minus one. Each of these polynomials is of degree at most n over to minus one, so the degree went down by half. Next, we recursively run the FFT algorithm on this pair of polynomials, and we'll evaluate these pair of polynomials at n points. What are the n points? The n points that we evaluate this pair of polynomials at are these points Y_1 through Y_n, which are the squares of these two n points. Since the original two n points satisfy the plus minus property, then x_1 is the opposite of X_n plus 1. So these squares are the same. That's our first point Y_1. X_2 is the opposite of X_n plus 2. So their squares are the same and that's Y_2, and so on up to Y_n, which is X_n square and x_2n square. Why do we want this pair of polynomials at the square of these two n points? We'll recall, to evaluate this polynomial A at this point X. It's straight forward if we know Aeven and Aodd at the point X square. So if we know this pair of polynomials at the square of these points then it's straight forward to get A at these two n points. In particular, in order one time per point, we can evaluate this polynomial A of x at that point, using Aeven and Aodd at the square of that point. So, in order n total time, we can get this polynomial A of x at these two n points, using Aeven and Aodd at these n points, which are the squares of these two n points. This is the high level idea of our divide and conquer algorithm for FFT. What's the running time of this algorithm? Well let T of n denote the running time of input of size n, we have two sub problems of exactly half the size. So, two t of n over two, takes us order and time to form these two polynomials, and it takes us order n time to merge their solution together, to get the solution to the original problem. So we get the following recurrence, T of n is most two T of an over two plus order n. This is the same recurrence in the classic merge sort and many of you must recall that it resolves to order n login. So it looks like we have an order, nlogn time algorithm to solve this problem. All we need is that these two n points satisfy the plus minus property. So the first n are opposite of the second n. But notice we're going to recursively run this problem on this pair polynomials Aeven and Aodd with the square of these points. So again, we're going to need the plus minus property for this sub problem, and for all smaller subproblems. Looking for this sub problem, we have Y_1 through Y_n. We need the first n over two to be the opposite of the second n over two. It will be straightforward to define two n points which satisfy the plus minus property. The challenge will be to define two n points which satisfy the plus minus property, and for all recursive sub problems to also satisfy the plus minus property. So let's dive into how we achieve.

DC4 FFT Part 1 - 219 - FFT Recursive Problem
In order to get two endpoints which satisfy the plus minus property, we are going to choose points so that the second N are the opposite of the first endpoints. What happens in the next level of the recursion? Then the points we're considering are the square of these two endpoints. So these are the endpoints which are X one square up to XN square and we want these endpoints to also satisfy the plus minus property. Let's assume that N is a power of two. Then we need that Y one is the opposite of YN over two plus one and up to YN over two is the opposite of YN. What does this mean in terms of X? This means that X one squared is the opposite of XN over two plus one square. But the square of a number is always positive, so this is a positive number, and this is a positive number. So how can they be opposites of each other? Well it's impossible if we're working with real numbers. The only way to achieve this is by looking at complex numbers, so we can easily achieve the plus minus property at the top level of the recursion. But for all further levels of the recursion, in order to achieve the plus minus property we need to look at complex numbers. So let's do a quick review of complex numbers and then we'll see the appropriate choice of these two N numbers.

DC4 FFT Part 1 - 220 - Review Complex Numbers
We saw that we need to consider complex numbers for our choice of the n or two n points where we evaluate the polynomial A(x). Here, I'll give a brief review of the relevant concepts regarding complex numbers. Some of you may get a bit scared at the use of complex numbers but the mathematics involved is fairly simple, so don't get intimidated at all. And the final algorithm that we get is very simple and very beautiful. We have a complex number Z which is a plus bi. A Is the real part and B is the imaginary part. It's often convenient to look at complex numbers in the complex plane. In the complex plane, one of the axis corresponds to the real component and one of the axis corresponds to the imaginary component. So, for this number A plus bi, we look at A in the real axis, B in the imaginary axis and we look at this point ab. This number Z corresponds to the point ab in the complex plane. Now, there's another way to describe this point, this number Z and it's called the polar coordinates. In polar coordinates, we look at the length of this vector from the origin to this point Z. Let's call R, the length and we look at this angle theta. This number Z in polar coordinates is R comma theta. Just as we had two representations of a polynomial, either the coefficients or the values. We also have two representations of a complex number. Either, with the cartesian coordinates ab or the polar coordinates R theta. Now it turns out the polar coordinates are more convenient for certain operations such as multiplication. Before we delve into properties of polar coordinates, let's review some basic properties. First off, how do we convert between polar coordinates and complex or cartesian coordinates? Well, if you remember a little bit of trigonometry, you'll recall that A, this length A is equal to R times cos theta and this length B is equal to R times sin theta. So, this gives us a way to convert between cartesian coordinates and polar coordinates. Now there's a third way of representing complex numbers. This is given to us by Euler's formula. This gives us a quite compact representation of complex numbers. The basic fact is Euler's formula, which says that cosine theta plus I times sin theta equals exponential of i times theta. Then we can multiply both sides by R and then we have that this number Z, this complex number Z equals R times exponential of i times theta. The basic idea of the proof of this Euler's formula is by looking at the Taylor expansion of exponential function cosine and sine function.

DC4 FFT Part 1 - 221 - Multiplying in Polar
As we mentioned earlier polar coordinates are convenient for multiplication. Let's consider an example. Consider Z1 and Z2, a pair of complex numbers and let's look at their product. Let's say that Z1 in polar coordinates is r1 theta 1 and Z2 in polar coordinates is r2 theta 2. Now let's look at the product of these polar coordinates. We simply multiply the lengths and we add up the angles. Now suppose that I have a point Z. I have a complex number Z and I want to look at its opposite. So I want to look at negative Z so this corresponds to multiplying Z times negative 1. What is negative 1 in polar coordinates? Here's the point negative 1 in the complex plane. It's a distance one from the origin and it has angle 180 degrees or pi, so its polar coordinates are 1 comma pi. And let's say Z is this point at polar coordinates r comma theta, then negative Z corresponds to the product of r comma theta times negative 1 in polar coordinates which is 1 comma pi. Then using this above rule we have that negative Z is at the polar coordinates r comma theta plus pi. So the point negative C is the reflection of the point Z. It's just the same distance from the origin. It's a distance r from the origin, and instead of going in angle theta we go in angle theta plus pi. So if you view the complex numbers in the complex plane, it's easy to find their opposites by looking at their reflection.

DC4 FFT Part 1 - 224 - Roots Graphical View
Now we're trying to find those Z. Where Z raised to the power n equals 1. And let's draw the number 1 on our complex plane. What is its point in polar coordinates? Its distance 1 obviously, from the origin. What's the angle? The angle is zero. It's also 2pi or 4pi or 6pi or 2pi times j for any integer j. And note j can also be negative number. So the angle is any multiple of 2pi. Now let's say Z equals the point r theta in polar. We're looking at those Z where Z raised to the power n equals 1. So it's equals 1 comma 2pi j. Recall the expression for multiplying two complex numbers in polar coordinates. So look at Z raised to the power n. So Z is r comma theta. So Z to the n is r raised to the power n. And then we add up the angles. So we get n times theta. Now we're assuming this equals 1. So what does r equal and what does theta equal? Well, r raised to the power n equals 1. So then we know that r equals 1. So what does that mean? That means that all the complex roots of unity lie on this unit circle, this circle of radius 1. Where do they line the circle? Well, we have to look at the angle. n times theta equals 2pi j. Solving for theta. We have 2pi j over n. Let's take the case n equals 8 and see what this looks like on the unit circle. The case j equals zero, corresponds to angle zero. So this is the point 1. That's good because we know that the point 1, that number 1 is an nth root of unity. Let's look at j equals 1. So we got 2pi over n. What does this correspond to, this angle 2pi over n? It's like you took the whole pi, the whole circle, and you subdivided it into equal slices. For the case n equals 8, this angle is pi over 4. So this is the point 1 comma pi over 4. j equals 2 corresponds to this point which is i. Which is 1 comma pi over 2. Next point is 1 3 pi over 4. And we get 1 pi which is negative 1. And we get 1 5 pi over 4, and so on. Notice that when we get to j equals 8, we repeat. We get back to this point 1. j equals 9, j equals 10, and so on. So notice there are n distinct values. And what we're doing is we're starting at the point 1 and we're taking steps sizes of 2pi over n.

DC4 FFT Part 1 - 225 - Roots Notation
And what we just saw are that the nth roots of unity correspond to the points in polar coordinates 1 comma 2pi over n times j. j equals 0 corresponds to this point which is 1. Then we take step sizes of angle theta, where theta equals 2pi over n, this gives us j equals 1 comma j equals 2 and so on and we keep taking the equal size steps around the unit circle. Let's introduce a little bit of notation to make it more convenient for expressing these nth roots of unity. So let's let omega sub n denote the point corresponding to j was 1. So it's this 1 right here so this is omega sub n. Now what is the next point correspond to? Well, we just doubled the angle so it's just a square of this point omega sub n. And the next one is going to be the cube of this point. And the jth point is going to be the jth power. So the last one is going to be omega sub n to the n minus 1th power and then we have omega sub n to the nth power, which is the same as omega sub n to the zero power which is the number 1. So the nth roots of unity are the following n numbers, it's omega sub n to the zero power, to the 1th Power, to the square, to the n minus 1 power. These n numbers in polar coordinates are 1 comma 2pi over n times j, where j varies from 0 to n minus 1. And recall Euler's formula, so this number omega sub n equals e raised to the power 2pi i over n so the jth of the nth roots of unity, omega sub n raised to the jth power, where j varies between 0 and n minus 1, is e raised to the power 2pi i times j over n.

DC4 FFT Part 1 - 226 - Roots Examples
That we saw that for the case n equals 2, the square roots of unity are the points 1 and minus one. For the case n equals 4, we also have the points i and minus i so we have the four points 1_i minus 1 minus i. This corresponds to going around the unit circle in step sizes of space Pi over 2. And for n equals 8, we further subdivide the unit circle. Now we go step sizes of Pi over 4 and then how do we get the points for n equal 16? We further subdivide. So this corresponds to omega_sub_16. The next one is omega_sub_16 squared and notice that it's also omega_sub_8. It's the first one of the eighth roots of unity. And this point i is omega_sub_16 to the fourth power, it's also omega_sub_8 squared, it's the second one of the 8th roots. And it's the first of the 4th roots. So take the nth roots and square them. And let's suppose that n is a power of 2 as in these examples. So what happens when you take the 16th root square? Well this guy goes there, this one goes there, this one goes there, this one goes there. What do you get? You get every other one of the 16th roots, which are the 8th roots so the 16th root squared are the 8th roots. And when n is a power of 2, the nth root squared are the n over 2nd second roots. The other key property is this plus minus property. Take this point omega_sub_16. What is the opposite of it? It's this point right here. This is negative omega_sub_16. It's also a 16th root of unity. Which one is it? Well, this is the 1st first, this is the 8th, this is the 9th. This is the point omega_sub_16 to the 9th power. And in general, for even n, omega_sub_n to the jth power is opposite omega_sub_n to the j plus n over two. By going an extra n over 2, I'm going an extra Pi around so I'm getting the reflection

DC4 FFT Part 1 - 229 - Key Property Opposites
There are two key properties of the nth roots of unity which we're going to utilize. The first holes for even n, the nth root satisfies the plus minus property. This was the key property for our divide and conquer approach. The first n over two of the nth roots of unity are opposite of the last n over two. In other words, omega sub n to the zero power equals negative omega sub n to the n over two. And in general, omega sub n to the jth power equals negative omega sub n to the n over two plus j. So the nth roots of unity look like a perfect choice for our divide and conquer approach, because they satisfy the plus minus property.

DC4 FFT Part 1 - 230 - Key Property Squares
The second key property holds when n is a power of two. If we look at the nth roots squared, we get the n over second roots. So we take the jth of the nth roots, so omega sub n to the jth power, and we square it. So in polar, we're taking the point one comma two pi over n times J squared. So we simply double the angle. So we get the point one comma two pi over n over two times J, which is omega sub n over two to the Jth power. So the Jth of the nth roots squared is the Jth of the n over second roots. And similarly, if we take the n over second plus Jth of the nth roots and we square it, well, this is just the opposite of this. So when we square the negative of it we get the same thing. So why do we need this property that the nth root squared or the n over second roots? Well, we're going to take this polynomial A of x, and we want to evaluate at the nth roots. Now these nth roots satisfy the plus minus properties, so we can do a divide and conquer approach. But then, what do we need? We need to evaluate A even and A odd at the square of these nth roots, which will be the n over second roots. So this subproblem is of the exact same form as the original problem. In order to evaluate A of x at the nth roots, we need to evaluate these two subproblems, A even and A odd at the n over second roots. And then we can recursively continue this algorithm. Now we're all set to do our FFT algorithm, and the end points where we choose to evaluate the polynomial A of x are the nth roots of unity.

DC5 FFT Part 2 - 231 - FFT High-Level
>> We now have all the pieces to define the FFT algorithm. Let's start with the high level idea of the algorithm once again. We're given a polynomial of X. We're given this polynomial by its coefficients. Let's assume this polynomial is of a degree, at most N minus 1. Where N is power of K. We want to evaluate this polynomial N points. Now in the end, when we do polynomial multiplication, we actually want this polynomial A of X at two N points. In order to obtain at two N points instead of N points, we can just pad the polynomial, the coefficients with zeros, so that we view the polynomial as a degree two N minus one polynomial. Now what are the N points that we're going to choose? We're going to choose the Nth roots of unity as our end points, which we're going to evaluate the polynomial of A X at. Now since N is a power of two, so N equals two to the K for some positive integer k. Then we know that these N points the, Nth roots of unity, satisfy the plus minus property. So the first N over two are opposite of the last N over two. And the other property is, that the square of the Nth roots are the N over second roots. Now we're going to take this polynomial A of X, and we're going to define a pair of polynomials, A even, and A odd. We take the even coefficients, and that defines this polynomial A even. We take the odd coefficients of A of X, and that defines this polynomial A odd. And the degree of these two polynomials is at most, N over 2 minus 1. So we went down from a polynomial of N minus one degree, to two polynomials of degree at most N over 2 minus 1. Now what we saw earlier is that in order to attain A of X at these N points, we need to evaluate A even and A odd at the square of these points. So what we do is we recursively evaluate A even and A odd, at the square of Nth roots. What's one of the key properties of the Nth roots of unity? It's that the square of the Nth roots equals the N over second roots. And there are N over two such roots. So in order to obtain A of X at N points, we need to evaluate these two polynomials, A even and A odd, of half the degree, at N over two points. So we've got two subproblems of exactly half the size, and these subproblems are of the same form. We want A of X at the Nth roots, A even and A odd at the N over second roots. Finally, given A even and A odd at these N over second roots, it takes order and time to get A of X at the Nth roots. We simply use this formula from before. A of X equals A even at X square, plus X times A odd at X squared. So it takes order one time, to compute A of X for each X, in the order N axis. So it takes order N total total time to compute A of X at the Nth roots of unity. Finally, what will be the running time of this algorithm? Well, for the original problem of size N, we define two subproblems of size N over two. We recursively solve those to get the polynomials A even and A odd at the N over 2nd roots. And then it takes us order N time to merge the answers, to get A of X at the Nth roots. This is the common recurrence, that you've seen many times probably for merge short and stuff like that. And this solves to order NlogN. And this is the sketch of the algorithm to take a polynomial in the coefficients form, and give return, the valuation of the polynomial at N points, where the N points are the Nth roots of unity. And it does so in time order NlogN.

DC5 FFT Part 2 - 232 - FFT Pseudocode
Now we can detail the pseudocode for the FFT algorithm. Now, the first input is vector A, which are the coefficients for this polynomial A(x), and we're assuming that n is a power of two. What is this second input? This Omega? Omega is an nth root of unity. For now, just think of Omega as Omega_sub_n. In polar coordinates, this is 1, 2 pi over n. And using Euler's formula, this is e to the two pi i over n. For now, you can view Omega as Omega_sub_n. But later, we're going to use this exact same algorithm. This is identical pseudocode with a different Omega. We're going to use Omega as Omega_sub_n to the n - first power. And that's going to allow us to do inverse FFT. In inverse FFT, we're going from the value of the polynomial at n points to the coefficients. Now, what's the output of the FFT algorithm? Well it's the value of the polynomial at the nth roots of unity. If Omega is Omega_sub_n, what are the nth roots of unity? Well it's just the powers of this. So, the output is a evaluated at Omega to the zero, a evaluated at Omega, a of Omega squared, and so on, up to a at Omega to the n - first power. When Omega equals Omega_sub_n, this gives a evaluated at the nth roots of unity.

DC5 FFT Part 2 - 233 - FFT Core
Let's dive into the FFT algorithm. It's a divide and conquer algorithm. So let's start with the base case. The base cases is when N equals one. What are the roots of unity in this case? Well, it's just one. So we can simply return A evaluated at the point one. Now, we have to partition this vector A into A even and AI. These correspond to the polynomials A even and AI. So let A even, the vector A even be even terms in the vector A. So A naught, A2, A4 up to AN minus two, and A odd are the odd terms. So A1, A3 up to AN minus one. The input vector A was a vector of size N. These two vectors A even and AI that we just defined are vectors of size N over two. Now, we have our two recursive steps. We call FFT the same algorithm with the vector A even, and instead of Omega, we use Omega square, and we also call FFT with A odd and also Omega square. What do we get back from this call? What we get back is A even at the square of these endpoints, which are these N over two points. Omega naught, Omega square, up to Omega to the N minus two. So if Omega is the Nth root of unity, then we get A even at the N over second roots of unity. And similarly, we get A odd at the N of a second roots of unity. Notice that if Omega equals Omega sub N, then the Jth of these points square is the Jth point in this sequence or actually the J plus first. This is Omega sub N over two to the Jth power. So this is the Jth or J plus first of the N over second roots. Now using these values for A even and A odd we can get A at the Nth root to unity. Now we use our formula for A of X in terms of A even and AI. So A evaluated at the point Omega to the Jth power equals A even, evaluated at this point square, plus Omega to the J times A odd at this point square. And similarly, if we look at the point Omega to N over two plus J. This is the opposite of Omega J. So using the same formula, this requires A even and A odd at exactly the same points. The only difference is we subtract these two terms instead of adding them together. This takes order one time for each J. So it takes order N total time. Finally, we have A evaluated at these endpoints and that's our output that we returned from the algorithm. Now, notice this algorithm works for any Omega which is an Nth root of unity. We only require that Omega to the Jth power is opposite Omega to the N over two plus J. That's true for any root of unity except when Omega is one, because then this would be one and this would also be one. So they're not opposites of each other, but for any other root of unity, the Jth power is opposite the N over second plus to the Jth power.

DC5 FFT Part 2 - 234 - FFT Concise
Part of the appeal of FFT is that the algorithm is quite concise. The algorithm is very simple. So let's re-express FFT in a more compact manner. First off we have the base case, n equals one. This is a polynomial of degree zero. So in this case we simply return the constant term a naught. Once again we define a even, the vector, as the even terms in the vector a, and a odd as the odd terms in the vector a. Then we recursively run the FFT algorithm with the vector a even and omega square. The output we get back we record as s naught through sn over two minus one. Similarly we will run FFT with a odd and omega square and we record the output in t naught through t n over two minus one, then we combine the solutions for these subproblems to get the solution to the original problem. So rj, which is going to be a of x at the point omega to the j equals s sub j, which is a even at the point omegas to the two J plus omega to the J times tj. And similarly r sub n over two plus j two equals s sub j minus omega to the j times tj. Finally we return these n numbers r naught through rn minus one.

DC5 FFT Part 2 - 235 - FFT Running Time
Now looking into running time of this algorithm, notice this step partitioning the vector a into a even and a odd that takes order and time. This is a recursive call which is of size n over 2. Similarly for a odd it's a recursive call of size n over 2. This computation of the Rs takes order one time for each pair. So takes order n total time. Therefore this running time satisfies the recurrence T event is 2T event over 2 plus order n. And of course this solves the order n log n that completes the FFTL.

DC5 FFT Part 2 - 236 - Poly Mult. using FFT
Now that we've completed the FFT algorithm, let's go back and look at our original motivation which was polynomial multiplication or equivalently, computing the convolution of a pair of vectors. The input is a pair of vectors A and B of length n, corresponding to the coefficients for a pair of polynomials A of X and B of X. The output is the vector C, which are the coefficients for the polynomial C of X, which is A of X times B of X. Equivalently, C is a convolution of A and B. In order to multiply these polynomials A of X and B of X, we want to convert from the coefficients A and B to the values of these polynomials A of X and B of X. How many points do we need these polynomials at? Well C is of degree two n minus two. So we want these polynomials are actually C of X at at least two and minus one points. In order to maintain that n is a power of 2, we'll evaluate A of X and B of X at two endpoints. In order to do that, we'll run FFT. We will consider A of X and B of X as polynomials of degree 2n minus one. So we'll just pad this vector with zeros. So we run FFT with this vector A and the two and three of unity. And this is going to give us a polynomial A of X at the two nth root unity. Similarly we went FFT with this vector B and the two nth root of unity and we get the polynomial B of X at the two nth roots of unity. So now we have these polynomials A of X and B of X at the same two endpoints. Now given A of X and B of X at the two nth roots of unity, we can compute C of X at the 2 nth roots of unity. We have a for loop J, which goes over 0 to 2n minus one. So goes over all these two endpoints and we multiply these pair of numbers. Even though these are complex numbers, it takes us order one time to compute the product of these pair of numbers. So it takes us order one time to compute C of X at the Jth of the two nth roots of unity. So it takes us order one time to compute T of J and therefore takes this order n time to compute C of X at the two nth roots of unity. Now we have C of X at the two nth roots of unity. Now we have to go back from the value of this polynomial at these two endpoints and figure out the coefficients. This is opposite of what we were doing before. Before we were going from the coefficients to the values. Now we want to go from the values back to the coefficients. How are we going to do this? What we're going to run an inverse FFT. And amazingly enough, the inverse FFT is almost the same as the original FFT algorithm.

DC5 FFT Part 2 - 237 - Linear Algebra View
Before we explore inverse FFT, it will be useful to explore the linear algebraic view of FFT. In this way, we can look at FFT as multiplication of matrices and vectors. So consider a point x_j. The polynomial A of X evaluated at the point x_j equals a_0 plus a_1 x_j plus a_2 x_j squared and so on. The last term is a_n-1 x_j to the n-1. Notice that this quantity can be rewritten as the inner product of two vectors. The first vector are the powers of x_j. And the second vector are the coefficients for this polynomial A of X. Now, FFT is evaluating this polynomial A of X and endpoints. So let's look at this linear algebra view for the endpoints. Now we're evaluating this polynomial A of X at these endpoints. So we're computing A of_x_0, A of x_1 and so on up to A_of_Xn-1. The rows of this matrix would correspond to the powers of these points x_0 through Xn-1. We'll fill that in a second. But first what is this vector? This vector are the coefficients of the polynomial A of X. Now, filling in the rows of this matrix. The first row are the powers of x_0, and then the powers of X1. And finally, we have the powers of Xn-1.

DC5 FFT Part 2 - 238 - LA View of FFT
We just saw this linear algebra view of the evaluation of this polynomial of A(x) at these end points x knot through xN-1. Now let's look at it for the perspective of FFT. For FFT, these n points correspond to the nth roots of unity. So let x of j be the jth of the nth roots of unity. So it's omega sub n to the jth power. Now let's rewrite these vectors and matrices. Replacing these n points by the nth root of unity we have now A(1), A of omega sub n and so on up to A of omega sub n to the n minus 1 power. So these are the nth width of unity. This column vector is going to stay the same it's still going to be the coefficients of this polynomial A(x). Now let's look at the rows of this matrix. Now the first of the roots of unity is 1, so the first row are the powers of it so it's just going to be one. The second row are going to be powers of omega sub n. This in fact are just the nth roots of unity. The third row is going to be one omega sub n square, omega sub n to the fourth and so on up to omega sub n to the two times n minus one. It's just the powers of omega sub n square. The last row are going to be the powers of this last root of unity. So it's going to be 1 omega sub n to the n-1 and so on. The last term is omega sub n to the n-1 times n-1. Now what's important thing to notice about this matrix. Well, first off it's symmetric, the entry IJ is the same as the entry JI, so it's probably going to have some nice properties. The other thing to notice is that it's just a function of omega sub n. The entries of this matrix are just powers of omega.

DC5 FFT Part 2 - 239 - LA for Inverse FFT
We had this linear algebraic view of FFT. Now let's simplify it a little bit. This column vector is just the vector a. Let's denote this column vector by capital A. And let's denote this matrix by M, let's use subscript n to denote its size M by n. And as we observed, it's simply a function of omega sub n. So given this variable, and this size, then we know it's M by n, and it says powers of this variable, in this case omega sub n. Therefore, this expression can be rewritten in the following manner. This vector capital A is equal to this matrix capital M times this vector little a. This product is exactly what FFT is computing. When we run FFT with inputs little a and omega sub n, it computes the product of this matrix and this vector and outputs this vector capital A, which is the value of this polynomial at the nth root of unity. Now what do we want to do for inverse FFT? Now we want to take this value of this polynomial at these nth roots of unity and compute the coefficients. Well, suppose this matrix has an inverse and we multiply both sides by this inverse. Well, then we have that the inverse of this matrix times this vector, capital A, equals this vector little A. So FFT computes this product, this matrix M times this vector, little a. For inverse FFT, we want to compute the inverse of this matrix times this vector, capital A. How does this inverse of this matrix relate to the original matrix? Well, actually they're very similar to each other.

DC5 FFT Part 2 - 240 - Inverse FFT
Once again, FFT when we run it with input little a. Which are the coefficients for this polynomial a of X and the nth root of unity, then it outputs capital A, which is a value of this polynomial at the nth root of unity, which corresponds to the product of this matrix M times little a. Now for inverse FFT, we want to take these values of this polynomial and multiply by the inverse of them. And that will give us little a the coefficients. What does the inverse of M look like? Well what we show is that the inverse of M equals one over n, just a scaling factor, times Mn with instead of Mn with omegas of n It's omegas of n to the inverse. So we take the same matrix Mn, and instead of plugging in the nth root of unity, we plug in the inverse of the nth root of unity. Now what exactly is the inverse of the nth root of unity? Well, this is the number when multiplied by omegas of n equals one. You multiply a number by its inverse you get one. So what is the inverse? It's omegas of n to the n minus one. It's the last of the nth roots of unity. Notice that if you multiply omegas of n times omega of n to the nth minus one, what do you get? You get omegas of n, to the nth. Which is the same as omegas of n to the zeroth power, which is one. So the inverse of omegas of n is omegas of n to the nth minus one. So the inverse of omegas of n is omegas of n to the n minus one. Now this is the basic facts, so we should make sure that is clear for you. If it's not intuitively clear, I would either convince yourself by plugging in these points in polar coordinates and also you can look at it geometrically. So draw the picture of the complex plane and look at these points on the unit circle. Now we can plug this in and simplify. So the inverse of this matrix M, with parameter omegas of n, is one over n times this matrix with the inverse of omegas of n which is omega of n to the n minus one.

DC5 FFT Part 2 - 241 - Inverse FFT via FFT
Now let's recap what we have, FFT is computing the product of this matrix capital M times little a and that's outputting capital A, which is the value of this polynomial at these nth roots of unity. For inverse FFT, we want to do the reverse so we want to compute the product of an inverse times capital A and get back the coefficient vector little a. Now we claim the following Lemma, which we'll prove momentarily, the inverse of M is one over n times M of Omega sub n to the n minus one so we take the same matrix instead of parameterize by Omega sub n, we parameterize it by Omega sub n to the n minus one which is just a different root of unity. So let's take this expression multiply both sides by little n and then substitute in n inverse with this quantity. So we have n times little a and then for M inverse, we replace it by this quantity M sub n of Omega sub n to the n minus one times capital A. Notice this corresponds to an FFT computation. In particular, we want FFT with instead of using input little a, we use in put capital A and instead of using Omega sub n as the nth root unity, we use Omega sub n to the end n minus one which is also an nth root unity. Now it's quite intriguing what's happening here, we're taking the value of this polynomial A inverse at the nth root unity and we're treating these values as coefficients for a new polynomial. Now we run FFT for this new polynomial and instead of using the nth root of unity Omega sub n, we're using this nth root unity. It's still an nth root unity so we can again run FFT. So we run FFT with these two inputs, we get back a vector which we scale by one over n and this gives us the coefficients for our polynomial A of x. So we can go from the values at the nth root unity to the coefficients. One more intriguing fact before we move on to the proof of this Lemma, now FFT normally, we run it with Omega sub n. It is this point right here. Then the end points we consider are Omega sub n to the powers which corresponds to the nth root unity going from one to Omega sub n and and so on around the unit circle in this manner. So we go counter-clockwise around the nth root unity. Now what happens when we run FFT with Omega sub n to the n minus one? That's this point here. Now the only difference is we're going over the same points but we're going over them in different order. Now we go over the nth root unity in clockwise order. So inverse FFT is the same as FFT, we just go over the nth root unity in the opposite order. That's the amazing fact. And we can use the same algorithm as we detailed before because when we detail the FFT algorithm we allowed any nth root unity there. Now I will proof the Lemma and that'll complete our polynomial multiplication algorithm and our convolution algorithm.

DC5 FFT Part 2 - 242 - Quiz Inverses Question
Before we dive into the proof of the Lemma, let's take a quick quiz on some basic properties of roots of unity. We saw just before about Omega_n to the negative one, the inverse of Omega_n. To make sure you understand that let's look at Omega_n square. What's the inverse of this number? And don't simply write it with negative two in the exponent, write in some manner so that you have a positive exponent.

DC5 FFT Part 2 - 243 - Quiz Inverses Solution
This solution is omega sub n to the n minus 2. If I multiply omega sub n square times omegas sub n to the n minus 2, we get omega sub n to the nth power which is one. Similarly, in polar coordinates, this number in polar coordinates is 1 comma two pi over n times 2. This number is 1 comma two pi over n times and minus 2. When we multiply we get 1 in the radius, we add up the angles so we get 2 pi over n times n. This is the same as 1 comma 2 pi which is 1. So this verifies that the inverse of this number omega sub n squared is omega sub n to the n minus 2.

DC5 FFT Part 2 - 244 - Quiz Sum of Roots Question
Let's take another quiz on some basic properties of the roots of unity. Let's consider even n. And let's look at the sum of the nth roots of unity. So let's take omegas sub n to the zero power, plus omega sub n to the first power, plus omega sub n squared, and so on, up to omega sub n to the n - 1. What does this sum equal?

DC5 FFT Part 2 - 245 - Quiz Sum of Roots Solution
The answer is zero. This sum is zero. The sum of the nth roots of unity is zero. Why is that true? Well, that follows just from the plus minus property, which was the key to our divide and conquer algorithm. One is omega sub n to the zero power. What is omega sub n to the n over two? Well, if you think of the complex plane and the roots of unity, we're going halfway around the unit circle. This is negative one. They're opposites of each other. So when we add them up they're going to cancel each other out. Similarly, the jth of the nth roots of unity is opposite of the n over second plus jth. So the first n over two are opposite the last n over two. They're are going to cancel each other out and we're left with zero. This is true for any even n.

DC5 FFT Part 2 - 246 - Proof of Claim
Now in a proof of dilemma about the inverse of the matrix M, we're going to need the following claim which is a generalization of the quiz you just took. The claim says that if we take any omega which is Nth root of unity. So omega raised to the nth is one, and we assume that omega is not one but any other root of unity. And if we look at the sum of powers of omega. So one plus omega plus omega squared and so on up to omega to the N minus one, then this sum equals zero. Now we just saw that this is true when omega equals omega sub N and N is even. We're going to need this more general claim so let's go ahead and prove it. Now first off though. Notice why it's not true when omega equals 1, when omega equals 1 then this is 1, 1, 1. All the terms are 1. So this is going to be N. it certainly does not equal to zero. Now let's forget about this claim for a second. For any number of Z the following holds, look at Z minus one times 1 plus Z plus Z squared up to Z to the N minus 1. So powers of Z. So looks a little bit similar to the claim. Multiply this out, what do you get? Well first two Z times this quantity. So you get Z plus Z squared and so on up to Z to the N. And you've got negative 1 times this quantity. So you have minus the same quantity. Notice most of the terms cancel each other out, Z minus Z, Z squared minus, there's the Z squared term there, Z to the N minus one minus Z to the N minus 1. What's left? The only term left here is the last term Z to the N and the only term left here is the first term, minus one. Now let Z equals omega from the hypothesis of the claim. What do we know? We know that omega is nth root of unity. So if we take omega or Z raised to the nth power, we get 1, so Z to the N minus one equals zero, which means either this quantity equals zero or this quantity equals zero or both. But we assume that omega is not 1. So that means Z is not 1. So Z minus 1 is not 0. So that can't be the case. So this quantity must be equal to zero. That's what we're trying to prove. When Z equals omega. We're trying to prove that this sum equals zero. So that completes the proof.

DC5 FFT Part 2 - 247 - Proof of Lemma
Now let's go back to proving the lemma. We're trying to show that the inverse of this matrix M is one over n times this matrix M parameterized by omega sub n to the negative one. So the inverse of this nth root of unity, which we saw before is omega sub n to the n minus one, but it will be more convenient to treat it as omega sub n to the negative one. Now rewriting this, so multiply both sides by the matrix capital M. This side becomes the identity matrix, capital I, and the right hand side becomes one over n times M parameterized by omega sub n to the negative one times M parameterized by omega sub n. Now, what is the identity matrix? Well, this has ones on the diagonal and zeros off the diagonal. So let's look at the product of these two matrices and we're going to look at the diagonal entries of this product and show that those are n and the off diagonals we're going to show are zeros. So to recap, we have to show that the product of these two matrices, the diagonal entries, are n because one over n times these products should be ones, and the off diagonal entries, so for k not equal to j, the entry k, j, these should be zeros. So we'll have two cases, the diagonal entries and the off diagonal entries.

DC5 FFT Part 2 - 248 - Diagonal Entries
Let's first look at the proof for the diagonal entries. So let's look at the product of these two matrices, capital M parameterized by omega sub n inverse and capital M parameterized by omega sub n. And we want to show that the diagonal entries, so the entries k, k equal n. First off, let's recall what this matrix capital M is. So capital M parameterized by omega sub n is this matrix. This is what we saw before when we analyzed FFT. Now this matrix capital M parameterized by omega sub n inverse is just this same matrix with omega sub n replaced by omega sub n to the negative 1. So we get this matrix here. Now we're looking at the entry k, k. So when you take the kth row and the kth column. The kth row of this matrix is the vector 1 omega sub n to the negative k and then powers up to omega sub n to the negative n minus 1 times k. The kth column of this matrix is this vector the entry k, k in the product matrix is the dot product of these two vectors. First term is one and then we do omega sub n to negative k times omega sub n to the k. This is the same as omega sub n to the 0, which is 1. So this term is 1. And similarly the third term is also 1. All the terms are 1. How many terms are there? There's n terms. So we get n. So we proved what we want for the diagonal entry.

DC5 FFT Part 2 - 249 - Off-Diagonal Entries
Now let's look at the off-diagonal entries of this product matrix. So we want to show that entries k, j equals zero when k is not equal to j. Because if k equals j, that's the diagonal entry. And we just showed that equals n. But, if they're not equal, we'll show it equals zero. Here are the pair of matrices once again. We're again going to take row k and now we're going to take column j over here. The kth row of this matrix is the same as before and the jth column of this matrix is this vector. When we take the dot product of these two vectors we get the following one plus omega sub n to the j minus k. And then we get powers of omega sub n to the j minus k. Well, let's simplify this. Let's do a change of variables to simplify it. Let's let omega be omega sub n to the j minus k then the dot product of these vectors can be simplified as one, first term is omega sub n to the j minus k which is omega plus omega squared up to omega to the n minus 1. Now what do we know about omega. Well, it's the nth root of unity raised to some power. So it's still an nth root of unity and we know that the exponent is not zero. Since it's not zero, then this thing is not 1. So omega is an nth root of unity it's not one. So we can apply our claim. We're just doing powers of the nth root of unity. We know for any nth root of unity which is not one. If we take powers of it, what do we get? We get zero. Which proves this off-diagonal entries are zero as we desire. And that completes the proof of the lemma.

DC5 FFT Part 2 - 250 - Back to Poly Mult.
Let's go back to this earlier slide with our polynomial multiplication algorithm. Now we know how to do this last step. We know how to do inverse FFT which goes from the values of this polynomial C_of_x to the coefficients of this polynomial. So let's add in the details of this last step which will complete our polynomial multiplication algorithm. Now in this last step, we're going to run FFT using these values t, t are the values of C_of_x at the 2nth roots of unity. Now we treat these values t as the coefficients for a polynomial. So this vector t is the first parameter in our input. The second parameter is a root of unity. What root of unity do we choose? We want to use the inverse of the 2nth root of unity, which is the last of the 2nth roots of unity, namely its omega sub-2n raised to the 2n minus one power. Now when we run FFT on this input, we're going to get 2n points return. Let's use this vector C as the return value, so C_nought through C_sub_2n minus one. But recall, we have to scale this output, so we have to take the vector that's returned by FFT, scale it by one over 2n, and that gives us the coefficients of this polynomial C_of_x, and that's it. That completes our polynomial multiplication algorithm and similarly, our convolution algorithm.

