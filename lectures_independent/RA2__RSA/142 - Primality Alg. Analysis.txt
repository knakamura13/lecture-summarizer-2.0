Let's take a look at how this simple Primality testing algorithm performs. If r happens to be prime, what does the algorithm do? It always outputs that r is prime, because for every z in this set, z raised to the power r_minus_one is congruent to one_mod_r, by Fermat's little theorem. So, the probability that the algorithm outputs that r is prime, is one, this is always going to do so. So, it's always correct when r is prime. Now, what happens when r is composite? And let's assume that r is not a Carmichael number. Now, sometimes the algorithm is going to be correct, it's going to output that r is composite. When is that the case? When it finds a z which is a Fermat witness. So z raised to the power r_minus_one is not congruent to one_mod_r. But sometimes it's going to get confused, it's going to make a mistake, and it's going to find a z, which z raised to the power r_minus_one is congruent to one_mod_r. So it's going to think that r is prime. What is the probability that the algorithm outputs that r is prime, so, it makes a false positive statement? Well, we know that at least half the zs in this set are Fermat witnesses. So what's the chance that it finds a non witness? Well, at most half of them are not witnesses, so therefore, the probability of finding a non-witnesses is, at most, a half. So, the probability of a false positive is, at most, a half. Okay, so, we have a reasonable algorithm with probability of most a half that we get a false positive, and when it is prime, it's always correct. But can we get this better? Can we improve this error probability of a false positive?