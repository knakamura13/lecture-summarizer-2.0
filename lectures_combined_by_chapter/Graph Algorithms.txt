GR1 Strongly Connected Components - 251 - Graph Algorithms
The next section is about graph algorithms. I'm sure you've all seen the basic graph algorithms before; DFS for depth-first search, BFS for breadth-first search and Dijkstra's single source shortest path algorithm. We'll do a quick review of how DFS is used to find the connected components of an undirected graph, and then we'll build on that to look at connectivity in directed graphs. We'll use DFS to find the strongly connected components of directed graphs. These are the analog of connected components in directed graphs. And then we'll see an application of our strongly connected component algorithm to solve the two set problem. Next, we'll look at the minimum spanning tree problem, MSTs. You've likely seen that Kruskal's and Prim's algorithm before for finding an MST. We'll look at the correctness behind these algorithms. Finally, we'll look at the PageRank algorithm. This is the algorithm that looks at the Web graph and assigns weights to vertices or webpages. It's a measure of their importance. This algorithm was devised by Brin and Page, and it's at the heart of Google's search engine. To understand the PageRank algorithm, I'll first give you a quick primer on Markov chains, and then you'll see how it relates to strongly connected components.

GR1 Strongly Connected Components - 252 - Outline
In this lesson, we're going to look at connectivity algorithms using algorithms based on DFS. DFS of course, refers to depth-first search. We'll start off by reviewing DFS for undirected graphs and look at the algorithm for determining connected components and undirected graphs. That algorithm is probably familiar to many of you. After that, we'll look at DFS for directed graphs and our goal is to determine the analog of connected components for directed graphs. We begin by looking at DAGs. DAGs are directed acyclic graphs. Acyclic means that it has no cycles. We'll see how the topologically sort DAGs. What this means is that, we can order the vertices, say from left to right, so that all edges go left to right. Now, this algorithm may be familiar to many of you, but we'll use it to derive some intuition for our more sophisticated algorithms for general directed graphs. For general directed graphs, we are going to be looking to find the SCCs. These are the strongly connected components. This is the analog of connected components for directed graphs. The algorithm for finding SCCs is really sweet. It's just two DFS. Actually, it's the same DFS algorithm we found before for undirected graphs and we just run it two times and we'll find this strongly connected components of any directed graph. Now, it's a very simple algorithm but we're going to get there a bit slowly. We're going to go through all of these steps in order to derive some intuition before we get to this more general algorithm for general director graphs. So let's start with undirected graphs and let me remind you about the DFS algorithm that you've probably seen a million times.

GR1 Strongly Connected Components - 253 - Undirected Graphs
For a given undirected graph, how do we find it's connected components? Well, we simply run DFS and we keep track of the component number. So, each vertex is going to have its component number stored. Let me remind you of this pseudocode for DFS, because we're going to make some tweaks to it during the course of this lecture. So, the DFS algorithm takes as input a graph, G. For now, let's think of G as an undirected graph. But later, we're going to run the same algorithm on a directed graph and it's going to be identical pseudocode. And we assume that the graph is given to us in adjacency list representation. And for now, we're looking at connected components of undirected graph. So, the vertices are going to be labeled by a connected component number. We're going to have a counter which is the current connected component number. We're going to have an array which keeps track of whether we visited a vertex yet or not, and we're going to start off by initializing the visited array to false for all vertices of the graph. Now, we go through the vertices in an arbitrary order. Now, if we get to a vertex that we haven't visited yet, what do we do? Well, this means that we found a new connected component. So, we increment the connected component number and then we start exploring from this vertex. And let's now look at the subroutine for explore.

GR1 Strongly Connected Components - 254 - Exploring Undirected Graphs
Now let's look at the pseudocode for the Explore procedure, and let's say we're running Explore from a vertex Z. This is our first time visiting Z. So we have to store its connected component number as the current count for the connected components, and we have to set Z to be visited. Now, we want to explore all edges out of Z. Recall that G was given to us an adjacency list representation, so now we can look through the linked list of neighbors of Z. Now for a particular neighbor W, if W hasn't been visited yet then we recursively explore from W and we repeat this procedure. Now what's a running time of this algorithm? Hopefully, you recall that DFS is a linear time algorithm. So the running time is O(N + M). For undirected graphs, this is it, this gets all the information that we're trying to glean, the connected components of the graph. Now we're going to turn to directed graphs and we're going to need more information from our DFS in order to obtain connectivity information.

GR1 Strongly Connected Components - 255 - DFS Paths
We saw in the last slide how to use the DFS algorithm to find the connected components of an undirected graph. Before we move on to directed graph, let's gleam a little bit more information from the DFS algorithm. In particular, suppose I have a pair of vertices, v and w which are in the same connected component. I want to find a path between this pair of vertices. To do that, we simply have to keep track of the predecessor Vertex when we first visit a vertex. Here's the DFS algorithm once again for finding connected components of an undirected graph. We're going to use this previous array as is used in Dijkstra's algorithm to keep track of the predecessor Vertex. We initialize this previous array to null for every vertex. And when we first visit a vertex. So at this point in the algorithm, we should set its previous array to its predecessor Vertex, which is Vertex Z. So, we set previous W to Vertex Z. Now, after running DFS algorithm, given this previous array, we can use this previous array to backtrack. So, for a pair of vertices which are in the same connected component, we can use the previous array to find a path between this pair of connected vertices. That completes our discussion of DFS algorithms for undirected graphs. Let's move on now to directed graphs.

GR1 Strongly Connected Components - 256 - DFS on Directed Graphs
We saw how to determine the connected components for an undirected graph, now let's take a look at directed graphs. How do we determine the connectivity properties for a directed graph? Once again we're going to use a DFS based approach, but now for directed graphs, we're going to need additional information from the DFS. The additional info that we use are the preorder or postorder numbers for the tree or forest of explorer the edges. The algorithm is going to be a slight variant of DFS that we saw just before, so let's look at that previous algorithm and just modify it a little bit. Here's a DFS algorithm for figuring out the connected components of an undirected graph. Our basic algorithm is going to be the same, but we no longer need to keep track of the connected component number, so let's remove those lines. This line, this line and this line. We can drop these three lines. So I've removed those three lines which talked about the connected component number. Now I want to add in lines which take care of the preorder and postorder numbers. In order to keep track of the preorder and postorder numbers, we're going to add in the clock. The preorder number for a vertex Z, is going to be the value of the clock when we first visit vertex Z, and the postorder number is going to be the value of the clock at the time when we finished exploring vertex Z. So we looked at all edges out of Z. First we need to initialize the clock to one. When we first visit a vertex Z, we can store as preorder number, which is the current value of the clock. After we do this, we need to increment the clock. Finally when we finish exploring vertex Z, then we can set its postorder number to be the current value of the clock. And then once again we have to increment the value of the clock. That gives as DFS undirected graphs. Now we want to see the properties of these preorder and postorder numbers. And actually for our connectivity algorithms, we're simply going to use the postorder numbers. And to be perfectly honest, I'm not even sure where preorder numbers come into play. The only application I know of for preorder numbers are in order to try to trick you on exams or homeworks. But just in case I kept the preorder number in here in the algorithm.

GR1 Strongly Connected Components - 257 - Directed DFS Example
Here's an example of a specific directed graph on eight vertices. Now lets run our DFS algorithm for directed graphs on this specific example starting at the vertex B. And for concreteness we'll assume that the linked lists are stored in alphabetical order. So when we look at the edges out of B for example, we're going to see A and C then E. And let's look at the tree of explored edges in our DFS run. We're going to start at the vertex B, so this is going to be the root of the tree. And let's keep track of the pre-order and post-order numbers of the vertices as they are stored. We start exploring from B, so it gets a pre-order number. We first see neighbor A of vertex B. So the next edge that we explore is the edge B to A. We then assign of pre-order number to A. We then see vertex D and we give it its pre-order number. Then exploring from vertex D we see vertex E and then from vertex E we see vertex G. G gets pre-order number five. But notice from G there's nothing to explore. So we're going to pop back up from G back up to E. So we're going to finish exploring from G so we can give it post order number six. Now notice from Vertex E when we do explore from E, it was in an an edge from E to A. A has already been explored, has already been visited at that time. So we're not going to rerun explorer from A. This is not an edge in the DFS tree in the sense that A has already been explored at this time. So we're not going to rerun explorer from A but let's keep track of this edge and mark it as blue edge in order to distinguish it from the explored edges which are marked as black. After we've explored the two edges out of E to A and to G then we're going to pop back up to Vertex D. But first we're going to assign a post order number to E. Now from D we're going to see this edge to H, so we're going to explore from H. Now from H we see this neighbor G. G has already been visited, so let's mark this edge from H to G as a blue edge. Then we're going to finish exploring from H and we're going to pop back up to D. Lets assign its pre or post order numbers. From D there was an additional edge to G. Then we can assign its post order number and pop back up to A. A is done and then we can pop back up to B. From vertex B we're going to see vertex C and then F, so we're going to get this right sub-tree. Now from F we're going to see this edge back to B, that's a blue edge, and we're going to see this edge across to H and we can send the pre-order post-order numbers. And that completes the DFS run for this example.

GR1 Strongly Connected Components - 258 - Types of Edges
Now let's look at a specific edge of the graph. Let's say goes from vertex z to vertex w. Now, that edge is going to appear in this graph over here of the DFS tree. It's either going to appear as a black edge, an explored edge, or as a blue edge. Let's look at the properties of this edge based on whether it's a black edge or it's a blue edge and what type of blue edge. Now, the black edges correspond to explored edges or edges of this tree, this DFS tree. Notice in this example, the DFS tree happened to be a tree. Every vertex happens to be reachable from the vertex B. It doesn't have to be the case. It could be a forest. We could have multiple components here. So, actually this should be called forest edges, but let's ignore that. We call this a DFS tree, even though it might be a forest. Some examples of these tree edges are B to A, A to D, and so on. All of the black edges are tree edges. Now, let's look at the properties of the post order numbers for these tree edges. Let's take the example of A to D. So we started exploring from A. Then we saw this neighbor D. So we recursively started exploring from D. We're going to finish off D, and assign a post order number, and then we're going to pop back to A. So, A is going to finish after D. So, the post order number of A is going to be bigger than the post order number of D, because it's going to finish later. And in general, the post order number of the head of this edge is going to be bigger than the post order number of the tail of this edge if it's a tree edge. Now, let's look at the blue edges. There's going to be three types of blue edges. Back edges, forward edges, and cross edges. The examples of back edges in this graph are E to A and F to B. The edge goes from a descendant to an ancestor. So, these are edges that go back up the tree. Now, let's take a specific example. Let's say E to A. Notice we're going to finish off E before we finish off A. So, in this case, the post order number of E is going to be smaller than the post order number of A. And in general, for back edges, the post order number of the head of the edge is going to be smaller than the post order number of the tail of the edge. This is opposite for the case of the tree edges. Now, let's look at forward edges. These are going down the tree. For example D to G. Another example of a forward edge is this edge, B to E. I forgot to notice this edge earlier when we were running DFS on our earlier example. Now what do you notice about the post order numbers for forward edges? Well, these behave just like tree edges. Tree edges are going down one depth. Forward edges are going down multiple depths. But, they have the same key property, that the post order number goes down. Finally, we have cross edges. For example, F to H and H to G. These are pairs of vertices that have no ancestor descendant relation to each other. Look at this edge from F to H. H must have been explored before F. Otherwise, H would be in the sub tree of F. Since this is an unexplored edge, that means that H was finished first, so the post order number of H is going to be smaller than the post order number of F. So, once again, the post order number goes down with the edge. Now, the key property is that for back edges, the post order number goes up. For all other edges, the other three types of edges, tree edges, forward edges, and cross edges, the post order number goes down. Now, that is the key property that we need for post order numbers. Back edges behave differently than the other three types of edges.

GR1 Strongly Connected Components - 259 - Cycles
Now, let's look at properties of the graph and how these properties manifest themselves in the DFS tree. Let's look now at cycles. How does the cycle manifest itself in the DFS tree? The key property is that graph G has a cycle, if and only if, it's DFS tree has a back edge. Now, this doesn't matter where we start the DFS. What is the starting vertex? It doesn't matter how the vertices are ordered in the adjacency list representation also. For any start vertex, for any ordering on the vertices, for any ordering on the neighbors of every vertex, the DFS tree will contain a back edge, if and only if, G has a cycle. So, if there is a cycle, there will be a back edge that will appear in our DFS tree. And if the DFS tree contains a back edge, then there is a cycle in the graph. Let's see why this property holds. This is an equivalence relation. So let's look at the two implications. Let's look at first at the forward implication. Let's suppose that G has a cycle and let's see how a back edge appears. Let's suppose the graph G has a cycle and let's label the vertices of that cycle as a, b, c, up to j. So, there's an edge from a to b, b to c, up to j, and then back to a. Now, one of these vertices has to be explored first. There always has to be somebody first. So, let's say the first vertex is vertex i. So, what do we know then about our DFS tree? We know we have this vertex i. And now if we look at the sub-tree of i, we know that all these other vertices of the cycle are reachable from i. So, they're all going to lie in the sub-tree rooted at i. So, all the other vertices of the cycle are going to be contained in this sub-tree rooted at i. We don't know anything else about the structure of this sub-tree, we just know that this sub-tree contains all the other vertices of this cycle because they're all reachable from vertex i. Now at least one of these vertices has an edge to i. In this case, we know that i - 1 has an edge to i. This edge is going to appear as a back edge because it goes from a descendant to an ancestor. So we're going to have a back edge from i minus one to i. Now, let's look at the reverse implication. Let's suppose that our DFS tree has a back edge and let's prove that the graph then must contain a cycle. Let's say there is a back edge from vertex a to vertex b. So, what do we know? We know that vertex a is a descendant of vertex b, and there's this back edge from a to b. But since a is a descendant of b in this DFS tree, we know there's a sequence of tree edges which go from b down to a. And notice we now have our cycle. These tree edges are edges in the graph and then this back edge is also the edge of the graph. So, we have our cycle from b down to a and then back to b. That shows that there is a cycle. For every back edge, there is a cycle. That proves this property.

GR1 Strongly Connected Components - 260 - Toplogical Sorting
Now let's take a look at DAGs. These are directed acyclic graphs. Acyclic means that there are no cycles in the graph. What we just saw is that the graph has a cycle if and only if the DFS tree has a back edge. Since there are no cycles, there's going to be no back edges in our DFS tree. Now what we're going to try to do is, we're going to try to top allegedly sort that DAG. What does that mean? We're going to order the vertices so that all edges go from lower order number vertex to a higher order number of vertex. So for instance, if we write down the vertices in order from lowest to highest, then all edges are going to go from left to right. We're not going to have any edges going backwards. To topologically sort this DAG, what we're going to do is, we're just going to run DFS on this DAG. What is the key property we know for this DAG? We know it has no back edges. What do we know about the post order numbers for all other types of edges? We know that for back edges, the post order number increases along the edge. For all other types of edges, the post order number goes down along the edge. Now we want to order the vertices from lowest to highest so that all edges go left to right. So which vertex do we want to put first? We want to put the vertex with highest post order number first and therefore all edges are going to go from higher post order number to lower post order number, because there are no back edges. Just to summarize, we know for every edge of the graph. So for instance this edge from Z to W. We know that the post order number of Z is greater than the post order number of W. So in order to topologically sort the graph, we order the vertices by decreasing post order number. The highest post order number comes first and the lowest post or number comes last. So in order to top logically sort a DAG, we just have to do one run of DFS and then sort by decreasing post order number. Now how long does it take us to sort by decreasing post order number? Now you might think this is order n log n time, because we have to do sorting. Now what is the range of these post order numbers? Where the clock starts at one, so all the post order numbers are at least one. How large can the post order number be? Or the maximum imposed order number is going to be two times n so all the post order numbers range between one and 2n. So what can you do? You can make an array of size 2n. Now we can go through the vertices, take their post order number and insert them into the appropriate place in the array based on their post order number. Now we go through this array from highest to smallest and we just output the vertices as we see them. This gives us the vertices in decreasing order of post order number. How long does this take to construct? Just takes linear time. So it takes order n time to sort the vertices by decreasing post order numbers and takes linear order n plus M time to run DFS. So the total algorithm takes order N plus M time.

GR1 Strongly Connected Components - 261 - Topological Ordering Quiz Question
Now here's an example of a graph on five vertices. Why don't you go ahead and give a topological ordering for this graph. There are five vertices, so let's write down the vertices in order so that all edges go from left to right. Now this graph happens to have multiple topological orderings. How many topological orderings does it in fact have? To make sure you understand topological orderings, why don't you go ahead and specify the number of topological orderings which are valid for this graph.

GR1 Strongly Connected Components - 262 - Topological Ordering Quiz Solution
Now here is one valid topological ordering, X, then Y, then Z, then U, then W. Notice that all the edges go left to right. Now how many valid topological orderings are there? Well, I can move U to any of these last three positions. So there are three choices for the position of U. After I specify the position for U, then the position for Z and W are forced because Z has to come before W. So this graph has three topological.

GR1 Strongly Connected Components - 263 - DAG Structure
Now let's take a look at some properties of a DAG that we can derive from this topological ordering. Now there are two types of vertices we want to distinguish, source vertices and sink vertices. A source vertex has no incoming edges. So everything goes out of a source. For a sink vertex. It's the opposite. Nothing comes out of sink. Things only come into a sink. Now a DAG always has at least one source and at least one sink. There may be multiple sources, there may be multiple sinks. But we're always guaranteed there's at least one source and at least one sink. How do we know that there's a source vertex in every DAG? We'll take any topological ordering and look at the first vertex, in this case x. What do we know about x? We know all edges in the topological ordering go from left to right, from earlier in the topological ordering to later. So edges can only come out of the first vertex in the topological ordering. No edges can come in, because then they would come from right to left. So every DAG has a topological ordering, the first vertex in every topological ordering must be a source vertex. So that guarantees that every DAG has at least one source vertex. And there might be multiple source vertices because there might be multiple topological orderings and they might have different vertices at the beginning. Now which vertex is first in our topological ordering? It's the half vertex with the highest post order number. Therefore, the vertex with the highest post order number is guaranteed to be a source vertex. Similarly, the last vertex in our topological ordering must be a sink vertex, because edges can come into it but nothing can come out, otherwise it would be going right to left again. Therefore, the vertex with the lowest post order number is guaranteed to be a sink vertex. And once again, there might be multiple sink vertices. These might be multiple topological orderings with different vertices at the end. In this example, u and w are both sync vertices and only x is a source vertex. Now let's look at an alternative topological sorting algorithm. Now this algorithm is not going to be very useful for DAGs, but it is going to be very useful when we look at general directed graphs. Now we know in a topological ordering, the last vertex in the ordering is a sink vertex. So what can we do? We can find a sink vertex in some way. We can put it at the end of our list and then we can repeat on the remainder of the graph. So we're going to find a sink vertex, output it and then we're going to delete it from the graph and then we're going to repeat. Now we repeat this first step, find a sink, in this case maybe it's u, and then we find z, and we find y and we find x. Notice when x is the only vertex remaining, it's the sink vertex in that graph of size one. Finally, we're left with the empty graph and then we stop. What we've done is we outputed the vertices from the end to the beginning. And this gives us a valid topological sorting. Now how we actually find asink vertex is another question. But this algorithm, this basic approach is valid. And we're going to use this basic approach when we consider general directed graphs. And that's what we're going to turn our attention to now, general directed graphs, and we're going to look at the general question of what does this kind of activity mean in general directed graphs.

GR1 Strongly Connected Components - 264 - Outline Review
We've seen now how to find connected components in undirected graphs and how to topologically sort a DAG. Both of these algorithms involved one run of a vanilla version of DFS. Now let's look at general directed graphs. First we have to talk about what is the right analog of connected components for directed graphs. It turns out to be strongly connected components. And then we'll see an algorithm to find the strongly connected components of a general directed graph. Now the amazing aspect is, that we're going to find these strongly connected components with just two runs of the vanilla version of DFS. Let's dive into this algorithm and let's start by defining strongly connected components.

GR1 Strongly Connected Components - 265 - Connectivity in Directed Graphs
We're looking now at connectivity in directed graphs. So the remainder of the lecture we're going to be talking about directed graphs. First off, we have to talk about what is the correct notion of connectivity between a pair of vertices. So let us take a pair of vertices, V and W and we we'll say they're strongly connected. If there's a path from V to W and W to V. I have this vertex V and this vertex W and there's a path from V to W, it may pass through many vertices on the way and there's a path from W to V that may pass through many vertices along the way and these paths may intersect. So these vertices V and W are strongly connected. So instead of connected components in undirected graphs, the analog for directed graphs is going to be strongly connected components which we'll denote as SCC. Now an undirected graphs, the connected component is a maximal set of connected vertices. We keep adding in connected vertices as long as we can. For directed graphs, strongly connected components are the maximal set of strongly connected vertices. So we keep adding in strongly connected vertices as long as we can. Let's take a look at a specific example and mark the strongly connected components in the example to make sure everybody understands it.

GR1 Strongly Connected Components - 266 - SCC Quiz Question
Here's a directed graph on 12 vertices. Let's go ahead and mark the strongly connected components in this graph, to make sure everybody understands the definition of SCCs. And then we can dive into some interesting properties of strongly connected points. First off, how many strongly connected components does this graph have? And now can you mark what are the strongly connected components in this graph?

GR1 Strongly Connected Components - 267 - SCC Quiz Solution
In this graph, there are five strongly connected components. Let's go ahead and mark them. An easy one to notice is A by itself. Notice that from vertex A, I can reach many other vertices but no other vertices can reach A, so A is not strongly connected to any other vertices, therefore A is a strongly connected component by itself. These five vertices form a strongly connected component. Notice that all five vertices are strongly connected with each other, in particular, from vertex J, I can reach all of the other vertices and all the other vertices can reach vertex A via I. Similarly, these three vertices form a strongly connected component. These two vertices B and E form a strongly connected component and D is by itself because other vertices can reach D but D can't reach anybody else. So I have these five strongly connected components, A by itself, B and E, C, F, G, D by itself and then H, I, J, K, L are together. I want to point out some interesting properties of strongly connected components so I want to look at this graph on five vertices. I'm going to make a meta vertex for each strongly connected component. So when I have a meta vertex for A by itself, B and E together C, F, G, D and H through L. So I'm going to think of compressing each of these purple blobs into a vertex, a meta vertex and then there's going to be an edge from this meta vertex to this meta vertex because there is an edge from some vertex in C, F, G to some vertex in this strongly connected component namely from F to I and similarly there will be an edge between these meta vertices and these meta vertices and so on. So when we look at this graph on the meta vertices of strongly connected components we're going to see some interesting properties.

GR1 Strongly Connected Components - 268 - Graph of SCC
I want to look at the metagraph on strongly connected components. So we're going to have a vertex for each strongly connected component. So let's go ahead and mark the five strongly connected components, the five vertices in this metagraph. Here are the five strongly connected components, A, BE, D, CFG, and H through L. So I've marked those five vertices down here, A, BE, CFG, D, and H through L. Now, what are the edges in this metagraph? Well, some vertex in this component, BE, has an edge to some vertex in this component, CFG, namely B to C. So, we'll put an edge from this component to this component. Similarly, there's an edge from this component, BE, to D, from A to B, F to I, and E to L. Now, what do you notice about this graph? This metagraph on strongly connected components? Now, this metagraph on strongly connected components may, in fact, be a multigraph. For example, I may have an edge from G to J and then, I'll have another edge from this component to this component. So, I have a pair of edges from this component, C F G, to H through L. Now, the multiplicity doesn't matter. So we can keep those multiple edges or we can drop them. It doesn't matter. So let's go ahead and drop those multiple edges for simplicity. Either way, what is the key property of this metagraph on strongly connect components? What do you notice about this metagraph? Notice that there are no cycles. So, this metagraph is a DAG. And that's always the case. Every metagraph on strongly connected components for every directed graph is a DAG. Why is that? Why are there no cycles in this metagraph? Well, suppose there's two strongly connected components which are involved in a cycle. Then that means there's a path from somebody in this component to somebody in this component, and from somebody in this component, there's a path back to this component. Now, we know everybody in this component S is connected to each other because it's strongly connected. Everybody over here in S prime is strongly connected to each other. So, therefore if there's these paths from S to S prime and S prime to S, that means everybody in S can reach everybody in S prime and everybody in S prime can reach everybody in S. Therefore, S union, S prime is a strongly connected component. Now these components are defined to the maximal sets of strongly connected vertices. Therefore, we have a contradiction because a strongly connected component should be S union, S prime, it shouldn't be S separated from S prime. So, if there is a cycle in this metagraph, then we can merge strongly connected components together to get a larger strongly connected component and therefore we get a contradiction. Therefore, there can't be any cycles in this metagraph and thus it must be a DAG. We now have this amazing property. Every directed graph is a DAG of it's strongly connected components. So, we can break up a directed graph into strongly connected components and then we can order these strongly connected components into topological ordering because it's a DAG. So, you can take an arbitrary directed graph, which may be very complicated, and you can find this beautiful structure hidden in it. You can break it up into strongly connected components and then you can topologically sort these strongly connected components so that all edges go left to right. That's what we're going to do now. We're going to find an algorithm which is going to find the strongly connected components and it's going to find the strongly connected components in some order and the order is going to be topological ordering of these strongly connected components. So, we're going to topologically sort these strongly connected components as we find them. Now, the amazing thing is that we're going to find these strongly connected components and this topological ordering with just two runs of DFS. Now, let's dive into the algorithm to see how we're going to find these strongly connected components.

GR1 Strongly Connected Components - 269 - SCC Algorithm Idea
Now, let's look at the main idea for our strongly connected component algorithm. Now, we're going to find these strongly connected components in topological ordering. So let's go back and look at our topological ordering algorithm for DAGs. Now, let's look at the topological ordering of a DAG where vertex V happens to be first and vertex W happens to be at the end. Now, what do we know about vertex W? We know it has to be a sink vertex. It might have some edges in but it can't have any edges out because those edges would go backwards in the ordering. Similarly, vertex V must be a source vertex. It can have edges out but it can't have any edges in. We have this alternative approach for topologically sorting a DAG. We could find a sink vertex, output it, rip it out, and repeat. Find a new sink in the resulting graph and repeat. Or, we could find a source vertex, put it at the beginning, rip it out of the graph, and repeat. Find a new source vertex in the resulting graph and so on. We can either work left to right, or right to left. Finding sink vertices and moving on, or finding source vertices and moving on. We're going to do a similar idea here but instead of finding a single vertex, we're going to find a sink strongly connected component. This is a component which is a sink vertex in a meta graph on strongly connected components. We're going to find a sink strongly connected component then we're going to output it. That's going to be at the end of our ordering. We're going to remove it from the graph, so we're going to remove all vertices from this strongly connected component from the graph and then we're going to repeat. We're going to find a sink component in this resulting graph, output it, remove it, and repeat until the graph is empty. Now, why do we do sink strongly connected components? Why not do source strongly connected components? For the topological ordering of the DAG, it didn't matter whether we started with sinks and worked that way backwards, or if we started with source and work forward. But for SCC it matters. Sinks are easier to work with. Why are sinks easier to deal with? Well, take any vertex which lies in a sink SCC. So S is a sink and strongly connected component, V is the vertex lying in that component. Now run Explorer from V. This is the basic procedure in the DFS algorithm. Suppose this is the first vertex that you explore from, which vertices do you visit when you explore from V? Well, take our earlier example where we had this sink strongly connected component which was H through L. Say we run explore from any of these vertices, what's going to happen when we run Explorer? We're going to visit all the vertices in this sink component but we're not going to visit any other components because we can't reach any other components from this component because it's a sink strongly connected component. So we visit all of this component and we visit nothing else. We don't see any other vertices, we just see this component itself. So if we can find a vertex which is guaranteed to be in a sink strongly connected component, then we can run Explorer from that vertex, and we're going to visit and we're going to find exactly that sink component. That's the key property about sink components. We just need to find a vertex which lies in that component, then when we explore from it, we're going to find the component itself and we're going to see nothing else. Therefore, we can mark all the vertices that we visited from this Explorer as lying in this sink component, then we can rip out those visited vertices and we can repeat the algorithm. Find a vertex in a sink of the resulting graph, run Explorer from it, mark those vertices as being in that component, and repeat. Now, what if we could find a vertex lying in the source component? For example, what if we can find vertex A? And we know that A is guaranteed to be in a source component, while in our earlier example, A happened to be a source component by itself. But suppose there are other vertices in this component and we want to figure out who are the other vertices in this component? When we run Explorer from vertex A, what happens? All we know is that from A we can reach many vertices. It's a source. So in fact, we can reach the whole graph from A. The whole graph is going to be visited. So we have no way of marking which vertices happen to be in this SCC and which vertices lie in other SCCs. But if we run Explorer from a vertex which lies in a sink SCC, we only visit that component and nothing else. That's the key property about sink components. Now, how can we find a vertex V which is guaranteed to lie in a sink component? That's our key task. Once we can find a vertex which is guaranteed to lie in a sink component, then we can run Explorer from that vertex, we'll find that sink component, rip it out, and repeat the algorithm. We'll find a sink component in the resulting graph, rip it out, repeat and so on.

GR1 Strongly Connected Components - 270 - Vertex in sink SCC
So, in a DAG, the vertex with the lowest post order number is guaranteed to be a sink vertex. Now, let's look in a general directed graph, so there may be cycles. Still, let's run DFS on this general directed graph. Is there's some property by post ordering numbers so that we can find a vertex which is guaranteed to lie in a sink SCC? While drawing inspiration from DAGs, we may say, the vertex with the lowest post order number. Maybe that happens to be guaranteed to lie in a sink SCC. Now we might hope for the following property in general directed graphs. We might run DFS on this general directed graph and we might hope that the vertex with the lowest post order number always is guaranteed to lie in the sink SCC. If that was the case, then in order to find a vertex in a sink SCC, we just run DFS on this general directed graph, take the vertex with the lowest post order number, and that has the guaranteed property that we're looking for. Now, this guess follows from our inspiration from the topological sorting algorithm for DAG. Now does this hold? Is this property true? Unfortunately, it's not true. Here's an easy example where it's not true. I have a graph on three vertices, A, B, and C. Now, A and B are strongly connected with each other and C is by itself. Now, let's say I run DFS starting from vertex A. So, A's a root and then from A, I visit vertex B, then from B, I pop back to A and then I visit vertex C, and then I pop back to A. Now, what are the pre order and post order numbers? I start with A. Then I go to B. Then I finished B, pop back to A, go to C. Then I finish C and then I pop back to A. So, which vertex has the lowest post order number? It's vertex B. But, B lies in this strongly connected component. Is this a sink SCC? No. In fact, it's a source SCC. So, in this example, the vertex with the lowest post order number lies in a source SCC. Complete opposite of what we're hoping for. But what if instead of finding a sink SCC, I want to find a source SCC. I don't know how to use that, but let's just say I wanted to find a source SCC. Let's go back to look at our DAG algorithm. So, I want to find a source vertex. Remember in our topological sorting algorithm, the vertex at the beginning of the topological ordering is the vertex with the highest post order number. So, the vertex with the highest post order number is guaranteed to be a source vertex in a DAG. So, in the general directed graph, does the vertex with highest post order number always lie in a source SCC? Well, in this example, that's actually true. The vertex with highest post order number is A, which lies in a source SCC. It turns out that this property is true and we're going to prove that it's true. So, in every directed graph, when we run DFS on that directed graph, it doesn't matter on which vertex we start at and which is the ordering on the neighbors, for every directed graph, for every DFS run on that directed graph, the vertex with the highest post order number is guaranteed to lie in a source SCC. Now, let's use this property to get an SCC algorithm and then we'll go back and we'll prove that this holds. First off, how can we use it? We notice before that we need a vertex that lies in a sink SCC. If we have a vertex which lies in the source SCC, that's not useful for us. But all we can guarantee is to find a vertex which lies in a source SCC. We don't know how to find a vertex in a sink SCC.

GR1 Strongly Connected Components - 271 - Finding sink SCC
This is the property that we just claimed is true. When we run DFS on any directed graph, the vertex with the highest post order number lies in a source SCC. We'll go back and prove this property momentarily, but let's first use it to get an SCC algorithm. We need a vertex which lies in a sink SCC. How can we find such a vertex W which is guaranteed to lie in a sink SCC? Well, we know how to find a vertex which lies in a source SCC. I claim we can use this as a subroutine in order to find a vertex which is guaranteed to lie in a sink SCC. And then we're all done then we have our SCC algorithm. Why don't you go ahead and think about this? How can we find a vertex which is guaranteed to lie in a sink SCC using this property that the highest post order number lies in a source SCC? In some sense we just want to redefine the terms. We want to redefine a source to be a sink and a sink to be a source. What do we mean by that? Think about our topologically ordering of a DAG. The edges go left to right, and the beginning of the ordering is a source and at the end is a sink. What if we flipped all the edges to go backwards? Then this vertex which used to be a sink would now be a source, and this vertex which used to be a source will now be a sink because all the edges go right to left. So the ordering will be opposite. That's what we want to do now for our general directed graph. We want to flip the graph. We want to look at the opposite graph or the reverse graph. And then the source component will become a sink component, and the sink component will become a source component. Now, for a general directed graph G which has vertex set V, and edge set E. We're going to look at GR. This is going to be the reverse of the graph G. The vertex set is going to stay the same. The edge set is going to change from E to E-reversed. We're just going to reverse all the edges. These are directed edges so we're just going to look at the reverse edges. What exactly is E-R? For every edge in E, if we have an edge from V to W, we're going to add into E-R, we're going to add in the edge from W to V. E-R is simply the reverse of every edge in E. Notice if we flip all the edges, we look from G to G-R then all of the sources and sinks get flipped. Now, how does these strongly connected components of G compared to the strongly connected componentes of G-R? Notice that if a pair of vertices are strongly connected in G then they're also strongly connected in G-R There's a path from V to W and G, and a path from W to V. Then in G-R there also is a path from V to W and W to B. The set of strongly connected components are the same in the two graphs. Now, how is the meta graph on the strongly connected components? Look in these two graphs It might be different. In particular, the meta graph of strongly connected components in G is a DAG. And now, what does it look like in G-R? Well, we flipped that DAG so all the strongly connected components which were at the beginning of the topological ordering are now at the end. The edges are not going right to left instead of left to right. If we take a component which was a source SCC and G then it becomes a sink as SCC in G-R because the ordering goes backwards. Similarly, if we take a component which was at the end of the ordering for G then it's going to be at the beginning of the ordering for G-R. Now, how do we address our original problem? We want to find a vertex which is guaranteed to lie in a sink SCC of G. All we can do is find vertices which are guaranteed to lie in a source SCC. Well, the sink SCC of G corresponds to a source of G-R. So we take this input graph G, we construct its reverse graph, and then we run DFS on this reverse graph, and then we take the vertex with highest post order number as guaranteed to be in a source SCC of this graph G-R. This vertex which is guaranteed to be in a source SCC of G-R, or that SCC, is a sink in G. So this vertex with highest post order number for the DFS run on G-R is guaranteed to be in a sink SCC of G, the original graph. That's it. That's our algorithm for finding a vertex which is guaranteed to lie in a sink SCC of a graph G. You just reverse the graph, run DFS, take the highest post order number of vertex, and is guaranteed to be in a source of G-R, and therefore a sink of G. And now we have our algorithm for finding strongly connected components and is going to find the strongly connected components in topological ordering. We're going to find a sink SCC and move on, so we're going to find this ordering from right to left.

GR1 Strongly Connected Components - 272 - SCC Example
let's look back at our earlier example and now we can illustrate our algorithm for finding strongly connected components in this example here's our input graph G now this graph G had to sink strongly connected components D by itself and these five vertices formed a strongly connected component now our main observation from before was if we ran DFS starting from one of these vertices in the sink SEC so suppose we ran DFS starting from this vertex K well initially everybody's marked as unvisited we want we set K to be visited then we visit L I J and H and noticed no other vertices in the graph are visited so the only visited vertices are exactly this strongly connected component containing K so we can mark all the vertices that were visited so far as component 1 and then we can rip it out of the graph and we can look at the remainder of the graph and we can find a sink SCC and the remainder of the graph for example D will run explore from it will find this component then the remainder of the graph and we'll find one of these vertices or an explore from it find this component and continue that's how our algorithm is gonna work now how do we find this vertex in a sink SCC well to do that we look at the reverse graph here's the reverse graph it's got the same vertex set we simply flipped all the edges so this has an edge from B to D this has an edge from D to B now what we claimed is that if we run DFS on this graph of the reverse graph the vertex with the highest post number is gonna lie in a source SCC in this graph what are the source strongly connected components in this reverse graph it's these five vertices and D so the sink SEC is over here in the original input graph our source SCC is over here in the reverse graph the vertex with the highest post number is gonna lie in one of these source strongly connected components and therefore it lies in our sink SCC in the original graph so let's run DFS on this graph here's an example run of DFS on this reversed graph and I'll make arbitrary choices for the order on the vertices and on the neighbors so let's choose a vertex to start DFS from let's say we start from vertex C we'll give see pre-order number one and then from see we visit G and we give it pre-order number two then we go to F from F there's nobody left to explore so we give it its pre-order number and it's post order number only popped back up to G and so on let me skip ahead from C these are the vertices that we can visit we can see G F B a and E and this is one example of the DFS tree that we see and these are the preorder and postorder numbers that will get now DFS will continue to an unfitted vertex let's say D from D we can't get anywhere so it'll just stop I'll give it a preorder and postorder number then finally we'll get one of these five vertices starting from L this is the DFS tree we get for this component and here's the preorder and postorder numbers and notice the vertex with the highest post order number is vertex L which is in this source SCC well you may think well the go-to started at vertex C if he would have started at one of these vertices then the vertex with the highest post order number would not lie in this component but in fact if we would have started the DFS from one of these five vertices then who can be visited from that vertex well we can visit all this graph except for D so all these other vertices will be in the subtree say of L so they'll all get post order numbers which are smaller than L and then finally we'll go to D and D will be the last vertex visited it'll have the highest post order number so we'll still get a vertex with the highest post order number lying in a source SCC so let's take this vertex L it has the highest post order number in this DFS run that's the important property that we needed now later we're gonna explore all these vertices in this component and then we're gonna need the vertex with the highest post order number of the remaining vertices so in order to obtain that it'll be useful to have all these vertices sorted by their decreasing post order number so let's do that so this is a list of vertices of the original graph or the reverse graph sorted by decreasing post order number from this DFS run on the reverse graph and now we're done with the reverse graph and we can go back to our original graph here's our input graph once again now what are we gonna do we're gonna run DFS starting from this vertex L when we run from L who do we visit we visit i j k and h now as we visit these vertices let's cross them out to mark that they're visited and let's assign them a strongly connected component number so our mark these five vertices are strongly connected component number one now we need to continue DFS who do we want to continue from we want to look at this graph and we want to find a sink in this remaining graph so who are we gonna choose we're gonna choose the vertex with the highest post order number of the remaining vertices in this case it's D so we run DFS from D we Explorer from D D becomes visited and we mark it with component number two then we run from C we see these three vertices and we mark them with component number three then we take the highest post order number of the unvisited vertices B in this case we explore from B who do we see we see B and E that would be component 4 finally a that's component 5 valla we have our strongly connected components and the vertices are labeled by their strongly connected component number and one other very cool feature what do you notice about these strongly connected component numbers here are the five strongly connected components notice there's an edge from meta vertex 3 to meta vertex 1 from 4 to 1 and so on these are the rest of the edges in the meta graph now what do you notice about these edges they all go right to left so notice we've outputted these strongly connected ponens in Reverse topological order so it's quite amazing we've done two runs of DFS one on the reverse graph and one on the original graph and what we found are the strongly connected components of the original graph and we output it these strongly connected components in topological order or to be precise in Reverse topological order so we can take any directed graph a general directed graph and with two runs of DFS we can find it's strongly connected components and we can structure these strongly connected components in topological order let's formalize this algorithm

GR1 Strongly Connected Components - 273 - SCC Algorithm Question
Finally, let's detail our strongly connected component algorithm. The input to our algorithm is a directed graph G in adjacency list representation. The first step of our algorithm is to construct the reverse of the graph G. Then we run DFS on the reverse graph. What do we know? We know that the vertex with highest post order number from this DFS run is guaranteed to lie in a source SCC of GR, and therefore is in a sink SCC of G. So, now what we want to do is we want to run explorer from this vertex with highest post order number from this DFS run. So now, what we're going to do is we're going to order the vertices. These are the vertices of the original graph, G. We're going to order these vertices by decreasing post order number. This is like we're ordering them by topological ordering, as for our DAG algorithm, and these are the post order numbers from this DFS run. So, we run DFS on the reverse graph and then we order the vertices in the graph by decreasing post order number from this DFS run. Now, finally, we run DFS on the original graph, where the vertices are ordered by this decreasing post order number from this DFS run. Now, what is the version of DFS we use for this last one? We're actually going to use the undirected connected components algorithm. If you recall, that runs DFS and it marks the components that we see along the way with a connected component number CCnum. We're going to run that identical pseudocode. Even though this is a directed graph and that was designed for undirected graphs, we run the identical pseudocode and the components, the strongly connected components, in this case, are going to be numbered and they're going to be numbered in topological ordering. So, the first component is going to be at the end of our topological ordering and so on, and we're going to work over. Now, just to remind you, this is our pseudocode for our undirected connected component algorithm. So, this was just DFS where we kept track of the connected component number and we marked the vertices with their connected component number as we visited them. So, for step four of this directed SCC algorithm, we're going to run this identical pseudocode and this CCnum, is going to give us the ordering, the topological ordering on the strongly connected components.

GR1 Strongly Connected Components - 275 - Simpler Claim
Here's a simpler claim once again. We have two components, two strongly connected components, S and S-prime. And there's some vertex in S, let's call it V. And there's a vertex in S-prime, let's call it W. And there's an edge from V to W. And the claim is that if we look at the maximum post-order number of all vertices lying in S compared to the maximum post-order number of all vertices lying in S-prime, then these maximum post-order number in S is bigger than the max post-order number in S-prime. So let's go ahead and prove that fact. Let's make one simple observation about this graph before we proceed. Notice we have an edge from this strongly connected component S to the strongly connected component S-prime. We know there's no cycles in this graph on strongly connected components, so there can't be a path from S-prime to S. Otherwise, these two strongly connected components would be a strongly connected component by itself. S union S-prime would be a strongly connected component and that would contradict the maximality of these SCCs. We know that there is no path from S-prime back to S because there's this edge that we assumed from V in S to W in S-prime. We're going to use this simple fact that there is no path from S-prime to S in the proof of this simpler claim. Run DFS on this graph and we're looking at the post-order numbers from this DFS run. Initially, all the vertices are unvisited. At some point some vertex in S Union S-prime must be visited. Let's take the vertex in S Union S-prime which is visited first and let's give it a name. Let's call it Z. We're going to have two cases. Either Z lies in S-prime or Z lies in S. Let's look at the first case. Let's say that Z lies in S-prime. Now, Z visits first so we're going to run Explore on Z. And who are we going to see? What vertices are we going to visit when we do Explore on Z? Well, which vertices are reachable from Z? Z lies in S-prime so from Z we can see all of S-prime but we can't see any of S. We see all of S-prime and we see none of S. All of S-prime is going to be visited and finished exploring, and none of S is going to be visited at all before we finish exploring from Z. Z and all of S-prime is going to be assigned post-order numbers before we even give a pre-order number for any vertex in S. The punchline is that all of the post-order numbers in S-prime. For every vertex in S-prime, its post-order number is strictly smaller than the post-order number of every vertex in S. Because we visit and finished exploring all the vertices in S-prime including Z before we even visit, before we even given a pre-order number to any vertex in S. And therefore, the maximum post-order number in an S-prime is strictly smaller than the minimum or the maximum post-order number in S which proves that claim. This completes the proof of the claim in this case where Z lies in S-prime. Now let's look at this case with a vertex Z lies in this component S. All these vertices in S and S-prime are unvisited. And now we visit Z and we start exploring from Z. Who do we reach? What vertices can we reach from Z? We can reach all of S because this is a strongly connected component, and we can reach all of S-prime because we can go from V over to S-prime, and then we can reach around S-prime. If we looked at the DFS tree, we're going to have this vertex Z. And what lies in that sub-tree? The rest of S Union S-prime lies in its sub-tree, in the DFS tree. Therefore, what do we know about Z? We know that Z is going to be the last vertex that we finish in S Union S-prime because it's the root of this sub-tree. We know that Z has the maximum post-order number of any vertex in S Union S-prime because it's the root of this sub-tree in the DFS tree. All the descendants have to be finished before we finished Z. And, therefore, since Z lies in S then we know that the vertex in S Union S-prime with maximum post-order number is Z which lies in S. So that proves the claim. The max post-order number in S, which corresponds to Z, is larger than anybody in S-prime. This completes the proof of this claim in this case where Z lies in S, and we also did the case where Z lies in S-prime. And that completes the proof of the simpler claim which also completes the proof of the key fact that the vertex with the highest post-order number lies in a source SCC. That completes the proof of correctness of our SCC algorithm and that's the end of our SCC algorithm description. It's quite an amazing fact. We can take any directed graph and we can compute its strongly connected components and we can topologically sort the strongly connected components. And we do it with just two runs of the vanilla DFS.

GR1 Strongly Connected Components - 276 - Proof of Key SCC Fact
Now, we took the following fact for granted in the design of our algorithm. The vertex with the highest post order number lies in a source SCC. Now, let's prove that fact so that we complete the proof of correctness of our algorithm. Let's look at the following simpler claim. Let's take a pair of strongly connected components S and S prime. Now, if some vertex in S has an edge to some vertex in S prime, then what can we say about the post order numbers for S versus S prime? What we show is that the maximum post order number in S is greater, strictly greater than the maximum post order number in S prime. Now, what does this simpler claim give us? Well, this simpler claim gives us a way to topologically sort the strongly connected components. How do we topologically sort them? We sort them by the maximum post order number in that component. So, for each strongly connected component, we're going to look at the max post order number of the vertices lying in that component. So we can think of the post order number for this component to be the max over the vertices in that component of the post order numbers. Now, we sort these strongly connected components by their post order numbers and we sort them in decreasing post order number. And this claim, this simpler claim, tells us that all edges will go from larger post order number to smaller post order number. So, all edges will go left to right in the ordering of these strongly connected components. Now, what do we know about the vertex with the highest post order number? Well, it's strongly connected component is going to have the maximum, the largest of these max post order numbers. So, its component is going to be at the beginning of this topological ordering and it's guaranteed to be a source SCC, since it's at the beginning of the topological ordering. So, therefore, the vertex with the highest post order number will be in the component which is at the beginning of the topological ordering and therefore it's a source SCC. So, if we prove this simpler claim, then this gives us a way of topologically sorting the components by the max post order number. And this implies that the vertex with the highest post order number lies in a source SCC. So, if we prove this simpler claim, it implies the fact that we used in our SCC algorithm. So all we need to prove is this simpler claim and then we're done.

GR1 Strongly Connected Components - 277 - BFSDijkstras
We've seen how to use the DFS algorithm to solve connectivity problems in undirected and directed graphs. Let's quickly remind you of some other common algorithms for exploring graphs. As opposed to DFS, which is depth first search, BFS is breadth first search. BFS explores the graph in layers. The input to the BFS algorithm is similar to the DFS algorithm. It's an undirected or directed graph G in adjacency list representation. But BFS has an additional input parameter. We specify a start vertex which we denote as little s. BFS returns the distance for every vertex from the start vertex little s. The graph G is unweighted, so the distance is defined as the minimum number edges to get from vertex s to vertex v. Now if there is no path from s to v, then this distance is defined as infinite. Now how do we get such a path of minimum length? Well, BFS also returns this previous array, which one can use to construct a path of minimum length from s to v. Now what's the running time of the BFS algorithm? BFS, like DFS, is linear time, so the running time is order n plus m, where n is the number of vertices in the graph G, and m is the number of edges in the graph G. Dijkstra's algorithm is a sort of more sophisticated version of BFS. It solves a similar problem as BFS, but instead, it considers a weighted version of the graph G. As in the BFS algorithm, the input to Dijkstra's algorithm is a graph G. It could be directed or undirected, and we have a specified start vertex, little s. But Dijkstra's algorithm has an additional input parameter. We were given a weight, a length for every edge, and this length has to be positive. What is the output of Dijkstra's algorithm? Well, it is the weighted analog of the BFS output, so it outputs this array dist and dist(v) is the length of the shortest path from s to v. Now one of the key requirements of Dijkstra's algorithm is that these edge lengths are positive. If you want to know how to deal with negative edge lengths, then you should refer to our dynamic programming lecture, DP3. Dijkstra's algorithm uses the BFS framework with the min-heap data structure. The min-heap data structure is often called the priority queue. Each operation in the min-heap data structure takes order log and time, so we get an additional log and factor on the BFS running time, and hence, the total runtime of Dijkstra's algorithm is order n plus m times logn. Now there are other variants of Dijkstra's algorithm with different data structures that they utilize. We'll always refer to the following the min-heap data structure in this class. And for concreteness, in this class we'll say the running time of Dijkstra's algorithm is order n plus m times logn. I assume that many of you have seen BFS and Dijkstra's many times in the past. If you need a quick review, I suggest you look at chapter four of the textbook.

GR2 2 Satisfiability - 278 - SAT Notation
Let's look now at an application of our strongly connected components algorithm. We're going to look at the satisfiability problem, also known as the SAT problem. This problem is going to play a central role later in our study of NP completeness. Let's start with some terminology. We're going to be looking at boolean formulas. We're going to have end variables, x1, x2 up to Xn. It's a boolean formula so these variables are going to take true or false. There are two times n literals. These refer to the positive and negative forms of the variables x1 and x1 bar, x2 and x2 complement and so on up to Xn and Xn complement. Our formulas are going to be composed of knots ands and ors. And we'll use the notation this symbol for the and, and this symbol for the or. Personally to help myself remember, I noticed that this one looks kind of like an A. Now we're going to look at formulas in CNF. This is conjunctive normal form. Let me tell you what that is. The formula would be composed of several clauses. Each clause is the or of several literals, for example, x3 or x5 bar or x1 bar or x2, that's a clause. In order to satisfy that clause, I have set x3 to be true x5 to be false, x1 to be false or x2 to be true. Finally to construct a formula in CNF, we're going to take the and of M clauses. Here's an example of a formula in CNF. It has four clauses, each clause is the or of some literals. Some are of size one. So, there's a unit clause. There's a clause of size two, clause of size four, clause of size two. And then we take the and of these clauses. So in order to satisfy this formula, we have to satisfy at least one literal in every clause. For example for this formula if I set x1 to be false, x2 to be true and x3 to be false, then it doesn't matter the setting for x4 and x5, I satisfy this formula. Plug in x1 to be false, that satisfies this clause, x1 to be false satisfies this clause as well, x2 to be true satisfies this clause, and x3 to be false satisfies this clause. So all the clauses are satisfied, so the end of these clauses is satisfied. So, the formula itself is satisfied. So this formula evaluates to true for this assignment of the true false to the variables. Now any formula can be converted into a CNF form, but the size of the formula may blow up. Now let's go ahead and precisely define the SAT problem.

GR2 2 Satisfiability - 279 - SAT Problem
The input to the SAT problem is a formula f in conjunctive normal form, which is composed of n variables; x1 through xn, and m clauses. The output is a satisfying assignment if one exists. This is an assignment of true or false to each of the variables so that the formula evaluates to true. Now if there is no satisfying assignment, no assignment where the formula evaluates true, then we output no.

GR2 2 Satisfiability - 280 - SAT Problem Quiz Question
Now here's an example of a SAT input. The formula F is (x1 bar or x2 bar or x3) and (x2 or x3) and (x3 bar or x1 bar) and (x3 bar.) To make sure you understand this problem, why don't you go ahead and specify the output for this input. Either give a satisfying assignment if one exists, or-

GR2 2 Satisfiability - 282 - k-SAT
A satisfying assignment for this formula is the assignment which sets X_1 to be false, X_2 to be true, X_3 to be false. X_1 to be false satisfies this clause, X_2 to be true satisfies this clause, X_3 to be false satisfies this clause and this clause. Now we're going to look at a restrictive form of the SAT problem called k-SAT. For example, for three-SAT, the input is going to be a formula in CNF with n variables and m clauses. But now, all the clauses are of size, at most, three, or k in general. Now, the size of a clause is the number of literals in it. So, this formula is an example of a three-SAT formula. Actually, it's an example of a k-SAT formula for all k at least three. Now, what we're going to see later is that SAT problem is NP-complete. And for every k at least three, we'll see that the k-SAT problem is NP-complete. So, three-SAT is NP-complete, four-SAT is NP-complete, and so on. What we're going to see now is a polynomial time algorithm for two-SAT. So, there's a very interesting dichotomy for two-SAT, that there's a polynomial time algorithm and we're going to solve it using our strongly connected components algorithm. And then, when k is at least three, so three-SAT is NP-complete. So let's dive into our algorithm for two-SAT.

GR2 2 Satisfiability - 283 - Simplifying Input
Consider input F for 2-SAT problem. Here's an example input for 2-SAT. This consists of 4 variables x1, x2, x3, x4 and it has five clauses. Now I want to simplify the input to our 2-SAT problem. In particular I want to remove unit clauses. What's a unit clause? This is an example of a unit clause right here. This is the clause with exactly one literal in it. Now how can I remove it? Well, in order to satisfy this formula, I know that I have to satisfy this clause. In order to satisfy this clause, there's only one way. I have to set x1 to be false. So, I might as well go ahead and set x1 to be false and the resulting formula will be true satisfiable if and only if the original formula was satisfiable. So, here's the basic procedure for eliminating unit clauses. Take any unit clause, in this case x1 bar, and let's call that literal AI. So in this case AI is x1 bar. Then I'm going to satisfy that literal. So I'm going to set AI to be true. In this case, I'm going to set x1 bar to be true. What does it mean to set x1 bar to be true? That means to set x1 to be false. If I set the variable x1 to be false, then I satisfy this literal. Now if I set x1 to be false, now any clause which contains x1 bar I can eliminate it because those clauses are satisfied. So I can eliminate this clause. There are no other clauses containing x1 bar, so I can't eliminate any other clauses in this example. Now what else can I do? What about this x1? Why no x1 is set to false? So this literal will not be satisfied. So I might as well remove it. So this clause now becomes just x4 by itself. So I drop all occurrences of AI bar, then this formula then reduces to x3 or x2 bar, that's from the first clause and x4, that's from the third clause, and x4 bar or x2 from the third fourth clause, and X3 bar or X4 from the last clause. The third clause got reduced and the second clause got eliminated. That's called the resulting formula F prime. So this simplified formula is F prime. The original formula is F. What is the claim? The claim is that F, the original formula is satisfiable if and only if this reduced formula F prime is satisfiable. Then what can I do? Well now I can take this simplified formula F prime, and again there's another unit clause which got formed and now I can set x4 to be true, and then I can simplify the formula and I can keep going. Eventually I either end up with the empty formula, which is satisfied, or I end up with a formula where all clauses are of size exactly two. And that's how I'm going to simplify my input for 2-SAT problem. So I can take an arbitrary input for the 2-SAT problem and I can simplify it so that all of the clauses are of size exactly two by repeating this procedure over and over. Now the key observation that we just mentioned is that the original formula F is satisfiable if and only if this reduced formula F prime is satisfiable. Now this is clearly true since the only way to satisfy F the original formula is to take this unit clause. I have to satisfy it and therefore I must satisfy this literal. And then steps two and three are forced. Now the implication of this procedure is that I can keep applying it over and over as long as I have unit clauses and eventually the formula is either satisfied or all clauses are of size exactly two. So now in order to design an algorithm, I can assume that the input to my 2-SAT problem all of the clauses in that input are of size exactly two. This is going to make the input to the problem slightly easier for us. Now let's go ahead and see how this 2-SAT problem relates to the strongly connected components algorithm.

GR2 2 Satisfiability - 284 - Graph of Implications
Let's take our input to the two set problem. This is the formula, F. And we can assume that all clauses are of size exactly two, as we just mentioned. And let's let N denote the number of variables in our formula. Let's call them X1 through XN, and let's say M is the number of clauses in our formula. We want to convert this logic problem into a graph problem. So what we are going to do is we're going to take this Boolean formula and we're going to create a directed graph. This graph is going to encode all of the information in this formula. Now first off, the vertices of this graph are going to correspond to the variables of this formula. There are end variables and there are two N literals, X1, X1 Bar, X2, X2 Bar, XN, XN Bar. So we're going to have two N vertices. One vertex for each literal, and then we're going to have two M edges in this directed graph. We're going to have two edges for each clause. Each clause has two implications, and the two edges are going to correspond to the two implications per clause. Let's look at a specific example and then we can see what we mean by implications per clause. Here's a sample formula with three variables and three clauses. Our graph is going to have six vertices corresponding to the six literals, X1, X1 Bar, X2, X2 Bar, X3, X3 Bar. And let's look at the first clause, X1 Bar or X2 Bar. Suppose that we set X1 to be true. So this first literal is not satisfied, then we better satisfy the second literal in order to satisfy the formula. So if X1 is set to true, then what does that mean about X2? Then X2 better be set to false in order to satisfy this formula. Similarly if X2 is set to true. So this literal is not satisfied, then we've got to satisfy the first literal. So we need to set X1 to be false. So to encode these implications, we're going to add in the edge from X1 to X 2 Bar. If this literal X1 is satisfied so X1 is set to true then we have to satisfy the literal X2 Bar. So we have an edge from X1 to X2 Bar. And similarly we have an edge from X2 to X1 Bar. Similarly, for this clause we have an edge from X2 Bar to X3 and from X3 Bar to X2. Finally, for the last clause we have an implication from X3 to X1 Bar and from X1 to X3 Bar. So this is our graph corresponding to this formula. And in general if we have a clause which has literals alpha and beta, so we have alpha or beta is the clause, then we're going to have the edges from Alpha Bar to beta because if we don't satisfy alpha then we have to satisfy beta and we are going to have the other edges from beta Bar to Alpha.

GR2 2 Satisfiability - 285 - Graph Properties
Now, here's our specific two-SAT example from earlier, and here's the graph corresponding to this Boolean formula. Let's take a look at this graph and explore some properties of it. Here's a particular path which is quite interesting. We go from X_1, we follow this edge to X_2-bar, to X_3, and then we can follow it to X_1-bar. So, we have this path which goes from X_1 and ends up at X_1-bar. Now this is a path of implications. Each edge is an implication. So, if we follow those implications out, so if we set X_1 to be true, we satisfy this literal, then we're going to have to satisfy this literal. How do we satisfy this literal? By setting X_1 to be false. So, if we set X_1 to be true, then in order to satisfy the formula, we have to set X_1 to be false. That's clearly a contradiction. So, there's no way we can satisfy the formula by setting X_1 to be true. Now, what happens if we set X_1 to be false? Well, there are no edges out of X_1-bar. So, there are no implications from setting X_1 to be false. So, it might be okay. We don't know. We have to proceed with the other variables. All we know is that, because there is a path from X_1 to X_1-bar and clearly we cannot set X_1 to be true. Now, what if there was a path from X_1 to X_1-bar and from X_1-bar to X_1? Then we know that we can't set X_1 to be true, because that would lead to a contradiction, and we can't set X_1 to be false, because that would lead to a contradiction. So, there's no valid setting of X_1. There's no setting of X_1, which leads to a satisfying assignment. Therefore, we can conclude that F is not satisfiable. Now, we saw that if there is a path from X_1 to X_1-bar, then we can't set X_1 to be true. We cannot satisfy the formula by setting X_1 to be true. Similarly, suppose there's a path from X_1-bar to X_1, then we know that we can't set X_1 to be false. So now, suppose that there is a path from X_1 to X_1-bar and also there's a path from X_1-bar to X_1, then what do we know about F? Well, we know we can't set X_1 to be true and we know that we can't set X_1 to be false. So in fact, there is no way to satisfy the formula F. What does this mean about the graph? If there is a path from X_1 to X_1-bar and from X_1-bar to X_1, that means that these two vertices are in the same strongly connected component. They're strongly connected with each other, so they're in the same SCC. So, if this literal and its negation are in the same SCC, then this formula is not satisfiable. And if this is true for any variable, the positive literal X_i and X_i-bar are in the same SCC, then the formula is not satisfiable. What we're going to see is that if this is not true. So, for every variable, the literal and its negation are in different SCCs. So, they never lie in the same SCC. Then, in fact, we're going to find a satisfying assignment. So, we're going to prove it satisfiable by finding a satisfying assignment.

GR2 2 Satisfiability - 286 - SCCs
Now let's formalize what we've just said. If for some i, xi the positive literal and the negative literal xi bar are both in the same SCC, so these two literals are strongly connected with each other, then the formula is not satisfiable. We just solved why this is true, because if we set xi to be true, then we get a contradiction because we have to set xi to be false. And if we set xi to be false, then we get a contradiction because that leads to xi being true. Now suppose this is not the case, for every i, xi and xi bar are in different SCCs. Now in this case f is satisfiable. So for every i, xi and xi bar are in different strongly connected components, then f is satisfiable. We'll prove this part by finding a satisfying assignment. So this statement that if xi and xi bar are in the same SCC then f is not satisfiable. We already discussed why that's true. For this one, where for every i, xi and xi bar are in different strongly connected components we're going to prove that f is satisfiable by finding a satisfying assignment. So we're going to construct an algorithm which is going to find a satisfying assignment and therefore we're going to prove that f is satisfiable by actually satisfying this f. Let's go ahead and construct the algorithm. So we're going to assume that for every i, xi and xi bar are in different strongly connected components. And we're going to use that fact to construct a satisfying assignment for f.

GR2 2 Satisfiability - 287 - Algorithm Idea
Now recall a strongly connected component algorithm. What was the main approach? We found a sink component. We marked those verses in the sink SCC and then we removed that sink SCC and recursed on the remainder graph. What are we going to do here? We're going to do a similar approach. We're going to find a sink SCC. We're going to do an assignment for that sink SCC and then we're going to rip it out and work on the remainder graph. So we're going to find a sink strongly connected component. Let's call it S. As an example, here's a sink SCC. It contains x3 bar and x1. They have edges coming in and no edge is going out. Now should we set these literals to be true or false? Well, we should set them to be true. Why? Well, let's consider this edge. Suppose the head of the edge is x2. Now if later in the algorithm we set x2 to be true, then this implication and says that we have to set x3 to be false. So we have to satisfy this literal. So if we want to rip out this component and not worry about it again, then we better satisfy all these literals which means setting x3 to be false in this case and x1 to be true in this case. So this is what we're going to do. We're going to go ahead and set this component to be true which means we're going to satisfy all the literals in S. So in this case we set x3 bar to be satisfied which means we set x3 to be false. And we want to satisfy x1, so we set x1 to be true. Now there are no outgoing edges. So there are no implications that we have to follow because of this setting. There are in-coming edges but we satisfied all of these literals, therefore any incoming edges, the later assignment which may imply this implication, we don't have to worry about because we've already satisfied the tail of this implication. So what can we do? We can rip out this component and work on the remainder of the graph. So we're going to remove this component and repeat the procedure on the remainder of the graph. But, there's one problem. What about the complement of this set? What about x3 and x1 bar? By satisfying this literal x3 bar, we've not satisfied x3 and we've not satisfied x1 bar. So maybe there are edges in to x3 and x1 bar and we have not satisfied these. And later we're going to have to follow this edge of implication and we're going to have to set x3 to be satisfied which means we're going to have a contradiction. We're going to have a problem. Now what would be great is if this complement set S bar is a source SCC. If this set S bar is a source SCC then what do we know? Then we know it has no incoming edges. Now it's safe to set S bar to false because there is no incoming edges. And since we set it to false, we don't have to worry about any implications coming out of this set. It's a source SCC so their edge is coming out. But we don't have to follow those implications because we only have to follow those implications if the head of the edge is set to true. But the head of edge is set to false so we don't have to follow the out edges and there are no in-coming edges. So it's safe to set it to false. So that's going to be the key. If we take a sink SCC, then that sink we want to set to true. But then we have to worry about its complement. The key is that the complement set is going to be a source. So what do we want to do for a source? We're going to set the source to be false.

GR2 2 Satisfiability - 288 - Algorithm Idea 2
Now, the previous idea we just discussed takes a sink SCC, and it satisfies that sink component. And then the issues come in by worrying about the complement of that set. Let's look at the reverse idea. Instead of taking a sink strongly connected component, let's take a source SCC. Let's call it S prime. So we have the source component containing X4 and X2bar. Now, it's a source so it has no edges coming in, it only has edges coming out. Now, for sink component we wanted to set it to true. What do we want to do for the source component? We want to set it to false. So by setting S prime to false, we're going to not specify each of these literals. That means X4 is not going to be satisfied so X4 is set to false. X2bar is not satisfied. That means that X2 is set to true. Now, these literals are not satisfied but there are no incoming edges so there are no later implications which are going to cause problems. Moreover, because these literals are not satisfied we don't have to worry about the implications coming out of these literals, so we don't have to worry about these edges coming out. We can safely remove this component and work on the remainder of the graph. But once again, what happens to the complement of this set? In this case, X4bar and X2? Well, it turns out that these are going to be in a strongly connected component, and this is going to be a sink SCC. So there might be edges coming in but there are no edges coming out. Now, if we set as prime to be false, that means we set its compliment to be true. So we satisfied all of these literals. Now, if we satisfied these literals that means later implications are going to be satisfied, and there are no edges coming out so we don't have to follow any implications out. Now, it turns out that these two approaches are the same. That's why it works. We can take a sink SCC and set it to true. And at the same time, we can take a source SCC and set it to false. These are compliments of each other. If the component S is sink SCC then its complement set is a source SCC. This is what we're going to do, we're going to find a source SCC S prime, or we're going to find a sink SCC S prime bar. These are compliments of each other. If this is a source then this is a sink and vice versa. And we're going to set this source, vertices, to be false. So these literals are going to be unsatisfied and in the sink we're going to set them all to be true. We're going to satisfy all the literals. If we do one, then we do the other. And once we do this, then we can remove all these literals which appear in S prime and S prime bar. Now, for each variable which appears in one of these, this is going to remove the positive and negative literal. If the positive literal appears here, then the negative appears here. And if the negative appears here, then the positive appears here. We remove both the positive and negative literal, so that variable goes away. We can simplify the formula, simplify the graph, and we can repeat.

GR2 2 Satisfiability - 289 - 2SAT Algorithm
Now the key fact that we just discussed and we haven't proven but we're going to use it for the algorithm is that if for all i, Xi, and Xi bar are in different strongly connected components, then we have the important property that if a set of vertices S is a sink SCC, then its compliment is a source SCC and vice versa. If S bar is a source SCC, then S is a sink SCC. So let's take this fact for granted and let's use it to design an algorithm for 2SAT, and then we'll go back and we'll prove this key fact. Here's our 2SAT algorithm for the input formula f. First off, we're going to simplify f,so that we eliminate all unit clauses. So now, we'll assume that f has all clauses of size exactly 2. Then, what we do is we construct the graph of implications corresponding to f. Now, we run a strongly connected component algorithm on this graph, G. So we have this strongly connected components of G and we have the strongly connected components in topological order. So we're going to take the last component. That's going to be a sink component. Let's call it S. What are we going to do? We're going to set this component to be true. So we're going to satisfy all the literals in this component. If we satisfy f, then that means S bar is unsatisfied. So what are we doing? We're taking this sink component and setting it to true. Meanwhile, its complements set is a source component. So we're taking that source component and setting it to false at the same time. Now, we remove this sink component and this source component and we repeat this procedure. We find our sync component in the resulting graph and we keep going until we're left with the empty graph, and then we satisfy the formula. This completes the algorithm description and it's easy to see that the main step in the algorithm in the running time is constructing the strongly connecting components, which takes linear time. So the whole algorithm takes order n+m time. It's linear. It remains just to prove this key fact, and then we're done with our algorithm description.

GR2 2 Satisfiability - 290 - Proof of Key Fact
It remains to prove this key fact that if S is a sink component then S bar, its complement, is a source component and vice versa. If S bar is a source then S is a sink. In order to prove this key fact, we're going to look at a simpler claim. Here's the claim. If we have a pair of literals alpha and beta and if there is a path from alpha to beta, then there's a path from beta bar to alpha bar and if there's a path from beta bar to alpha bar, then there's a path from alpha to beta. Now this claim looks much easier to prove. Let's assume the claim for now and let's go back and prove this key fact. Let's prove that if S is a sink then S bar is a source and vice versa. So take a sink component S and we're going to prove that S bar is a source component. Now take a pair of vertices in this sinc component S. So these vertices correspond to literals alpha and beta. Let's say alpha and beta R and S. So what do we know? We know that alpha and beta are strongly connected to each other because they're in the same strongly connected component. If they're strongly connected with each other that means we have paths between the pair. So we have a path from alpha to beta and a path from beta to alpha. That's the definition of being strongly connected. Now what does our claim imply? Well if there is a path from alpha to beta which is what we know, then there is a path from beta bar to alpha bar and this path from beta to alpha again applying the claim then that means that we have a path from alpha bar to beta bar. So what does that mean? That means that alpha bar and beta bar are strongly connected with each other, so they lie in the same SCC. So if alpha and beta are in S then we know that alpha bar and beta bar are in S bar in SCC. So what we've shown is for pairs of vertices in S, then their complements form an SCC. For alpha and beta in S, alpha bar and beta bar must be in the same SCC. So if all pairs over here are strongly connected, then their complements or pairs are strongly connected. So, these guys form SCC their complement forms an SCC. Now this is part of what we wanted to prove. We wanted to prove that if S is a sink SCC then S bar is a source SCC. What have we shown so far? We've shown that if S is a SCC then as S bar is an SCC but we haven't shown anything about a sink implies a source and vice versa. But we're halfway there. We've shown that if S is an SCC, then its complement is an SCC and vice versa. You can do the same proof in reverse. And now if S bar is in SCC then its complement is also in SCC. Now let's prove the last part about sink implies source and vice versa.

GR2 2 Satisfiability - 291 - Rest of Proof
Now let's finish off the rest of the proof of this key fact. What have we shown so far? We've shown that if we take a sink SCC S, then its complement is a SCC. Now we have to show that S bar is also a source SCC. So we have to show that if this is a sink, then this is a source. Now, since S is a sink SCC, if we take a vertex, a literal in S, then we know that alpha has no outgoing edges. So there's no edges of the form alpha to beta for any beta. Now, if there are no edges from alpha to beta, then what does the claim imply? That implies that there are no edges into alpha bar. So if there are no outgoing edges from alpha, then there are no incoming edges to alpha bar. So if for all alpha in S there are no incoming edges to the complement of alpha, then that means that the complement of S bar has no incoming edges. So that means that as S bar is a source SCC. So we've shown that if S is a sink SCC, then by this claim, S bar must be a source SCC. And we can do the argument in reverse and show that if S bar is a source, then S is a sink SCC. So we've shown that if S is a sink SCC, then S bar is a source SCC. And that proves the key fact. Now it just remains to prove this claim.

GR2 2 Satisfiability - 292 - Proof of Claim
Now let's prove this claim that, if there is a path from alpha to beta then there's a path from beta bar to alpha bar, and vice versa. So let's take a path from alpha to beta and let's construct a path from beta bar to alpha bar. Let's give some notation for the vertices along this path from alpha to beta. So let's say this path from alpha to beta goes along gamma0 to gamma1 and so on up to gammal. So it starts at alpha, so gamma0 is alpha and gammal is beta. Now let's take one of these edges. Let's look at the edge from gamma1 to gamma2. How do we get an edge from gamma1 to gamma2? Well this edge from gamma1 to gamma2 comes from this clause, gamma1 bar or gamma 2. Now every clause has two edges which it implies. This is one of the edges. What's the other edge? The other edge we get, the other implication is gamma2 bar implies gamma1 bar. Similarly, if we look at this edge gamma0 to gamma1, we're going to see that there's also an edge from gamma1 bar to gamma0 bar. And if we look along this part of the path, we're going to see that we get a path from gammal bar all the way over to gamma0 bar. What is gammal bar? This is beta bar. And what is gamma0 bar? It's alpha bar. So what we've shown is that if there is a path from alpha to beta, then there must be a path from beta bar to alpha bar, and that's what we wanted to prove. We wanted to prove that if there is a path from alpha to beta, then we can show that there is a path from beta bar to alpha bar. And then we can apply the same argument in reverse, and we can show that if there is a path from beta bar to alpha bar, and there's a path from alpha to beta. That proves the claim, and that completes the proof of correctness of our algorithm. So we've completed the argument for the linear time algorithm for two set.

GR3 Minimum Spanning Tree - 293 - Greedy Approach
We're going to look now at the greedy approach for optimization problems. So we're going to take a locally optimal move. In particular, we have a partial solution and we're going to take the optimal move for the next step and the question is, when does this approach lead to the global optimum? Now, we saw earlier for the knapsack problem, that this greedy approach doesn't work. There are examples where the greedy does not lead to the optimal solution. Nevertheless, we were able to use dynamic programming to find the optimal solution for the knapsack problem. What we're going to work on now, is the minimum spanning tree problem and we're going to see that for this problem, that the greedy approach does work, as a particular algorithm that we're going to use for the minimum spanning tree problem is the Kruskal's algorithm. Many of you may have seen Kruskal's algorithm before, but the important thing that we're going to stress in this lecture, is the proof of correctness of Kruskal's algorithm for the minimum spanning tree problem. Why exactly does the greedy approach work for this problem? We're going to see a general lemma known as the cut property. This lemma is going to imply that Kruskal's algorithm works correctly for the minimum spanning tree problem. And it's important that we understand the statement of the cut property and also the proof has some useful ideas that I want you to understand. And we're going to see is a nice byproduct of this general lemma. We're also going to see that Prim's algorithm for the minimum spanning tree problem also works correctly. So, this general lemma implies that Kruskal's algorithm works correctly and Prim's algorithm works correctly. Let me remind you of the precise formulation of the minimum spanning tree problem and then we'll look at the general formulation of the cut property, its statement and its proof, and then we'll see that it immediately implies that Kruskal's algorithm works correctly and Prim's algorithm work.

GR3 Minimum Spanning Tree - 294 - MST Problem
The input to the minimum spanning tree problem, also known as the MST problem, is an undirected graph, G, and each edge, E, has a weight w(e). Our goal is to find the subgraph of minimal size which is still connected. Now the connected subgraph of minimal size is of course a tree. We call it a spanning tree to stress the fact that it's connected, that it's a tree and not a forest. And we want to find such a subgraph of minimum weight. In summary, our goal is to find the minimum weight spanning tree of G. And just to remind you what do we mean by minimum weight, well, for a particular tree, T, its weight is the sum of the weights of the edges in that tree.

GR3 Minimum Spanning Tree - 295 - Tree Properties
Before we dive into the algorithm and the proof of correctness, let's review some basic graph theory about properties of a tree. Now, I would remind you, a tree is a connected acyclic graph. Here's an example of a graph on 17 vertices with a tree denoted by the blue edges. The first basic property is that a tree on n vertices has exactly n - 1 edges. In this example, it has 17 vertices and there are 16 edges. To see why this property holds, you'll notice that you need a least n - 1 edges to connect up the n vertices. And if you have more than n - 1 edges, then you'll have at least one cycle. The next property about a tree is that, in a tree, there is exactly one path between every pair of vertices. Clearly, if there are zero paths between a pair of vertices, then that graph, that tree is not connected. And also, if there are two or more paths between a pair of vertices, then we have a cycle in this graph. So, it's not acyclic. So, in order to be connected and acyclic, there has to be exactly one path between every pair of vertices. Now, the final property is the one that we're going to use in our proof of the cut property. We're going to construct a sub-graph and we want to establish that it's a tree. How are we going to prove there's a tree? Well, we're going to show that it's connected and that the number of edges is exactly n - 1. Any connected graph with exactly n - 1 edges is a tree. Now, this third property follows from combining the first property and the second property together and apply the third property. This last property is the one that we're going to utilize later. So, let me repeat it again. In order to establish that a graph is a tree, I just have to show you that the number of edges is exactly n - 1 and that this graph is connected.

GR3 Minimum Spanning Tree - 296 - Greedy Approach for MST
Now, here's a particular graph. Let's put some weights on these edges and then let's see how we would approach the MST problem using a greedy approach. Now, let's give the edges of this cycle the smallest weight, so we can focus on the behavior on this cycle. For the rest of the edges, I don't want to worry about so much. So, let me give you a bunch of edges weight seven, and I'll give a bunch of edges weight nine, and the remaining edges I'll give weight 12. Now, most of the weights don't matter for this example. I want to focus on this particular cycle. So, really, the first six weights are the only ones that matter. Now, let's look at how I would approach the MST problem from the greedy perspective. In the greedy approach, the first edge I would consider is the edge of weight one. I would start off with the empty graph. So, my current MST is empty and I'm going to build up the MST one edge at a time. I'll consider the edges by increasing weight. So, I'll start with the first edge of lowest weight, which is this edge of weight one. Now, I'm going to add it in if I can. Well, currently I have the empty graph as my current MST. And then, when I consider this edge of weight one, can I add it in? Yes, of course I can add it in because it doesn't create any cycles. So, let's add it in. Then I consider the edge of weight two. Can add that in to my current MST? Yes, I can add it in because it doesn't create any cycles. Similarly, for the edge of weight three, and edge of weight four, edge of weight five. Finally I consider the edge of weight six. Can I add that into my current MST? No, I cannot add this edge into my current MST because it creates a cycle. After that, I'll consider the edges of weight seven, in some order. But both of these edges can get added in. And similarly, these other three edges of weight seven can also get added in. Now, let me consider these other edges of weight nine. This edge I can add and without creating a cycle. Similarly this edge. Finally, when I consider this last I edge of weight nine. Can I add it in without creating a cycle? We'll say the end points of this edge are V and W. Notice that V and W are already connected. They lie in the same component. See, I have this path from V to W. So, since V and W lie on the same components, they are already connected. So, if I add an edge between them that creates another path and therefore I have a cycle which contains this edge. So, I don't add in this edge. And similarly any of the edges of weight 12 I don't add in. And this is in fact an MST of this graph.

GR3 Minimum Spanning Tree - 297 - Kruskals Algorithm
If we formalize the gritty approach that we just outlined on the previous slide, this gives us Kruskal's algorithm for the MST problem. So the input to Kruskal's algorithm is an undirected graph with weights on edges. Now we want to consider these edges of this graph in increasing order of weight. We want to consider the lowest weight edge first, and the highest weight edge last. So our first step of our algorithm is to sort the edges by increasing weight. To do this, we can use something like merge sort. We're going to use a set X to keep track of the set of edges that we've inserted so far into our MST. So we initialize X to the empty set. Now we're going to go through the edges one by one. And we are going to go through these edges in order. The order is a sorted order by increasing weight. So the first edge we consider, is the one of the lowest weight and the last edge we consider, is one of the edges of highest weight. Now for a particular edge between V and W, when do we add this edge E into our current tree? Will we add it in if it doesn't create a cycle? If adding this edge doesn't create a cycle, then we add this edge into our current tree X. Finally, you return X. This will be an MST at the end of the algorithm.

GR3 Minimum Spanning Tree - 298 - Kruskals Analysis
Now let's take a look at the running time of Kruskal's algorithm and then we'll look at the proof correctness of Kruskal's algorithm. How long does it take to sort the edges by increasing weight? This takes o(m log n) time. M is the number of edges and N is number of vertices. Note of course (m log n) is the same as (m log m). Now how long does it take to do this step? How long does it take to check whether adding edge E into X creates a cycle or not? What exactly do we want to check to see whether it creates a cycle or not? Look at the sub graph on this set of edges X. Now in this sub graph, let C of V be the component containing V, and C of W is a component containing W. Now to see whether the edge E creates a cycle when we add it into this subgraph, we want to check whether there's already a path between V and W. If there already is a path between V and W, then the components containing V and W are the same, V and W are in the same component. So what this step does is, it checks whether the component containing V and the component containing W are different. So if V and W are in different components, then we add the edge E into X. Now how do we check the component containing V and the component containing W? Well, we use this data structure, the simple data structure known as union-find data structure, using this data structure it takes O(log N) time per operation. So it takes O(log N) time to check the component containing V and the component containing W and then we can see whether they're the same or different components. And then, once we add this edge E into X then we can update the component containing V and the component containing W. We can merge those two components in O(log N) time. So the union-find data structure takes O(log N) time for each check operation, in order to check whether the component containing V and the components C and W are the same or not. And it takes the O(log N) time to do a merge operation, where we merged the component contained V and the component containing W because we added edge E into X. Now I'm going to skip the details of the union-find data structure, since many of you may have seen in the data structures class before and if you want, if you haven't seen it before then you can review it in the textbook. But it's a very simple data structure using this notion of rooted directed trees, and once we have this data structure which has O(log N) per operation, then the key fact is that we're doing at most M operations and then since there are order M operations each one taking O(log N) time, then the total run time for the step three is o(m log n) again. So step 1 and step 3 both take o(m log n) time, so the total run time is o(m log n) for the whole algorithm. Now what I want to focus on in this lecture is not the data structure but the proof of correctness of this algorithm. Why does the greedy approach work for this problem?

GR3 Minimum Spanning Tree - 299 - Kruskals Correctness
Now, let's get the idea for the proof of correctness of Kruskal's algorithm by looking at our earlier example. So, recall what Kruskal's algorithm is going to do. It's going to consider these edges by increasing weight. So, we're going to add in these first five edges. The sixth edge we're going to skip because it creates a cycle. Then we add in these five edges of weight seven. And then finally we're going to consider one of these edges of weight nine. Now, which edge of weight nine do we consider first? Well, that's an arbitrary choice. Let's suppose we consider this edge of weight nine first. Let's mark the end points as V and W. Now in general how are we going to prove that this algorithm is correct? Well, we're going to do it by induction. So, let's suppose that the algorithm is correct so far, so that the current X is correct so far. What does that mean to be correct so far? That means that these edges are part of an MST. So, there is MST T and X is a sub-graph of T. So, we're on our way to an MST to a solution to this problem. So, we're assuming that these red edges, which I've now highlighted as green edges, are part of an MST. Now let's suppose that we add this edge that we're considering right now to this sub-graph X. If we don't at this edge to the sub-graph X, then X is still part of MST. But, if we add this edge E to this sub-graph X, then we want to make sure that X union E is part of MST. And that's our claim that we're going to prove. We're going to prove that X union E is part of MST. T prime, T prime might be different than T. So, we might be heading in the direction of a different a MST than we were headed for before, but we're still on our way to an MST. Now, what is the key property that ensures that adding E to X ensures that we're still on our way to an MST. Well, when are we adding this edge E to X? When we look at the component containing V, which are these six vertices and we look at the component containing W, which are these two vertices. These are different components. Therefore, we can add this edge and it doesn't create any cycles. Now, the key property is that if we consider one of these components, let's say the component containing V and we let S be the set of vertices and the component containing V. An S_bar is the complement of this set. So that's the rest of these vertices. Then the key property is that our current subset X, has no edges which go from S to S_bar. If there was an edge that goes from S to S_bar, then that vertex in S_bar which is connected to S would be added into S. So, it would be in the component containing V. This component is a maximal set. So, we know that nobody in S_bar is connected to anybody in S. Therefore, there are no edges of X going from S to the rest of the graph. That's the definition of a component. So X has no edges that go between S and S_bar. So, X doesn't cross between S and the rest of the graph, S_bar. But this edge E does cross between S and S_bar, because V is in S and W is not in S. Now, what is the key property about E? E is a minimum weight edge crossing from S to S_bar. Now, there are several edges of minimum weight crossing between S and S_bar, two in particular. We can take any of those minimum weight edges. For any minimum weight edge crossing between S and S_bar. If we add E to X, then X union E will be part of an MST. That's what we're going to prove. And we're going to prove in general that if we take a subset of edges, which are part of an MST and we take an edge which crosses from S to S_bar of minimum weight and no other edges of this current subset cross from S to S_bar. So, this minimum weight edge crossing from S to S_bar. Then we can add this edge into our current subset and that will still be part of MST. This general claim will be known as the cut property. Before we go ahead and formalize the cut property statement, let me formalize what I mean by a cut. This partition of vertices into S and S_bar.

GR3 Minimum Spanning Tree - 300 - Cuts
Before we give the statement of the cut property, let's do a little bit of terminology. Here again is our running example of a graph. In general, we have an undirected graph, G and a cut of the graph is a set of edges which partition the vertices into two sets, S and the complement of S. So for a particular set S, the cut edges are those edges with one endpoint in S and the other endpoint in S bar. In words, what are the cut edges? The cut edges are the edges crossing between S and S bar. Let's take a look at a particular cut in this graph. So I partitioned it into S and S bar. Now what are the cut edges? The cut edges are the five edges which are crossing between S and S bar. These five edges are the cut S, S bar. Later in the course, we're going to look at various optimization problems involving cuts. We're going to try to find a minimum cut. So the fewest number of edges in order to disconnect the graph into at least two components. And we're also going to look at the max cut problem where we're trying to find the cut of largest size. But for now we're just looking at the general notion of what is a cut. So to summarize, if I give you a set S and S bar, then the cut S, S bar is a set of edges crossing between S and S bar.

GR3 Minimum Spanning Tree - 301 - Cut Property
Finally state the structural Lemma known as the cut property. This is a Lemma from which we'll derive the correctness of our MST algorithms. So the Lemma considers an undirected graph G. So let's consider the graph that we've been having as a running example. Now we're going to take a subset of edges. I've marked a particular subset of edges in this graph by the green edges here. Now we're going to assume that this subset of edges is part of an MST T. Now you should think of this set X as, you have an algorithm for the MST problem, and you have a partial solution that you're building up edge by edge. X is your partial solution so far. And you assume by induction that this partial solution is correct so far. So to be correct so far, that means that this subset X that you've constructed so far, is a subset of MST T. Now you don't know what T is. But you know that your solution is correct so far. So it is a subset of some MST T So for this particular example I have the green edges, which are marked here. And these are what my algorithm has constructed so far. And we're assuming that our partial solution X, the green edges, are part of an MST T, which I marked by red edges here. So the red edges are an MST T, and we don't know this MST. But we have the green edges. And we just know by induction that the green edges are part of an MST. So we have a partial solution, which is correct so far. And our algorithm is working edge by edge. So we're going to consider the next edge that we're going to add into X. And we want to prove correctness of this next edge being added into X. Now what are we going to assume about the edge that we're trying to add into X? We're going to assume that it crosses a cut of this graph. So in particular, we're going to take a subset S, and we're going to look at the cut between S and S bar. So here's a particular set S, and here's a compliment of S. Our assumption is that no edge of our partial solution so far X, crosses this cut S, S bar. So notice that no green edge crosses between S and S bar. I notice there are many sets S, where this is true. But we get to choose any set S where no green edge crosses between S and S Bar. Now there are no green edges crossing between S and S Bar. But there is at least one edge of the graph G which crosses from S to S bar, because it's a connected graph. So we're going to look at all edges of the graph G, which cross the cut S S bar. So I've marked in pink or purple, the five edges of the graph G, which cross between S and S bar. Now of these five edges, I want to take a minimum weight edge. So let E star be a minimum weight edge. Now there might be multiple edges of minimum weight. It doesn't matter. You get to choose which one you want. So let E star be any minimum weight edge across this cut. So let's mark this particular edge as E star. So we're assuming that these four other edges crossing the cut, have weight which is at least the weight of E star. Now finally what's our conclusion? Our conclusion is that we can add E star to our current MST construction, and we'll still be on our way to an MST. So if we look at our partial solution X, and we add E star to it. So we look at X unit E star. Then this is a subset of T prime, where T prime is an MST. So if our partial solution X was on its way to an MST T, we didn't know T, but we just knew that it was correct so far. Then if we add an edge, which is the minimum weight edge across the cut, and no edge of the partial solution crosses this cut so far. Then if we add this edge E star, to our partial solution, then we'll still be on our way to an MST T prime. Now this MST that we're going towards now, it might be different than the previous MST. But our goal is just to find a MST. We're not looking for a particular MST. So if we were on our way to an MST before, so our partial solution was correct so far, then we'll be on our way to an MST by adding this edge E star in.

GR3 Minimum Spanning Tree - 302 - Cut Property Kruskals
Now how did this apply for Kruskal's algorithm? For Kruskal's algorithm we said this edge e star was between vertices v and w. We added this edge e star, if the component containing v, one of the endpoints was not the same as a component containing w. So v and w were not connected in the current subgraph. So what was the set s in this example? To set s in this example was the component containing v or the component containing w. Let's just say that s was a component containing v and therefore since the component is a maximal set of connective vertices we know that there is no edge in the partial solution which crosses from s to s bar. And then since we consider the edges in increasing order, in sorted order, then we know that e-star must be the minimum weight edge crossing from this component to outside. So this edge e star or that Kruskal's algorithm considers is satisfies the hypothesis of the Lemma. So if we add an e star to our partial solution we'll still be on our way to an MST. So this proves correctness of Kruskal's algorithm. So Kruskal's algorithm uses a particular type of set s, but this is true in general for any cut s. Now it's important to understand the statement of the cut property and also to understand the proof. The statement, the main idea is that any edge which is minimum weight across a cut is going to be part of some MST. Why? Because if you give me a tree T which does not contain this minimum weight edge across the cut then I can add this edge into the current tree and I can construct a new tree which is of smaller weight. Actually, to be precise, I'm not necessarily going to construct a new tree with a smaller weight, but if you give me a tree T which does not contain this edge e star, or construct a new tree, T prime where the weight of T prime is at most the weight of T. So the weight doesn't increase. And that's going to be the intuition for the proof. We're going to take this tree T, we don't know this tree T but we know there exists an MST T which contains X, our partial solution so far. And now we're going to add this edge e star into X. If this MST T contains X unit e star then we're all done. Now there's no reason why this edge e star has to be in this tree T. So what happens if edge e star is not in the tree T? Well then we're going to construct a new tree, T Prime, where the weight of T prime is at most the weight of T, and in this tree T prime will be the edge e star and also X, and we'll construct this tree T prime by modifying the tree T slightly by adding in the edge e star and then removing an edge, in order to construct a new tree. So let's go ahead and dive into the proof of the cut property.

GR3 Minimum Spanning Tree - 303 - Proof Outline
Now let's outline the proof of the code property. Let's first fix a graph G, here's our running example of a graph. Let's fix the subset of edges x which are past solutions so far. These are the green edges here. Our assumption is that x is a subset of T, where T is an MST. Let's mark this tree T by the red edges. Now we don't know these red edges, we don't know this MSTT but we know there exists such a tree T. So even though our algorithm doesn't know this tree T, we can use it in the proof because we are assuming the existence of this tree T. Now in addition to this tree T, I want to fix the subset of vertices which form the cut which no edge of x crosses. So I want to fix this subset S. In this example, this is x and this is s bar. Now our assumption was that none of the green edges, none of the edges of x cross between s and s bar. Now here are the green edges once again. And notice none of these green edges cross between s and s bar. Now there are edges of T which cross between s and s bar. Now that has to happen because T as an MST, is connected. So it has to connect s to s bar. But we're just asking that no edge of the partial solution crosses between s and s bar. So no edge of x crosses s to s bar. No edge of x is part of the cut between s and s bar. Now we're going to take a particular edge e star which crosses this cut. Which edge are we allowed to choose? We're allowed to choose any edge of minimum weight across this cut. So there are four other edges crossing this cut, and our assumption is that the weight of e star is at most the weight of any of these other edges. Now they may be equal, but no other edge crossing this cut has strictly smaller weight than e star. Now finally, what's our goal in this proof? Our goal is to construct a tree T prime, where T prime is a minimal spanning tree and where x union e star are all part of T prime. So x union e star are a subset of T prime. Now how can we possibly construct this T prime? Well we have this tree T. T is an M S T and T contains the partial solution x. Now the problem is that this tree T might not contain this edge e star. For example, in this case e star is not in one of the red edges. So what do we do? Well we try to modify this tree T to construct a new tree, T prime, and we'll show that this new tree T prime is a minimal spanning tree. So the weight of T prime is at most the weight of T, actually the weights must be equal, and this new tree T prime is modified a little bit from T so that it contains e star in addition to containing the partial solution x.

GR3 Minimum Spanning Tree - 304 - Constructing T
Once again, our goal is to construct this MST T′, which contains X and e*. Now, know there is an MST T which contains X. Now, there are two cases to consider either this edge e* is a part of T, or is not part of T. Now, the easy case is suppose that e* happens to be part of T then in this case, what is T′? T′ is the same as T. There's nothing to do in this case. Notice we have that X is part of T. And we're supposing that e* happens to be part of T as well. What's our goal? Well, our goal is to show a tree, T′ which is an MST while T is an MST. If we said T′ to be T then we have that T′ is an MST. And we know that X is part of this T or T′. And also e* is part of it. So we have that XUe* is part of T′. So there's nothing to prove in this case. Done. The hard case is, what if e* is not part of T? In this particular, example this e* is not part of this particular MST T. So what do we do in this case? Well, we have to modify T in order to add edge e* entity and construct a new MST T′. How do we construct this tree T′ in this case? Well, let's take this tree T which contains X, and let's add in the edge e* to it. Let's look at TUe*. We're taking the red edges in this graph and we're adding in this edge e*. Let's say that the endpoints of e* are A and B. Now, T is a tree. What happens when we add an edge to this tree? Well, it creates a cycle. It's going to be a cycle which contains this edge. Now, this tree contains another path between A and B. The path between A and B in the tree T goes along here, along the blue edges. And then when we add in this edge e*, we get a cycle containing edge e*. Let's call this cycle C. Now, I want to drop an edge from this cycle in order to make a tree T′. We'll show that we can drop any edge from this cycle and we'll have a tree T′. But we want it to be a minimum spanning tree so we want the weight of the T′ to be minimum. How do we show there's a minimum weight spanning tree? Well, we know that the weight of T is minimum. If we show that the weight of T′ is in most the weight of T then T′ is also minimum weight. We need that to the edge that we drop has weight at least that of e*. Which of these blue edges has weight which is at least that of e*? Well, what was special about e*? e* was chosen to be the minimum weight edge across this cut S to Sbar. In this particular example, there are three other edges, three other blue edges crossing this cut S to Sbar. So what do we know about these three blue edges crossing this cut compared to e*? We know that the weight of e2, e3, and e4 is at least that of e*. So we can drop any of these three. We take any of these three edges, let's call it e′, which is in the tree T and which crosses S S-bar. Now, in this example once again there are three edges crossing this cut S S-bar which are in this tree T. Now, in general, why do we know that there is an edge e′, which is in the tree T which crosses S S-bar where this edge e* has one end point in S and one end point in S-bar. Now, this tree T it's a tree once again so that means it's connected. That means there's got to be at least one path between A and B. Now, that path has to cross from S to S-bar at some point. In this example, it crosses multiple times, but it's got to cross at least one time from one side to the other side. Now, when it crosses from one side to the other side, it has to have an edge, which crosses from one side to the other side. So take one of those edges, which crosses from one side to the other side. Call that e′. And finally, we have our tree T′. We're going to set T′ to be T, which is MST, which contains the partial solution X. We're going to add in this edge e*, which is the edge we're trying to add into our partial solution. And then we're going to subtract, we're going to take away any edge which crosses from S to S-bar, which is part of this tree T. And we know there exists at least one such as e′ take any such edge e′ and take it out of the tree T, add in the edge e*, and this gives us our tree T′. What remains? Well, we've constructed this T′ and now we have to prove that T′ is in fact a tree and that is a minimum weight. Therefore, it's a minimum spanning tree.

GR3 Minimum Spanning Tree - 305 - T is a Tree
Now let's first prove that T prime is a tree. How do we construct T prime once again? We took the tree T and we added it in this edge e* that created a cycle. Now, along that cycle we removed any edge of that cycle. In this case we removed, let's say, this edge and let's call it e prime. What we're going to show now is that we can remove any edge of this cycle and this result in graph T prime will be a tree. Now, how are we going to show that the prime is a tree? Well, we're going to show that T prime is connected and it has exactly n-1 edges. Now, if you recall from the beginning of the lecture, we said that if a sub graph has exactly n-1 edges and is connected then it must be a tree. Now the fact that it has exactly n-1 edges is obvious. Why? Because T is a tree so it has exactly n-1 edges, we added one edge in and removed one edge. So we still have exactly n-1 edges. So what remains? We just have to show that it's connected. So take any pair of vertices y and z and let's show that there's a path between y and z in this sub graph T prime. I chose a particular y and z in this example. So y is over here and z is over here. Now noticing T, there is a path between y and z of course because T is connected. The path goes along this edge e prime. Now what happens in T prime? When T prime, I've removed this edge e prime. So this old path between y and z no longer exists in T prime. So let's let P denote the path from y to z in T in the original tree T. And now we have to construct a path in T prime which goes from y to z. Now, recall that when we looked at TUe* what do we get? We got a cycle, these blue edges are the cycle in TUe*. So let C denote the cycle in TUe*. Let's denote the end points of e prime as c and d. Now e prime is a path from c to d in T. Now this cycle C has two paths between c and d, e prime is one of those paths, let's take the other path. So let's look at this cycle C and take out the edge e prime, what do we have? We have a path and this path goes from c to d and all these edges in this path, exist in T prime because T prime contains all these edges except for e prime but we're not using e prime, we took e prime out. So we took the cycle, dropped off the edge e prime and this gives us a path from c to d in T prime. So, how do we get from y to z in T prime? Well, we're going to follow the original path P for whenever we hit this edge e prime, we replace this edge e prime by this new path P prime. Now, to show that y and z are connected in T prime. So we want to show that there is a path from y to z in T prime. Well, we're going to use the original path in T and then whenever we hit the edge e prime, we're going to replace the edge e prime by this path P prime because the edge e prime does not exist in T prime but the path P prime does exist. So we're going to go along this path and now the original path uses this edge e prime but it doesn't exist now so we use the rest of the cycle. It gets over to d and then from d we follow the rest of the path P. So, actually, in this example we're backtracking. So this doesn't actually give us a path, it gives us a walk. But it shows that y and z are connected in T prime and then, of course, we can truncate this walk to make a path. But our whole point is to show that y and z are connected and that's all we need to show. We've shown that's the case for any y and z therefore, T prime is connected and since it has exactly n-1 edges then T prime is in fact a tree.

GR3 Minimum Spanning Tree - 306 - T is a MST
Now once again we have this subgraph T prime which is obtained by taking the tree T adding in this edge e star and taking out the edge e prime. We've just showed that T prime is a tree and once again, notice that we could have removed any edge of this cycle and we would have attained a tree. But now we want to show that it's a minimum spanning tree. How do we show it's of minimum way? Well you know that T is an MST, so we know that the weight of T is a minimum over all trees. So to show that T prime is a minimum spanning tree, we have to show that the weight of T prime equals the weight of T. What do we know about the weight of each star compared to that of e prime? Well, we know that the weight of each star is minimum over all edges cross the cut, e prime also crosses the cut as the s spur therefore, the way to e star is the most that of the weight of e prime. Now what is the weight of T prime? It's the weight of T plus the weight of e star and then finally minus the weight of e prime. The weight of e star is the most the weight of e prime so this part is almost zero. So this whole thing is at most the weight of T. So we've shown that the weight of T prime is the most of the weight of T T is minimum weight, therefore T prime is also a minimum weight and in fact, the weight of T prime must equal the weight of T. So the weight of e star must equal the weight of e prime. Otherwise we retrain the new tree T prime which is strictly smaller weight than t, which would contradict the fact that T is an MST. So all these edges of T which cross this cut, must be of exactly the same weight as e star otherwise, we can obtain a new tree which is of smaller weight. That completes the proof of the cut property. Now the key idea that I want to stress, is that I can take a tree T and I add an edge into that tree so I take T union e star that creates a cycle. Now I can remove any edge of that cycle and I get a new tree T prime. Now that's the idea that I wanted to get from the proof of the cut property. The other idea I want you to get, is the idea from the statement of the cut property. From the statement of the cut property, I want you to get the idea that a minimum weight edge across the cut is part of a MST. Those are the two key ideas I want you to get from the statement of the cut property and the proof idea of the cut property.

GR3 Minimum Spanning Tree - 307 - Prims Algorithm
Now one last point about the cut property. There's another algorithm, Prim's algorithm. Prim's algorithm is for Minimum Spanning Tree problem. And is very similar to Dijkstra's algorithm for the shortest path. Now to make sure you understand the cut property, you should use it to prove correctness of Prim's algorithm. Make sure you understand why Prim's algorithm is correct, by using the cut property.

GR4 Markov Chains and PageRank - 308 - PageRank Lecture Outline
In this lecture, we're going to look at the PageRank algorithm. This is an algorithm for assigning the importance of a webpage. Now, what exactly do we mean by importance? Well, this is a subjective term, but we're going to give a precise quantitative interpretation of importance. Now, the PageRank algorithm is the algorithm devised by Brin and Page which is at the heart of Google search engine. Now, the PageRank algorithm itself is fairly simple, but to fully understand the algorithm, we have to first understand some basic mathematical tools known as Markov chains. Now, I'm going to give you a quick primer on Markov chains. What exactly are Markov chains? And what are the key properties of Markov chain? Now, the PageRank algorithm is itself a Markov chain but Markov chain is come up quite often. For example, you may have heard of the term MCMC, That stands for Markov chain Monte Carlo. Also, you may have heard of simulated annealing, which is another example of a Markov chain. Now, we're not going to look at these two examples in detail, but if you understand Markov chain, the basics of Markov chains, then that'll help you understand these more sophisticated concepts if you decide to delve into them in more detail. Now, the key concept for Markov chain is the notion of a stationary distribution. So, I'm going to spend some time explaining to you what exactly does a stationary distribution mean. How do we determine what the stationary distribution are and what are the important properties of a Markov chain which connect to the notion of a stationary distribution? Finally, once we understand Markov chains and stationary distributions for Markov chains, then we will be able to fully appreciate the PageRank algorithm and the design choices in the design of the PageRank algorithm. Now, let's go ahead and dive into a Markov chain and look at a specific example of a Markov chain.

GR4 Markov Chains and PageRank - 309 - Example Markov Chain
Now, this directed graph is an example of a Markov Chain. Now this example is meant to illustrate your state of being at various times while sitting in CS 6210. It certainly doesn't illustrate 6505 since we have this sleep here. Now we think a discretizing time. So that the time is a parameter t which goes from zero, one, two. It has integer values. You can think of the time as being like the time in seconds or the time in minutes. Now for this particular example there are four possible states at each time. You can be listening to Kishore, you can be sleeping, you can be checking your email or you can be playing this video game StarCraft. So each vertex in this directed graph corresponds to a state of the Markov Chain. So there's a Markov Chain has four possible states. Now the edges of this directed graph have weights. The weights correspond to the probability of a transition. So the weight of the edge is between say, checking email and StarCraft is the probability of changing from checking email at time t to playing Starcraft at time t plus one. So let's say I'm checking email at times zero. Then at time one, with probability point three I'll be sleeping, with probability point five I'll be playing Starcraft, and with probability point two I'll be listening to Kishore. Similarly, if I'm listening to Kishore at some time t then at time t plus one with probability point five I'll be listening to Kishore again and with probability point five I'll be checking email. So in general, a Markov Chain is defined by a directed graph and one key thing is that this directed graph might have self loops. For instance, if I'm listening to Kishore at time t then with probability point five I'm listening to Kishore at time t plus one. Now notice that the out edges out of each vertex to find the probability distribution for the next state. So if I'm checking email at time t then I'm listening to Kishore with probability point two, sleeping with probability point three, and playing Starcraft with probability point five in the next time-step. Notice that these edge weights point two plus point three plus point five have to sum up to one because this is a probability distribution for the next state. So for every state i, in this case i equals check e-mail, if I sum over the out edges, so I sum over the weights of these out edges, this is P(i,j). Then, what do these some to? They have to sum up to one because this is a probability distribution for the next state given I'm in state i at time t then I'm going to be in state j at time t plus one. Also, what are valid edge weights? Will these correspond to probabilities? Probabilities have to be between zero and one. So all the edge weights are between zero and one. You give me any directed graph with edge weights between zero and one, that defines a Markov Chain. And similarly, any Markov Chain can be viewed as a directed graph with these edge weights.

GR4 Markov Chains and PageRank - 310 - General Markov Chain
>> So how do I define a Markov chain in general? In general I'm going to have capital N states, and we are going to label these states by 1 through N. So in this example, capital N equals four. But in general, think of in our applications capital N is going to be huge. For instance, when we do PageRank, capital N is going to be the number of webpages on the internet. The total number of webpages on the internet. Now, this weighted directed graph is defined by its adjacency matrix. This is the adjacency matrix for this graph where we think of this is vertex, this state. Listening to Kishor is state one, this is state two, playing Starcraft is state three and sleeping is state four. So this is the adjacency matrix for this graph, the weighted adjacency matrix. Now I denote this by P, and we refer to this as a transition matrix Y, because the entry P I J corresponds to the probability of transitioning from state I to state J. So if I'm at state I at time T, the probability I'm in state J at time T plus 1 is exactly the entry P I J. I look at row I and column J. For instance, if I'm playing Starcraft at time T, the probability that I'm checking email at time T plus one is exactly 0.3. Now, the property that you noticed before about P is that each row sums up to one. The terminology for this is that P is a stochastic matrix. If the columns also sum to one, then it's called doubly stochastic. But for a Markov Chain, all I know is that the rows sum up to one. It doesn't necessarily mean that the columns have to sum up to one.

GR4 Markov Chains and PageRank - 311 - 2-Step Transitions
Here again, is our earlier example of a Markov chain with four states. This is a weighted graph. But for a moment let's ignore the edge ways and let's look at the unweighted version of this graph. Here's the adjacency matrix, where this unweighted version of this graph. From State 1, there are edges to itself to State 2 and that's it. So the first row looks like 1 1 0 0, and is straightforward to check the remainder of the adjacency matrix. Now, I want to look at the matrix, A square. So A times A. This is still going to be a four by four matrix. Let's simply multiply it out, and what do we get? I claim that this is the matrix A square. What do these entries mean? Well, take a look at this first entry, 1 1. It has value 2, why? Well, because there are two pass going from State 1 to State 1 of Lane 2 In particular, I can self loop and then self loop, or I can go to State 2 and then back to State 1. Take another State, four comma one. From State 4, look at the pass of Lane 2 to State 1. I can self loop and then follow this edge or I can follow this edge, and then self loop. But if I look at from four to two, what can I do? Well, there's only one path of Lane 2. I can go to State 1 and then over to State 2, and if I look at the pass of length two from 4 to 3, where there are no pass of Lane 2 that go from State 4 to State 3. There's no way to reach 3 from 4 by path of Lane 2. So this matrix A square encodes the number of pass of Lane 2. Now let's go back to the weighted version of this graph. This is the transition matrix for this Markov chain. Let's write the transition matrix once again for this Markov chain. I got half probability of staying at State 1, half probability of going from State 1 to State 2, and then zero probability of going from State 1 to State 3 or State 4. So this is the first row. This is the remainder of the transition matrix. We're going to look at this matrix, P square, which is P times P. Well let's simply multiply it out first and see what matrix we get. Well if we multiply out P times P, this is the matrix P square that we get. What are these entries mean? Well one thing we noticed first, is that we get a 0 entry here, and we also had a zero entry in the matrix A square. But why is that? Well, there's no passive Lane 2 in this graph, from State 4 to State 3. So this entry four, three is 0. These other entries there are no longer integer values, so they no longer correspond to the number of pass between this pair, I J. But they too correspond to the total weight of the pass, from I to J. Suppose we start the Markov Chain at time zero, in State 2, checking email. And I ask, what is the state at time t=2? Two time steps away. Well, one time step away is defined by the matrix P. Now let's look more precisely at the probability of going from State 2 at time zero, to State 1 at time two. So what's this probability of going from State 2 to State 1, in two time steps? Well, there are two ways to do it. I can go from 2 to 1, and then self loop, or I can go 2 to 4 and then to 1. The probability of going from State 2 to State 1 is point 2, and then from 1 to 1 self looping is point 5 probability. The other path has probability point 3 times .7. Now if you work this out, what do you get? You get point 31, which is exactly the entry two one in this matrix P square. So in this matrix P square, at the entry I J, tells us the probability of going from State I to state J in exactly two times steps.

GR4 Markov Chains and PageRank - 312 - k-Step Transitions
Now if we look at this entry (i,j) , in this matrix P, this transition matrix P, corresponding to the way to the adjacency matrix. This tells me the probability of making the transition from I to J in one step. So if I'm in state I at time T, then PIJ is the probability that I'm in State J at time T plus one. Now what we just saw is that if we look at the square of the transition matrix, so P squared, then the entry (i,j) tells me the probability of going from state I to state J in two steps. And in general for any non-negative integer K, P to the K. So the Kth power of this transition matrix P. This tells me the probability of making a transition in exactly K steps.

GR4 Markov Chains and PageRank - 313 - Big k for 6210 Example
Now let's stick with our 6210 example, and here's the transition matrix again for that 6010 example. And now I want to look at powers of P. So I want to look at P to the K for big K and then we're going to see some interesting properties about this matrix. Now once again what we saw before the square of this matrix, so P squared is this matrix. Now let's look at it for big K. So let's look at it from P the 10 and P to the 20. Now actually if you like to code up, you just code it up yourself. Take this matrix and look at powers of it for big K, or take a different matrix. Make up a matrix and look at powers of it for big K. Just make sure that it's not stochastic matrix so each row sums to one, then it corresponds to a Markov chain. So this is the first row for P to the 10th. And here's the second row. Notice that it's quite similar to the first row. Is that just the fluke? Let's look at more rows to see whether that was just a fluke or it has some important properties. Now the exact numbers in this matrix aren't important, but what's important is this interesting property that seems to be coming up. All the roads seem to be converging to the same value. Now there's still a little bit of variation look in this third column. Let's see what happens for P to the 20. While looking at the first column of P to the 20, we noticed that it seems to be converging quite nicely. They all agree on the first four significant digits. If we look at the other columns, we see that those are converging quite nicely as well. So what's our conclusion? Our conclusion is that there seems to be a row vector and all of the rows are converging to this row vector. Now let's look at this matrix see what it means. Take any particular column. Let's take column two. Now let's look at this entry one, two in this matrix P to the 20. What does that mean? Well it means if I start in stage one at time zero, what's the probability that I'm in stage two at time 20? Well that's exactly this entry. Similarly, if I start in stage two or three or four, what's the probability that I'm in stage two at time 20? Well regardless of where I start in, it seems like it's independent of where I start in. At time 20, it's going to be exactly this. And if I look at a larger time then this is going to converge even more. So there's going to be a specific probability that I'm in stage two for big time regardless of where I start at time zero. So that's the key property of this Markov chains. Regardless of where you start, it doesn't matter. If you look for big time, I'm going to converge to some value. So where I am at some big large time is independent of where I start. Let's say that again a little bit more precisely.

GR4 Markov Chains and PageRank - 314 - Infinite Time
Now, what we saw in the previous slide was we looked P raised to the power T or K for big T. We did T equals 20, but let's take T going to infinity and see what it looks like. So let's take the limit as T goes to infinity and look at P raised the power T. It turns out there's this row vector pi. Now these entries might look quite familiar. They look very similar to the row of P to the 20. Now what is P to the T going to look like as T goes to infinity? Well, each row is going to converge to pi. So every row is going to converge to this row vector pi. What does this mean? This means that no matter where you start, it doesn't matter where you start, because that's the row here, independent of where you start. If you look for big enough T, the probability that you are at state J at time T is going to be exactly defined by this row vector pi. So pi of J is going to be the probability that I'm in State J at time T, for Big T. So for our 6210 example, what does this mean? This means no matter where you start at time zero, if the class is long enough, the probability that you're sleeping at time T is exactly 0.104. And similarly the probably you playing Starcraft at time T for big T is exactly point 0.406 for big T. Regardless of where you start at time zero. This pi is referred to as a stationary distribution. You can think of is like a fixed point of the process. For this particular example, regardless of where you start, you eventually reach this stationary distribution and once you're at the stationary distribution, you're going to stay at the stationary distribution. It's going to be invariant. Now what we want to understand is, does every Markov chain have a stationary distribution? And does every Markov chain have this property that regardless of where I start I eventually reach this stationary distribution? Moreover, is there a unique stationary distribution or is there multiple stationary distributions? If you think of the analogy with fixed points are the multiple fixed points? Well, there's only one fixed point and regardless of where I start, the basin of attraction is everywhere. So regardless of where I start, I always reach that one attractive fix point. So is there one stationary distribution which I reach regardless of where I start? Or can there be multiple stationary distributions? Certainly there can be multiple, but we want to look at conditions where there are a unique stationary distribution and regardless of where I start, I always reach that stationary distribution. Then we want to look at what is this pi? What is the stationary distribution? Now this is quite important. The stationary dispersion y. For page rank, what is going to correspond to where we're going to do a random walk. A Markov chain on the web pages and then the page rank is going to correspond to the stationary distribution of that Markov chain. So all this technology is going to be useful when we're trying to understand the page rank algorithm. Now before we move on and look at details about stationary distributions, I want to look at it from another perspective. I want to look at it from a linear algebra perspective. What does a stationary distribution pi mean from a linear algebra perspective?

GR4 Markov Chains and PageRank - 315 - Linear Algebra View
Now here's a running example once again. Now let's suppose that at time zero I'm in state two and I want to know the state at time t equals one. What's the distribution for the state at time t equals one? Well, how do I get it? Why just look at this row. The second row of this transition matrix tells me the distribution for the state at time t equals 1. It's the one step transition matrix. Now what's another way to get row two. Well, I can take this vector which has a one in entry two and zeros everywhere else and I can multiply this row vector by this matrix, and then what do I get? I get row two of this matrix. So this my distribution at times zero and I multiply that by the transition matrix and I get the distribution at time t equals one. And in general at time zero I don't have to be in a fixed state. I can be in a distribution over the states. So let M be an arbitrary distribution over the end states. So what exactly does that mean. That means M is this vector and this row vector of size N and it's a probability distribution, so the sum of these entries is exactly one and all these entries are between zero and one. Now we're placing this factor by distribution, M. Then here we have P and then we get a distribution at time t equals one. So let's call this M zero to denote the distribution at times zero. And over here, we get a distribution for the time t equals one, so let's call this M one. In general, if you take M zero so the distribution at time zero and multiply by this matrix P, the transition matrix, then we get the distribution for the state at time t equals one. So we take this row vector for the distribution of time zero, multiply by one step. So we do one step of our random walk and then we get the distribution, the row vector for the state at time t equals one. Now the key property is that for a stationary distribution pi for any stationary distribution pi. So if I am in the stationary distribution at time zero or at any time t and I look at the state at time t plus one, then what is the distribution going to be? Well once I reach this distribution, I stay in it. It's the limiting distribution. It's like a fixed point of the process. So once you reach a fixed point you stay in a fixed point and that's the same for a stationary distribution. So if I'm in the stationary distribution at time t, and I do one step then I'm still in the stationary distribution pi. So pi times P equals pi. What does that mean in terms of linear algebra? Well this pi is an eigenvector with eigenvalue one. So pi is an eigenvector for this matrix P and it's an eigenvector with eigenvalue one. One turns out to be the largest eigenvalue for this matrix. So this is the principle eigenvector for this matrix. Now there can be multiple eigenvectors with eigenvalue one. We're going to look at situations where we know that there is at most one eigenvector with eigenvalue one. So there is at most one stationary distribution.

GR4 Markov Chains and PageRank - 316 - Stationary Distribution
Let's recap what we know so far about stationary distributions of a Markov Chain. So let's consider a Markov Chain defined by the transition matrix P, now if our Markov Chain is defined on end states then we consider any distribution pi on those end states, so this is a row vector of size N, so for any pi which satisfies pi times P equals pi, so pi once again is a eigenvector with eigenvalue 1 for P then such a pi is a stationary distribution. What this equation says is that, if we start in distribution pi and we do one step of our random walk defined by P then we'll stay in the stationary distribution pi, so pi' isn't variant. Once we reach it we stay in it. Now this defines a stationary distribution. Now, when is there such a pi? What Markov Chains have a stationary distribution? And if there is one, is it unique or there's multiple ones? Under what properties do we have multiple or unique stationary distributions? And for our simple 62-10 example, we noticed that no matter where you started, you always reached the stationary distribution. So in this case when there's a unique stationary distribution, regardless of where we start, do we always reach this stationary distribution? Finally, in this case where there is a unique stationary distribution and we always reach it, we could ask how fast we reach it. This is known as the mixing time of the Markov Chain. How fast the Markov Chain reaches its stationary distribution. This is one of the things I study in my research, trying to prove bounds about the mixing time of Markov Chains. We're not going to look at the mixing time here but what we are going to look at, our properties of the Markov Chain which ensure that we have a unique stationary distribution and that we always reach the stationary distribution. So regardless of where we started from, we eventually in the limit over time will reach a stationary distribution. In order to see which properties ensure a unique stationary distribution, let's look at examples where we have multiple stationary distributions. That will give us some insight into the properties needed to guarantee a unique stationary distribution.

GR4 Markov Chains and PageRank - 317 - Bipartite Markov Chain
Now here's an example of a Markov chain whose graph corresponds to a bipartite graph. Let's look at some basic properties of this Markov chain. Suppose we start our random walk from one of these vertices on the left side, either state one, three, or five. What do we know? We know at time one we're on the right side, and actually at any odd time we're on the right side and at any even time we're on the left side. Now suppose we start on the right side, so at times zero we're at either vertex two or vertex four. Now we have the opposite situation, at odd times where on the left side, at even times we're guaranteed to be on the right side. So the punch line is that the starting state matters in this graph. Whenever we have a bipartite graph the starting state matters. Now we want a simple way to ensure that our Markov chain is not bipartite, and even more so we want to ensure that the Markov chain has no periodic structure. Instead of being bipartite, having these cycles of period two we could have cycles a period three. So in general, if we want to ensure that the graph has no periodic structure, that is aperiodic. Well what's an easy way to ensure that? An easy way is to have a self loop on each vertex. So with some probability we stay where we are. This can be a very small, miniscule probability. So let's say with probability point 01 we stay where we are, and then we can rescale the other probabilities so that they sum up to one. In terms of their transition matrix, this means that our diagonals, the self loops are out diagonal entries in this transition matrix. We want all of these diagonal entries to be strictly greater than zero. So we want the diagonal entries to be positive. If they're positive, that destroys any periodic structure. It can't be bipartite or any periodic structure. So for every state i, we'll make sure that the probability of going from state i and staying in state i is strictly greater than zero. Okay that gets around this pitfall.

GR4 Markov Chains and PageRank - 318 - Multiple SCCs
Now another pitfall that can happen is if our underlying graph has multiple strongly connected components, for example in this graph, there are three strongly connected components. Now look, if we start at one of these three vertices we're only going to reach these three vertices. If we start at one of these two vertices, we only reach these two vertices. So the starting state definitely matters. What we would like is that the graph has one strongly connected component. The terminology for this is that P is irreducible. If the graph has one strongly connected component, every pair of vertices are strongly connected with each other, then the transition matrix is irreducible. Now what's an easy way to assure that the graph is one strongly connected component, while connect up every pair of vertices. So make it the complete graph. For all pairs of states i and j, we make the entry p (ij) be strictly greater than zero. So there's a positive probability of going between every pair of vertices. The matrix P is all positive, and therefore the graph is fully connected, so it's one strongly connected component. This is the easiest way to ensure that the graph is one strongly connected component and therefore irreducible.

GR4 Markov Chains and PageRank - 319 - Ergodic MC
These are the two key properties of a Markov chain, that is aperiodic and irreducible. Recall that aperiodic means that the underlying graph is not bipartite. How do we get around it? We add self-loops, and that ensures that the graph is not bipartite and has no aperiodic structure. Irreducible means that it has one strongly connected component. How do we ensure that? By making sure that the graph is fully connected. All pairs of states can get between each other in one step, there's an edge between every pair of vertices. Now, a Markov chain which is a periodic and irreducible is called the Ergodic, and that's the key property. In Ergodic Markov chain has nice properties. A Markov chain with these two properties has a stationary distribution pi and is unique. There is exactly one stationary distribution. Moreover, we have the following nice property for the raised to the power T. For some big enough T then the matrix looks like pi, pi, pi, every row is pi. What does that mean? That means regardless of where we start. So, we starting at one of these rows, and then we're doing T steps of our random walk. We're going to reach this distribution pi. So no matter the starting state, we eventually reach the unique stationary distribution pi. This was the same scenario that happened for our simple example on four states for the 6210 example. So in other words, we always reach pi no matter where we start. How is page rank going to be defined? Well, page rank is going to be defined by looking at a Markov chain on the web graph, so, we're going to do a random walk on the web graph, the sites, the vertices are going to be web pages, the edges are going to correspond to hyperlinks. Now, we're going to have some technicalities to ensure that the underlying Markov chain is aperiodic and irreducible. How are we going to solve them? Exactly as we mentioned earlier. We're going to add self loops and we're going at it to be fully connected and then the page rank is going to be defined as the stationary distribution of this Markov chain. Since it's Ergodic, there's a unique stationary distribution and no matter where we start as random walk, we'll always reach this unique stationary distribution. So, it's well-defined, the stationary distribution will correspond to the page rank. So, our measure of the importance of a web page will be related to the probability of ending at that web page. We start a random walk from any state, we run the random walk for many steps. What's the probability we end at a particular web page J? That corresponds to the page rank, the importance of the web page J.

GR4 Markov Chains and PageRank - 320 - What is Pi
Now what is this Stationery Distribution PI? Is there a nice formula for this PI? In general no but in some cases yes, there is. A simple case is when P is symmetric, what does this mean? That means the entry p_i_j is equal to the entry p_j_i. So if I look at row I and column J, the probability of going from I to J, is the same as the probability of going from J to I, if that's the case then PI is the uniform distribution. PI of I is one over N for all I. Now that's the simplest case, a generalization is known as reversibility. It's kind of a weighted version of symmetry, in which case PI is not necessarily uniform but we can still figure out the PI easily. But what reversibility requires is that, if there's an edge from I to J then there has to be edge from J to I. Now symmetry says that these probabilities are the same. We don't necessarily need them to be the same but we need that if there's an edge from I to J, then there's an edge from J to I and vice versa. Now this is the case, then we might be able to figure out what PI is, we might have a nice formula for what PI is. If this is not the case, so the chain is not reversible. So for instance there might be an edge from I to J but there is no edge from J to I. Then, in general we have no idea what the stationary distribution, there is no way to figure it out with a close formula. Now we can try to look at P to some high power and figure out what the stationary distribution is but we're not going to get some nice formulas such as this, for the stationary distribution. That completes our description of the introduction to Markov chains. Now we can dive into the details of the page rank algorithm.

GR4 Markov Chains and PageRank - 321 - PageRank
Now we're going to talk about the Page Rank algorithm. This is the algorithm invented by Brin and Page. It was published in 1998. We're going to start by forgetting about Markov chains. You can understand the basic idea of the page rank algorithm without even knowing what a Markov chain is. Page Rank is a natural simple algorithm for determining the importance of web pages. Now importance is a subjective term and it's something that we're going to have to quantify ourselves. Now part of the appeal of Page Rank is how they define importance has an interesting interpretation in terms of Markov chains. Now we're going to get to that at the end of the lecture, for now we'll forget about Markov chains and we'll just look at the simple idea for defining the Page Rank. To understand the appeal of Page Rank, let's go back and think about how search engines worked in the mid-90s. Well the search engines would maintain a database of web pages and then given a query term Q, what would they do? They would do a grep for Q. So they would search for all web pages containing that query term Q. Now it's easy to spam or trick those search engines so that many of the common query terms are embedded in your web page. That's another issue, let's ignore that for now. The bigger issue is that you have many web pages that contain this query term and now how do you present it to the user? How do you sort the web pages? You want to put the most relevant web page up front so there are many web pages let's say containing this query term Q. How do you sort them? Well that's where the importance of the web page comes in. We're going to put the most important web pages containing this query term at the beginning of our list. For example, if you search for Markov chains and CNN lets say, happens to have an article about Markov chains, I'm not sure why that might be the case but let's just say, CNN has an article about Markov chains and I have on my web page I have several lecture notes about Markov chains. Now when we do this search, this grep, both web pages, mine and CNN are going to contain the query term for Markov chains. Which one should be presented first? Well, presumably the user is more likely to be interested in the CNN article about Markov chains rather than my lecture notes about Markov chains. So let's dive into the algorithm description.

GR4 Markov Chains and PageRank - 322 - Webgraph
We're going to look at the so called Web graph. The graph on web pages. What exactly is our graph? Well the vertices of our graph are going to be web pages. Now this the humongous graph, because we have a vertex for every web page and the edges of our graphs are the hyperlinks. Now these are directed edges. A web page X might have a hyperlink to a web page Y or Y doesn't necessarily have a hyperlink back to X. So it's important we think of these edges as directed edges. Now let's introduce some notation, for a web page. X, let's define pi of X to be the rank of this page. Now the rank is our measure of the importance of the web page. Now rank or importance are subjective terms. We need to define pi of X, the rank in a sensible way. Now of course sensible is also a subjective term, but in any case we're going to define pi of X so that it has a very nice natural mathematical interpretation. Now if you just watched the bit about markup change, you might think. Why is he using pi once again? Well it's not a coincidence, before we used pi to correspond to the stationary distribution and that's going to come out later, but for now once again we're not going to talk about Markov chains. Now it would be useful to have a little bit on notation. So let's consider a page X is the vertex in our graph, so it's denoted here. Now there are a bunch of hyperlinks at the web page X. Those are directed edges out of X. We want some notation for this set of neighbors which have edges from X to them. So we're going to find this set out of X. So for a web page X, out of X are the out neighbors of X. These are the web pages which have a hyperlink from X to them. So these are the web pages Y, where there's a hyperlink from X to Y, there is a directed edge from X to Y. So out of X are the set of all Ys such that there's a link from X to Y. Similarly, we're going to have to look at the set of web pages or vertices which have a link to X. So we'll let In of X denote the set of in neighbors of X. This is a set of web pages W, which have a hyperlink from W to X. So there's a direct edge from W to X. So just remember, out of X are the out neighbors of X and in of X are the in neighbors of X. This is the only totation that we'll need.

GR4 Markov Chains and PageRank - 323 - First Idea
So imagine you're studying for your PhD, and you're trying to come up with some measure for the importance of web pages. So you might think of the analogy with academic papers. What's the importance of an academic paper? How do you measure the importance of an academic paper? Well one way that we still use, is using citation counts. How many other papers cite your paper? What does that mean in terms of web pages? Well might be the number of links, hyperlinks to your web page. So our first idea for defining the rank of a page x, is to define pi of x to be the number of links to the page x, number of hyperlinks to the web page x. In terms of the graph, this means we're going to look at the number of in neighbors to x. How many edges come into x.

GR4 Markov Chains and PageRank - 324 - Problem 1
So here was our first idea. We set the rank of a page X to be the number of links into the page X. So pi of X is the in degree of x. Now what's the obvious problems with this? Well, here's a simple example. Let's suppose that Georgia Tech has a webpage which lists all the faculty, okay that's probably true, and the faculty list is probably like a thousand long. One of those is going to link to my webpage. So one out of a thousand of these links links to my webpage. Now let's suppose that Georgia Tech's front page or the COCs front page has only five links on it and one of those happens to be the kishore's webpage. Now in the current measure of the rank of a page X, I get a plus one for this link and Kishore gets a plus one for this link. So both of those links count the same for us. Now that doesn't seem so fair. This is one out of a thousand, this is one out of five. So how do we get around it. Well, the obvious way is to scale it by the number of links. Now the natural solution to this problem is if George Tech's webpage has a thousand links and one of them goes to my webpage then I get one out of a thousand and if Georgia Tech's front page has five links and one of those is to Kishore, then Kishore gets one-fifth of a citation. And in general if a page Y has these many outgoing links, then each webpage is going to get one over the number of outgoing links of a citation. So Georgia Tech's faculty list webpage has a thousand links let's say, so I'm going to get one over thousand of a citation. This webpage has five links, one to kishore's, so kishore is going to get one-fifth of a citation.

GR4 Markov Chains and PageRank - 325 - Second Idea
So we've got a web page X. We're going to look at the In neighbors. So these are the web pages Y which have a link to X. So we're going to sum over these. The old scheme just counted the number of these In neighbors. So the old scheme you can view it as a sum over the in neighbors and we get plus one for each in neighbor. In the new scheme, we want to scale it by number of outgoing links from each of these web pages. So if Y1 has a lot of outgoing links, let's say it has 1000, then this link is going to give us one over 1000. And if this web page has only one link to X, then this one gives us one. And in general, from a web page Y, we're going to get one over the number of out links from Y. And we're going to sum this up over the Ys which have a link to X.

GR4 Markov Chains and PageRank - 326 - Problem 2
So here's our current proposed solution. So instead of setting the rank of the page X to be the number of in neighbors, the number of links into X, we've scaled each of those links by the number of out neighbors from that page y. Now there are some obvious problems with this let's look at one example. Let's suppose that my kids made a web page and it has only one link on it, to my web page. So under this count, I get one citation for it and now let's look at CNN web page. They probably have many links on their web page but perhaps they have an article about Kishore's awesome new research and they have a link to Kishore's web page. So if they have 100 links, Kishore is going to get one over a hundredth of a citation here whereas I'm going to get one citation. Now this doesn't seem very fair because CNN's web page is very important. So we should scale this citation from CNN based on the importance of CNN's web page. So instead of a web page having one citation to give out the web pages should have PI it's rank to give out. So CNN should have Pi of CNN its rank, that's how many citations it has to give out or that's the worth of its citations to give out. Now we can scale each of those citations as we did before based on how many links CNN has. So CNN has 1000 links in each of these web pages which has a link from it, is going to get one over thousands of CNN's worth. So CNN total citation value is Pi over CNN, that's its rank and then each web page that has a link from it, is going to get one over thousands of that worth, the recommendation value. So a citation from a more important web page, has more value because it's going to be proportional to the value the rank of the web page.

GR4 Markov Chains and PageRank - 327 - Rank Definition
So, let's go ahead and formalize this idea. So, a citation from web page y, it's value is going to be proportional to the importance the rank of web page y. So, we're going to scale this quantity by pi of y. Formally, the rank of the web page x. We're going to get it by looking at this sum over all web pages y, which have a link to x. The total value of the citations from y is pi of y. And y has this many outgoing links. This is the number of out neighbors. So, this link from y to x has value pi of y divided by the number of out neighbors of y. And this is going to be the definition of the page rank of web page x. Well, now to be precise, this is not exactly the definition of the page rank of a web page x. There's a technical glitch that we'll notice by looking at this from the perspective of Markov chains. Before we dive back into Markov chains, let's look closely at this proposed definition of the rank. It's a recursive definition. So, first off, is it well defined? Is there a pi satisfying it for every vertex x? This question of whether there exists a pi satisfying it, corresponds to whether there exists a stationary distribution for the corresponding Markov chain. Now, if there does exist such a pi, is there any unique such pi? Or are there multiple pis? This corresponds to the question of whether there is a unique stationary distribution or multiple stationary distributions. We'll address both of these questions using our intuition from Markov chains. First, we'll derive this definition using Markov chains and then we'll see how to ensure that there is a unique stationary distribution. So, let's dive back into Markov chains now.

GR4 Markov Chains and PageRank - 328 - Random Walk
Let's look at this problem from a completely different perspective. So we have the web graph G. The vertices are web pages and the edges are the hyperlinks. These are directed edges. Now let's do a random walk on this graph. What exactly does that mean? It's just like you're surfing the web. So you write a web page, you're going to follow a random hyperlink from that web page, go to the next web page, look at it for a second, hit a random hyperlink and so on. So we started at some web page say we're currently at web page X. Then let's choose a random hyperlink, so uniformly at random from all hyperlinks on this web page X and then we follow that hyperlink and then we repeat the procedure from the new web page. This is a random walk on the directed graph. From a vertex, we're choosing a random outgoing edge. So this is the Markov chain. What's the transition matrix for this Markov chain? Well for Web page Y has a hyperlink to a web page X, so there's a direct edge from Y to X in this web graph. Then the weight of this edge or in the transition matrix, this entry YX is one over the number of out neighbors from Y. The probability of following this particular link when we're at web page Y is exactly one over the number of links at web page Y. So if Y has a thousand links and the probability of following this particular link is one over X. And if web page Y doesn't have a link to web page X, so there's no edge and this transition matrix is zero at that entry, so this defines the transition matrix for this Markov chain. Now this is the Markov chain. So it has a stationary distribution. What does this stationary distribution look like?

GR4 Markov Chains and PageRank - 329 - Stationary Distribution
So recall what is a stationary distribution? A stationary distribution is any vector, any distribution pi which satisfies the following identity, pi times p, the transition matrix, equals pi. So pi is an eigenvector of P with eigenvalue 1 or in other words, if I'm in distribution pi and I do one step of the random walk to find by P, then I'm still in distribution pi. So pi is an invariant distribution. Once I reach it I stay in it. Let's expand this out to see what this means a little bit more precisely. I have pi here. If there are a million web pages then pi is a row vector of size a million. Now I have P here. Now if there are a million web pages then P is of size a million by a million. Now let me just flip this. So let us look at pi equals pi times p. So I just flip the left and right hand side and here's the right hand side, pi equals pi times P. Let's look at an entry X here. So pi of X, this X entry here.If there are a million web pages and let's say X is maybe the 900th entry. How do I get this entry? Well, this X is going to define the column I looked at over here. So this is the 900th entry here and it's going to be the 900th column here. And then what I do, I take this column and then multiply by this row. So Y is going to vary over this row and then Y is going to vary over the rows of this matrix. I'm going to multiply the first entries together plus the second entries multiplied together and so on. So I'm going to sum over Y, the number of Ys is the number of web pages. So Y is varying over all vertices in the graph, all web pages in the graph and I take the Ys entry over here which is pi of Y and I multiply by the Ys entry in this column which is the P, Y over X. So I do the dot product of this row vector with this column vector and that gives me the X entry in pi. Now let's look at this term. What do we know about this term? What do we say that the transition matrix was for this random walk and the last line? Well if there's an edge from Y to X and it's one over the number of outgoing edges out of Y and there's no edge from Y to X then this transition matrix is zero. There's no probability of this random walk going from Y to X in one step. So, for any Y which does not have an edge to X, then this term is zero. So we can drop it. So we only have to look at Ys which have an edge to X. In other words we only have to look at Ys which are in the in neighbor set of X. So we can simplify this sum over all Ys to only Ys which are in the in neighbors. So now we only consider those Ys which have an edge from Y to X and we get pi of Y again and now we can replace this term P, Y of X by this quantity. So we're going to divide pi of Y by the number of our neighbors of Y because we know that if there's an edge from Y to X then P, Y, X is exactly this. So we can replace P, Y, X by this. So we have that pi of X equals this quantity. So what have we shown? We've shown that the stationary distribution of this random walk on the web graph. If we do this simple random walk on the web graph, it's stationary distribution satisfies the following identity. This is what we just saw because pi equals pi times P and we expanded our pi times P and we got this slightly simpler expression. Now, where have we seen this before? Well, what we saw before when we ignored Markov chains and we defined the rank of a web page in terms of citation count intuition. We got this definition. And look, they're identical. This definition and this definition are identical. So these two views are identical. This intuition from citation counts and this intuition from random walks, we get equivalent definitions. Now this random walk interpretation is very appealing. Think about it. So you started any web page, you run the random walk. So you just do random surfing. Now what's the chance you edit a page X. Well, an important web page like CNN or Google, I mean, there's probably a pretty good chance we're going to end up at that web page when we do a random surfing. Whereas somebody is like my web page well there's probably a very small chance that we're going to end up there. So the stationary distribution of this random walk is a very natural nice appealing measure of the importance of a web page. So, this is what we wanted to find the page rank of the web page to be.

GR4 Markov Chains and PageRank - 330 - Problems
Now here's what we just saw. We saw that if we did the random walk on the web graph, then a stationary distribution satisfies this identity. And this is what we'd like to define the PageRank for this web page to be. First off, is this well defined? Think back to our discussion about Markov chains and about stationary distributions of Markov chains. What are some of the key issues that can arise when we talk about stationary distributions of Markov chains? Was there a unique stationary distribution or are there multiple stationary distributions? Well, if there are multiple stationary distributions and it's not clear which one we are referring to. Also, we want that no matter where we start this random walk, we're always going to reach the stationary distribution. For instance, what if G has many strongly connected components? I mean, this is definitely the case. The web graph is going to have many strongly connected components. So, there might be one strongly connected component, very small strongly connected component containing my web page. And, perhaps, if you start the random walk in that strongly connected component, you might have a high probability of ending at my particular web page. Now, does that mean that I should have a high PageRank? Probably not. Now, there might be another strongly connected component, a very giant component and this giant component probably contains Google's web page. And now, if you start the random walk in that giant component, then you probably have a good probability of ending at Google's web page. So Google's web page should have a high PageRank because in that component it has a high rank. So how can we ensure that the Markov chain has a unique stationary distribution? We want to ensure that there's only one strongly connected component and also we want to get rid of any periodicity. There might be some parts of the graph which are bipartite. The easy way we discussed for making sure that there's a unique stationary distribution is to make the graph fully connected. So we're going to add edges, maybe with very small probability, but there will be edges between every pair of vertices. So, from any web page, we'll have some positive probability of going to any other particular web page. So, suppose your web browser had a random button. If you hit this random button, then it's going to take you to a random web page. The web page will be chosen uniformly at random from all web pages. So if there are a million web pages, you have probably one over a million of ending at any particular web page. Now, let's suppose you're surfing the web. What's the random walk going to look like? So, with some probability you're going to follow a random outgoing link from the current web page and with some probability you're going to hit the random button. And that's going to take you to a random web page in the whole graph. So, that random button is going to make the graph fully connected. Let's formalize this random walk that we're talking about here.

GR4 Markov Chains and PageRank - 331 - Random Surfer
So let's formalize this notion of doing a random walk on the web graph, where occasionally we hit the random button. And by hitting the random button, we're going to go to a random web page. So we're going to have an additional parameter Alpha and this Alpha is going to be the probability that we hit the random button. Actually, to be consistent with the actual PageRank definition, Alphas can be a complement of that event. So with probability one minus Alpha, we're going to hit the random button and go to a random web page in the graph. And with probably Alpha, we're going to follow a random edge out of the current web page. So this parameter Alpha is called the damping parameter. It's strictly greater than zero and it's at most one. Why is it called the damping parameter? Because what we're doing is we're scaling down this original webgraph by a factor Alpha. So we're scaling down the webgraph by a factor Alpha and then we're adding in a complete graph of weight one minus Alpha. So now let's look at our random walk. Let's say we're currently at a web page Y. Then with probability Alpha, we're going to follow a random outgoing link from Y. So if Alpha equals one, this is exactly the same as our original random walk. So we're not using this random button at all, because one minus Alpha is zero. But when Alpha is strictly less than one then we're going to use the random button sometimes. So with probability one minus Alpha, we're going to go to a random page. This destination page is chosen uniformly at random from all web pages. So this is our random surfer model. So you give me a parameter Alpha and then my random walk looks like the following. I'm currently at a web page Y, with probability Alpha. I'll look at all the outgoing links from Y and I'll choose one of those uniformly at random. And with probability one minus Alpha, I'm going to go to a random web page uniformly at random from all web pages in the graph. So I'm at this web page Y, I flip a coin or I choose a random number uniformly at random between zero and one. If this random number is at most Alpha, then I choose a random outgoing link. If this random number is strictly greater than Alpha , then I go to a random web page uniformly at random from all web pages. Now what Alpha should we choose? Well according to Wikipedia, Google apparently chooses an Alpha which is at roughly 0.85.

GR4 Markov Chains and PageRank - 332 - Transition Matrix
This random walk that we just defined is a Markov chain. Let's look at the precise definition of the transition matrix for this Markov chain. Let's let capital N denote the number of web pages. So this is the number of vertices in our directed graph. This is going to be humongous. Now, we have this transition matrix of size N by N. Now, there are two cases, either Y to X is an edge in the original graph or it's not. So let's look at these two cases separately. Either Y to X is an edge in the original graph there's a hyperlink from Y to X or there's no hyperlink from Y to X. If there's no hyperlink from Y to X then the only way to go from Y to X is to use the random button. What's the chance of that? Well, there's probability one minus Alpha that we hit the random button. Now when we hit the random button, we go to a random web page. So the probability of going to a particular web page is one over the number of web pages. So it's one over capital N. So the chance of going from Y to X using the random button is one over capital N. So in this case, the probability of going from Y to X if there's no hyperlink from Y to X then it's one minus Alpha for hitting the random button. And then one over N is the probability of going to this particular web page X. Now, if there is a hyperlink from Y to X, well we can still get from Y to X using the random button, probability of that is the same. But we can also get from Y to X using the hyperlink. There's probability Alpha of following a random hyperlink. And given where in this case the probability of following a particular hyperlink is one over the number of hyperlinks. The number of hyperlinks at Y is the number of out neighbors of Y. Now, this transition matrix corresponds to a fully connected graph. Every pair of vertices has an edge. Now, the probability the weight of a particular edge might be very small but still there's an edge between every pair of vertices. So in our terminology from Markov chains this corresponds to an ergodic Markov chain which implies that there is a unique stationary distribution Pi. And regardless of where you start the random walk you always reach this stationary distribution in the limit over time. So the definition of this Pi is independent of the starting state of the random walk. So this Pi is well defined. So we can set this Pi a stationary probability of web page X to be the PageRank, the importance of web page X.

GR4 Markov Chains and PageRank - 333 - Sink Nodes
Let's consider the random surfer model we just defined. There's a problem in the current definition in particular suppose that a page x or web page x has no outgoing links. In a random surfer model with probability one minus alpha, we go to a random web page from the entire graph. And with probability alpha, we follow a random link from this current page x. Now what happens if this page x is a sink node and has no outgoing links? What do we do in this case with probability alpha? Currently the model is not well-defined because of this case. And there are several alternatives that we can consider which will make it well-defined. The simplest option is just to self-loop. What exactly do we mean? We mean that if there's no outgoing links, if this page acts as a sink node, then with probability alpha, we just stay at the page x. We had a link from x to itself. The downside of this approach is that we're adding an incoming link into x. So this is going to artificially boost the page rank of page x. Another option is to simply remove these sink nodes. Now once we remove some of these sink nodes, then there will be new sink nodes that might be created. So we have to recursively apply this approach. We have to keep removing sink nodes recursively until there are no sink nodes remaining in the graph. The problem with this approach is that we shrunk the graph, so there are some nodes which will not get a page rank. One more natural approach I want to consider is that in this case of a sink node x, with probability alpha, we'll go to a random web page chosen uniformly at random from the entire graph. In other words, we're going to set alpha equal zero for just these sink nodes. So, for a sink node with probability one, we'll choose a random web page from the entire graph and go to that random web page. This is a quite natural approach and this is apparently what page rank actually does according to Wikipedia.

GR4 Markov Chains and PageRank - 334 - Ergodic
Let's look again at why the random surfer model is ergodic. We have our original web graph, capital G. Now, what happens in the random surfer model? Well, with probably alpha, we follow a random outgoing link in G. And with probably one minus alpha, we go to a random web page in the entire graph. This corresponds to adding in the complete graph, where capital N is the number of vertices in this original graph. So, this defines our new graph, G-prime. Now, suppose that alpha is strictly less than one. That means, with some positive probability, we use these edges from the complete graph. So, if we consider any pair of states, i and j. Let's look at the probability of going from state i to state j in one step. Well, if alpha is strictly less than one, then there's some chance, some probability, of going from state i to state j, using this last type of transition. That means, in the transition matrix, the entry i j in the matrix P is positive and all the entries in this matrix are positive. There is no zero entries in this matrix. It's a fully connected transition matrix. Therefore, if alpha is strictly less than one, then this random surfer model is ergodic. Now, what happens if alpha equals one, then we're not using this random button here? So, we're just using the original graph and there's no reason why the original graph is going to be ergodic. The original graph that we're interested in is this graph, G. Instead, we're looking at this graph, G-prime. For the case alpha less than one, we need this condition that alpha is less than one in order for it to be ergodic. But how does this new graph, G-prime, compare to this original graph, G? In particular, how does the principle eigenvector for this graph G-prime, this PageRank vector for G-prime, compare to the properties of the original graph G? While this is a somewhat vague, very triggering, but a very vague question. i don't know how to address it. But what we can look at is, what is the effect of varying alpha? How does the PageRank vector change as we vary alpha? Well, if alpha is large, if it's close to one, then this graph G-prime is close to the original graph, G. So we hope that the properties of G-prime are close to the properties of G. As alpha gets smaller, then this complete graph is becoming bigger and we're becoming further away from the original graph, G. But there's a trade-off. As alpha decreases, our convergence rate to the PageRank vector to the principle eigenvector is going to go faster. We're going to converge faster to this principle eigenvector, because of this complete graph, as this becomes bigger, yet the mix faster. Now, according to Wikipedia, Google uses alpha as 0.85. This presents a reasonable trade-off between these two scenarios. But an interesting question is, how does the PageRank vector change as we vary alpha? But if we look at alpha big like 0.99 or 0.95, compared to alpha is 0.85 or 0.75. How does a PageRank vector change? For example, if you look at the top sites, those sites with the largest PageRank vectors, how does a set of top sites change with alpha? And does their ordering change with alpha? So, if you implement the PageRank algorithm and you take a large dataset, then you can look at what is the effect of varying alpha on the ordering of the sites, according to PageRank.

GR4 Markov Chains and PageRank - 335 - Finding Pi
Now, how do I find this vector pi? How do I find the stationary distribution of this Markov chain? If I want to use pi as the measure of the importance of a web page, I have to find pi, or find something which is a close approximation to it. Now to find pi, what do I do? Well, I start at some initial distribution. I can take any initial distribution. Let's define that initial distribution by a row vector Mo and then I'm going to run the Markov chain, the random walk for T-stat for Big T. Computationally, that means I take Mo multiply it by P raised to the power t. Now how big of a t do we need to use? It turns out for the random walk that we're considering, just a very small t suffices. Why is that the case? Well for this particular Markov chain we have with probability one minus Alpha, we choose a random web page. Those links corresponding to the random button allow the Markov chain to mix rapidly. In practice, to check whether you did a big enough t what do you do? Well you empirically check whether this thing seemed to converge or not. Now what do you use for this initial vector? Well, if you're running this on all the web pages, well that's a humongous quantity. So you want to use a reasonable approximation to the real pi as your starting state. Well, if you're updating the webgraph every week let's say, then I would use last week's pi as my initial distribution and then I would run the random walk for a small number of steps hopefully and then that would give me my approximation for the new pi so I use my last week's pi as my initial distribution. Now one important thing to consider, this matrix is huge. Capital N, the number of web pages, is humongous. So order N square is too big. So how long does it take me to compute this vector times this matrix? Well the naive way it's going to take me order N square time that's too long. For the Ns we're considering, there's no way you can run for order N square time, you won't even have order N square space. You need to do it in order m time, m is the number of edges in the original web graph. Now of course some web pages might have many hyperlinks out of it but typically there's going to be a constant number of hyperlinks out of each web page. So m is probably going to be on the order of N and if you think about the transition matrix that we use for PageRank, then one can implement this in order m time with just a little bit of thought and this will typically be order n as opposed to order N square. So linear time is more reasonable that's something we can implement for large N. Well that completes the description of the PageRank algorithm.

