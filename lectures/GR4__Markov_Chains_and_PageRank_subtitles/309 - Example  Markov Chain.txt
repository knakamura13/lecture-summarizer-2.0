Now, this directed graph is an example of a Markov Chain. Now this example is meant to illustrate your state of being at various times while sitting in CS 6210. It certainly doesn't illustrate 6505 since we have this sleep here. Now we think a discretizing time. So that the time is a parameter t which goes from zero, one, two. It has integer values. You can think of the time as being like the time in seconds or the time in minutes. Now for this particular example there are four possible states at each time. You can be listening to Kishore, you can be sleeping, you can be checking your email or you can be playing this video game StarCraft. So each vertex in this directed graph corresponds to a state of the Markov Chain. So there's a Markov Chain has four possible states. Now the edges of this directed graph have weights. The weights correspond to the probability of a transition. So the weight of the edge is between say, checking email and StarCraft is the probability of changing from checking email at time t to playing Starcraft at time t plus one. So let's say I'm checking email at times zero. Then at time one, with probability point three I'll be sleeping, with probability point five I'll be playing Starcraft, and with probability point two I'll be listening to Kishore. Similarly, if I'm listening to Kishore at some time t then at time t plus one with probability point five I'll be listening to Kishore again and with probability point five I'll be checking email. So in general, a Markov Chain is defined by a directed graph and one key thing is that this directed graph might have self loops. For instance, if I'm listening to Kishore at time t then with probability point five I'm listening to Kishore at time t plus one. Now notice that the out edges out of each vertex to find the probability distribution for the next state. So if I'm checking email at time t then I'm listening to Kishore with probability point two, sleeping with probability point three, and playing Starcraft with probability point five in the next time-step. Notice that these edge weights point two plus point three plus point five have to sum up to one because this is a probability distribution for the next state. So for every state i, in this case i equals check e-mail, if I sum over the out edges, so I sum over the weights of these out edges, this is P(i,j). Then, what do these some to? They have to sum up to one because this is a probability distribution for the next state given I'm in state i at time t then I'm going to be in state j at time t plus one. Also, what are valid edge weights? Will these correspond to probabilities? Probabilities have to be between zero and one. So all the edge weights are between zero and one. You give me any directed graph with edge weights between zero and one, that defines a Markov Chain. And similarly, any Markov Chain can be viewed as a directed graph with these edge weights.