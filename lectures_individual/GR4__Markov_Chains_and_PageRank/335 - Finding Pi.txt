Now, how do I find this vector pi? How do I find the stationary distribution of this Markov chain? If I want to use pi as the measure of the importance of a web page, I have to find pi, or find something which is a close approximation to it. Now to find pi, what do I do? Well, I start at some initial distribution. I can take any initial distribution. Let's define that initial distribution by a row vector Mo and then I'm going to run the Markov chain, the random walk for T-stat for Big T. Computationally, that means I take Mo multiply it by P raised to the power t. Now how big of a t do we need to use? It turns out for the random walk that we're considering, just a very small t suffices. Why is that the case? Well for this particular Markov chain we have with probability one minus Alpha, we choose a random web page. Those links corresponding to the random button allow the Markov chain to mix rapidly. In practice, to check whether you did a big enough t what do you do? Well you empirically check whether this thing seemed to converge or not. Now what do you use for this initial vector? Well, if you're running this on all the web pages, well that's a humongous quantity. So you want to use a reasonable approximation to the real pi as your starting state. Well, if you're updating the webgraph every week let's say, then I would use last week's pi as my initial distribution and then I would run the random walk for a small number of steps hopefully and then that would give me my approximation for the new pi so I use my last week's pi as my initial distribution. Now one important thing to consider, this matrix is huge. Capital N, the number of web pages, is humongous. So order N square is too big. So how long does it take me to compute this vector times this matrix? Well the naive way it's going to take me order N square time that's too long. For the Ns we're considering, there's no way you can run for order N square time, you won't even have order N square space. You need to do it in order m time, m is the number of edges in the original web graph. Now of course some web pages might have many hyperlinks out of it but typically there's going to be a constant number of hyperlinks out of each web page. So m is probably going to be on the order of N and if you think about the transition matrix that we use for PageRank, then one can implement this in order m time with just a little bit of thought and this will typically be order n as opposed to order N square. So linear time is more reasonable that's something we can implement for large N. Well that completes the description of the PageRank algorithm.